{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b623545b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-07 16:53:52.633888: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/opt/gurobi912/linux64/lib\n",
      "2022-02-07 16:53:52.633910: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import or_suite\n",
    "import gym\n",
    "from garage import wrap_experiment\n",
    "from garage.envs import GymEnv, normalize\n",
    "from garage.experiment.deterministic import set_seed\n",
    "from garage.experiment import deterministic\n",
    "from garage.sampler import LocalSampler\n",
    "from garage.torch.algos import TRPO\n",
    "from garage.torch.policies import GaussianMLPPolicy\n",
    "from garage.torch.value_functions import GaussianMLPValueFunction\n",
    "from garage.trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94716dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "04afaddc",
   "metadata": {},
   "source": [
    "Generic packages.  Expect some 'warnings' because you have no GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36cc6fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_parameters = {\n",
    "    'hidden_sizes': [32, 32], # neural network parameters\n",
    "    'max_kl': 0.01, # \"essentially\" some function of the policy regularization\n",
    "    'gae_lambda': 0.97, # no clue\n",
    "    'discount': 0.99, # Oh, this will do infinite horizon with discount instead of finite horizon\n",
    "    'n_epochs': 999, # num episodes\n",
    "    'batch_size': 1024, # batches used for computing gradient steps\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9a3eb0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONFIG =  or_suite.envs.env_configs.ambulance_metric_default_config\n",
    "CONFIG['epLen'] = 500\n",
    "env = gym.make('Ambulance-v0', config=CONFIG)\n",
    "env._max_episode_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "432beeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@wrap_experiment\n",
    "def trpo_ambulance(ctxt=None, seed=1):\n",
    "    \"\"\"Train TRPO with MultipleSuppliers-v0 environment.\n",
    "    Args:\n",
    "        ctxt (garage.experiment.ExperimentContext): The experiment\n",
    "            configuration used by Trainer to create the snapshotter.\n",
    "        seed (int): Used to seed the random number generator to produce\n",
    "            determinism.\n",
    "    \"\"\"\n",
    "    set_seed(seed)\n",
    "    CONFIG =  or_suite.envs.env_configs.ambulance_metric_default_config\n",
    "    CONFIG['epLen'] = 500\n",
    "    env = gym.make('Ambulance-v0', config=CONFIG)\n",
    "    env = GymEnv(env)\n",
    "    trainer = Trainer(ctxt)\n",
    "\n",
    "    policy = GaussianMLPPolicy(env.spec,\n",
    "                               hidden_sizes=[32, 32],\n",
    "                               hidden_nonlinearity=torch.tanh,\n",
    "                               output_nonlinearity=None)\n",
    "#     env.spec.max_episode_length = CONFIG['epLen']\n",
    "\n",
    "    value_function = GaussianMLPValueFunction(env_spec=env.spec,\n",
    "                                              hidden_sizes=(32, 32),\n",
    "                                              hidden_nonlinearity=torch.tanh,\n",
    "                                              output_nonlinearity=None)\n",
    "    sampler = LocalSampler(agents=policy,\n",
    "                           envs=env,\n",
    "                           max_episode_length=CONFIG['epLen']) # TODO: Issue here\n",
    "\n",
    "    algo = TRPO(env_spec=env.spec,\n",
    "                policy=policy,\n",
    "                value_function=value_function,\n",
    "                sampler=sampler,\n",
    "                discount=0.99,\n",
    "                center_adv=False)\n",
    "\n",
    "    trainer.setup(algo, env)\n",
    "    trainer.train(n_epochs=100, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c871be5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0899a7a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-02-07 16:53:53 | [trpo_ambulance] Logging to /home/sean/Programming/ORSuite/examples/data/local/experiment/trpo_ambulance_10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sean/.local/lib/python3.8/site-packages/garage/experiment/deterministic.py:36: UserWarning: Enabeling deterministic mode in PyTorch can have a performance impact when using GPU.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-02-07 16:53:54 | [trpo_ambulance] Obtaining samples...\n",
      "[0.3342127]\n",
      "[1.4968015]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrpo_ambulance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/garage/experiment/experiment.py:369\u001b[0m, in \u001b[0;36mExperimentTemplate.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    368\u001b[0m     ctxt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_context(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_options(\u001b[38;5;241m*\u001b[39margs), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 369\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctxt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    370\u001b[0m     logger\u001b[38;5;241m.\u001b[39mremove_all()\n\u001b[1;32m    371\u001b[0m     logger\u001b[38;5;241m.\u001b[39mpop_prefix()\n",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36mtrpo_ambulance\u001b[0;34m(ctxt, seed)\u001b[0m\n\u001b[1;32m     31\u001b[0m algo \u001b[38;5;241m=\u001b[39m TRPO(env_spec\u001b[38;5;241m=\u001b[39menv\u001b[38;5;241m.\u001b[39mspec,\n\u001b[1;32m     32\u001b[0m             policy\u001b[38;5;241m=\u001b[39mpolicy,\n\u001b[1;32m     33\u001b[0m             value_function\u001b[38;5;241m=\u001b[39mvalue_function,\n\u001b[1;32m     34\u001b[0m             sampler\u001b[38;5;241m=\u001b[39msampler,\n\u001b[1;32m     35\u001b[0m             discount\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.99\u001b[39m,\n\u001b[1;32m     36\u001b[0m             center_adv\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     38\u001b[0m trainer\u001b[38;5;241m.\u001b[39msetup(algo, env)\n\u001b[0;32m---> 39\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/garage/trainer.py:402\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, n_epochs, batch_size, plot, store_episodes, pause_for_plot)\u001b[0m\n\u001b[1;32m    399\u001b[0m summary_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(log_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexperiment.json\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    400\u001b[0m dump_json(summary_file, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 402\u001b[0m average_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_algo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown_worker()\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m average_return\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/garage/torch/algos/vpg.py:224\u001b[0m, in \u001b[0;36mVPG.train\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstep_epochs():\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_samples):\n\u001b[0;32m--> 224\u001b[0m         eps \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobtain_episodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_itr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m         last_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_once(trainer\u001b[38;5;241m.\u001b[39mstep_itr, eps)\n\u001b[1;32m    226\u001b[0m         trainer\u001b[38;5;241m.\u001b[39mstep_itr \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/garage/trainer.py:224\u001b[0m, in \u001b[0;36mTrainer.obtain_episodes\u001b[0;34m(self, itr, batch_size, agent_update, env_update)\u001b[0m\n\u001b[1;32m    222\u001b[0m         policy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_algo\u001b[38;5;241m.\u001b[39mpolicy\n\u001b[1;32m    223\u001b[0m     agent_update \u001b[38;5;241m=\u001b[39m policy\u001b[38;5;241m.\u001b[39mget_param_values()\n\u001b[0;32m--> 224\u001b[0m episodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sampler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobtain_samples\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mitr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43magent_update\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43magent_update\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m    \u001b[49m\u001b[43menv_update\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv_update\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stats\u001b[38;5;241m.\u001b[39mtotal_env_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(episodes\u001b[38;5;241m.\u001b[39mlengths)\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m episodes\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/garage/sampler/local_sampler.py:160\u001b[0m, in \u001b[0;36mLocalSampler.obtain_samples\u001b[0;34m(self, itr, num_samples, agent_update, env_update)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m worker \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers:\n\u001b[0;32m--> 160\u001b[0m         batch \u001b[38;5;241m=\u001b[39m \u001b[43mworker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m         completed_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch\u001b[38;5;241m.\u001b[39mactions)\n\u001b[1;32m    162\u001b[0m         batches\u001b[38;5;241m.\u001b[39mappend(batch)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/garage/sampler/default_worker.py:184\u001b[0m, in \u001b[0;36mDefaultWorker.rollout\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;124;03m\"\"\"Sample a single episode of the agent in the environment.\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \n\u001b[1;32m    179\u001b[0m \u001b[38;5;124;03mReturns:\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;124;03m    EpisodeBatch: The collected episode.\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \n\u001b[1;32m    182\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_episode()\n\u001b[0;32m--> 184\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollect_episode()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/garage/sampler/default_worker.py:110\u001b[0m, in \u001b[0;36mDefaultWorker.step_episode\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eps_length \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_length:\n\u001b[1;32m    109\u001b[0m     a, agent_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent\u001b[38;5;241m.\u001b[39mget_action(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prev_obs)\n\u001b[0;32m--> 110\u001b[0m     es \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_observations\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prev_obs)\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_env_steps\u001b[38;5;241m.\u001b[39mappend(es)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/garage/envs/gym_env.py:220\u001b[0m, in \u001b[0;36mGymEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step_cnt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreset() must be called before step()!\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 220\u001b[0m observation, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_visualize:\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_env\u001b[38;5;241m.\u001b[39mrender(mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Programming/ORSuite/or_suite/envs/ambulance/ambulance_metric.py:118\u001b[0m, in \u001b[0;36mAmbulanceEnvironment.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28mprint\u001b[39m(action)\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(action, np\u001b[38;5;241m.\u001b[39mndarray): action \u001b[38;5;241m=\u001b[39m action\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mcontains(action)\n\u001b[1;32m    120\u001b[0m old_state \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# The location of the new arrival is chosen randomly from the arrivals\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# distribution arrival_dist\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trpo_ambulance(seed = 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
