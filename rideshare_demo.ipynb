{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "respective-shelter",
   "metadata": {},
   "source": [
    "# OR Suite \n",
    "\n",
    "Reinforcement learning (RL) is a natural model for problems involving real-time sequential decision making, including inventory control, resource allocation, ridesharing systems, and ambulance routing. In these models, an agent interacts with a system that has stochastic transitions and rewards, and aims to control the system by maximizing their cumulative rewards across the trajectory. Reinforcement learning has been shown in practice to be an effective technique for learning complex control policies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "female-employee",
   "metadata": {},
   "source": [
    "# Ridesharing Code Demo\n",
    "\n",
    "Reinforcement learning (RL) is a natural model for problems involving real-time sequential decision making. In these models, a principal interacts with a system having stochastic transitions and rewards and aims to control the system online (by exploring available actions using real-time feedback) or offline (by exploiting known properties of the system).\n",
    "\n",
    "This project revolves around providing a unified landscape on scaling reinforcement learning algorithms to operations research domains.\n",
    "\n",
    "In this notebook, we walk through the Ambulance Routing problem with a 1-dimensional reinforcement learning environment in the space $X = [0, 1]$. Each ambulance in the problem can be located anywhere in $X$, so the state space is $S = X^k$, where $k$ is the number of ambulances. For this example there will be only one ambulance, so $k = 1$.\n",
    "\n",
    "The default distribution for call arrivals is $Beta(5, 2)$ over $[0,1]$, however any probability distribution defined over the interval $[0,1]$ is valid. The probability distribution can also change with each timestep.\n",
    "\n",
    "For example, in a problem with two ambulances, imagine the ambulances are initially located at $0.4$ and $0.6$, and the distance function being used is the $\\ell_1$ norm. The agent could choose to move the ambulances to $0.342$ and $0.887$. If a call arrived at $0.115$, ambulance 1, which was at $0.342$, would respond to that call, and the state at the end of the iteration would be ambulance 1 at $0.115$ and ambulance 2 at $0.887$. The agent could then choose new locations to move the ambulances to, and the cycle would repeat."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "drawn-virus",
   "metadata": {},
   "source": [
    "# Step 1: Package Installation\n",
    "First we import the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "flush-pulse",
   "metadata": {},
   "outputs": [],
   "source": [
    "import or_suite\n",
    "import numpy as np\n",
    "import itertools as it\n",
    "\n",
    "import copy\n",
    "\n",
    "import os\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greenhouse-continent",
   "metadata": {},
   "source": [
    "# Step 2: Pick problem parameters for the environment\n",
    "\n",
    "Here we use the ambulance metric environment as outlined in `or_suite/envs/ambulance/ambulance_metric.py`.  The package has default specifications for all of the environments in the file `or_suite/envs/env_configs.py`, and so we use one the default for the ambulance problem in a metric space.\n",
    "\n",
    "In addition, we need to specify the number of episodes for learning, and the number of iterations (in order to plot average results with confidence intervals)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "functional-encounter",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG =  or_suite.envs.env_configs.rideshare_graph_default_config\n",
    "\n",
    "epLen = CONFIG['epLen']\n",
    "nEps = 100\n",
    "numIters = 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threaded-tradition",
   "metadata": {},
   "source": [
    "# Step 3: Pick simulation parameters\n",
    "\n",
    "Next we need to specify parameters for the simulation. This includes setting a seed, the frequency to record the metrics, directory path for saving the data files, a deBug mode which prints the trajectory, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "organic-colorado",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_SETTINGS = {'seed': 1, \n",
    "                    'recFreq': 1, \n",
    "                    'dirPath': '../data/rideshare/', \n",
    "                    'deBug': False, \n",
    "                    'nEps': nEps, \n",
    "                    'numIters': numIters, \n",
    "                    'saveTrajectory': True, \n",
    "                    'epLen' : epLen,\n",
    "                    'render': False,\n",
    "                    'pickle': False\n",
    "                    }\n",
    "\n",
    "starting_state = CONFIG['starting_state']\n",
    "num_cars = CONFIG['num_cars']\n",
    "\n",
    "rideshare_env = gym.make('Rideshare-v0', config=CONFIG)\n",
    "mon_env = Monitor(rideshare_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spoken-sender",
   "metadata": {},
   "source": [
    "# Step 4: Pick list of algorithms\n",
    "\n",
    "We have several heuristics implemented for each of the environments defined, in addition to a `Random` policy, and some `RL discretization based` algorithms. \n",
    "\n",
    "The `Stable` agent only moves ambulances when responding to an incoming call and not in between calls. This means the policy $\\pi$ chosen by the agent for any given state $X$ will be $\\pi_h(X) = X$\n",
    "\n",
    "The `Median` agent takes a list of all past call arrivals sorted by arrival location, and partitions it into $k$ quantiles where $k$ is the number of ambulances. The algorithm then selects the middle data point in each quantile as the locations to station the ambulances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "experimental-stick",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sean/anaconda3/envs/ORSuite/lib/python3.8/site-packages/stable_baselines3/ppo/ppo.py:131: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 5`, after every 0 untruncated mini-batches, there will be a truncated mini-batch of size 5\n",
      "We recommend using a `batch_size` that is a multiple of `n_steps * n_envs`.\n",
      "Info: (n_steps=5 and n_envs=1)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "agents = { 'SB PPO': PPO(MlpPolicy, mon_env, gamma=1, verbose=0, n_steps=epLen),\n",
    "'Random': or_suite.agents.rl.random.randomAgent(),\n",
    "'maxweightfixed' : or_suite.agents.rideshare.max_weight_fixed.maxWeightFixedAgent(CONFIG['epLen'], num_cars, [1 for _ in range(len(starting_state))]),\n",
    "'closestcar' : or_suite.agents.rideshare.closest_car.closetCarAgent(CONFIG['epLen'], CONFIG)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharing-decline",
   "metadata": {},
   "source": [
    "# Step 5: Run Simulations\n",
    "\n",
    "Run the different heuristics in the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "vertical-distributor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SB PPO\n",
      "**************************************************\n",
      "Running experiment\n",
      "**************************************************\n",
      "New Experiment Run\n",
      "Iteration: 0\n",
      "Iteration: 1\n",
      "[-3.75, -10001.25, -1.75, -3.5, -10002.5, -20001.25, -30001.0, -3.5, -0.25, -2.75, -1.75, -10001.25, -2.75, -10001.75, -20001.25, -10003.0, -30000.75, -3.0, -1.25, -10002.25, -10002.75, -2.75, -1.25, -1.25, -1.75, -2.25, -2.75, -3.5, -3.5, -10002.5, -1.0, -30001.25, -1.5, -2.5, -2.75, -0.75, -30001.5, -10000.75, -30000.5, -40000.75, -40000.75, -30001.25, -20001.25, -10002.75, -10001.25, -30001.5, -50000.0, -30001.25, -30001.5, -20002.0, -30001.25, -20002.0, -10002.5, -20002.25, -10003.0, -10002.25, -1.5, -3.25, -1.0, -2.25, -1.25, -20001.25, -29999.5, -40000.75, -20001.25, -20001.5, -10002.75, -20001.25, -40000.75, -10001.0, -20002.0, -20002.0, -20001.5, -30001.5, -10002.0, 0.0, -20002.0, -20002.25, -30001.25, -2.25, -2.25, -20001.5, -10002.75, -10001.5, -10000.75, -10001.0, -30001.5, -20001.25, -10000.25, -10002.25, -20001.25, -2.25, -10000.75, -10001.75, -30001.25, -20001.0, -1.25, -2.75, -20002.0, -0.25, -20002.0, -3.5, -30000.5, -20001.5, -1.25, -20000.25, -10002.5, -2.75, -2.25, -10002.25, -20001.0, -20002.25, -10002.75, -30001.5, -10002.0, -1.0, -10000.75, -1.5, -10000.75, -40000.5, -2.0, -50000.0, -20001.25, -3.75, -3.5, -10001.25, -39999.75, -10002.0, -10002.0, -40000.75, -1.75, -10002.0, -10001.75, -2.5, -10002.5, -20002.0, -10001.25, -20002.0, -40000.5, -30001.5, -20000.5, -30001.5, -20001.75, -0.5, -2.5, 0.0, -3.75, -1.5, -1.25, -10002.0, -2.25, -2.75, -2.75, -20002.0, -20002.0, -10002.75, -10001.5, -30000.5, -30001.25, -30001.25, -1.25, -10002.75, -30001.5, -10000.75, -10001.25, -10001.0, -2.25, -2.75, -20001.75, -1.25, -10002.5, -10002.5, -1.25, -2.5, -2.75, -2.25, -1.0, -1.0, -0.5, -3.5, -10002.75, -40000.5, -10001.75, -10002.0, -3.0, -20001.5, -50000.0, -2.75, -1.5, -3.25, -1.5, 0.0, -2.0, -2.0, -2.5, -1.75, -2.75, -2.75, -10001.75, -2.5]\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "**************************************************\n",
      "Experiment complete\n",
      "**************************************************\n",
      "**************************************************\n",
      "Saving data\n",
      "**************************************************\n",
      "     episode  iteration  epReward      time    memory\n",
      "0        0.0        0.0     -3.75 -3.624870  261459.0\n",
      "1        1.0        0.0 -10001.25 -3.110107  261459.0\n",
      "2        2.0        0.0     -1.75 -3.191880  261459.0\n",
      "3        3.0        0.0     -3.50 -3.360084  261459.0\n",
      "4        4.0        0.0 -10002.50 -3.365246  261459.0\n",
      "..       ...        ...       ...       ...       ...\n",
      "195     95.0        1.0     -1.75 -3.405162  121223.0\n",
      "196     96.0        1.0     -2.75 -3.380661  121223.0\n",
      "197     97.0        1.0     -2.75 -3.396026  121223.0\n",
      "198     98.0        1.0 -10001.75 -3.402136  121223.0\n",
      "199     99.0        1.0     -2.50 -3.401391  121223.0\n",
      "\n",
      "[200 rows x 5 columns]\n",
      "Writing to file ../data/rideshare_SB PPO_10data.csv\n",
      "**************************************************\n",
      "Data save complete\n",
      "**************************************************\n",
      "Random\n",
      "**************************************************\n",
      "Running experiment\n",
      "**************************************************\n",
      "**************************************************\n",
      "Experiment complete\n",
      "**************************************************\n",
      "**************************************************\n",
      "Saving data\n",
      "**************************************************\n",
      "Writing to file data.csv\n",
      "**************************************************\n",
      "Data save complete\n",
      "**************************************************\n",
      "maxweightfixed\n",
      "**************************************************\n",
      "Running experiment\n",
      "**************************************************\n",
      "**************************************************\n",
      "Experiment complete\n",
      "**************************************************\n",
      "**************************************************\n",
      "Saving data\n",
      "**************************************************\n",
      "Writing to file data.csv\n",
      "**************************************************\n",
      "Data save complete\n",
      "**************************************************\n",
      "closestcar\n",
      "**************************************************\n",
      "Running experiment\n",
      "**************************************************\n",
      "**************************************************\n",
      "Experiment complete\n",
      "**************************************************\n",
      "**************************************************\n",
      "Saving data\n",
      "**************************************************\n",
      "Writing to file data.csv\n",
      "**************************************************\n",
      "Data save complete\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "path_list_line = []\n",
    "algo_list_line = []\n",
    "path_list_radar = []\n",
    "algo_list_radar= []\n",
    "\n",
    "linspace_alpha = []\n",
    "\n",
    "param_list = [list(p) for p in it.product(np.linspace(0,1,4),repeat = len(starting_state))]\n",
    "\n",
    "for agent in agents:\n",
    "    print(agent)\n",
    "    DEFAULT_SETTINGS['dirPath'] = '../data/rideshare_'+str(agent)+'_'+str(num_cars)\n",
    "    if agent == 'max_weight_fixed':\n",
    "        or_suite.utils.run_single_algo_tune(rideshare_env,agents[agent], param_list, DEFAULT_SETTINGS)\n",
    "    elif agent == 'SB PPO':\n",
    "        or_suite.utils.run_single_sb_algo(mon_env, agents[agent], DEFAULT_SETTINGS)\n",
    "    else:\n",
    "        or_suite.utils.run_single_algo(rideshare_env, agents[agent], DEFAULT_SETTINGS)\n",
    "\n",
    "    path_list_line.append('../data/rideshare_'+str(agent)+'_'+str(num_cars))\n",
    "    algo_list_line.append(str(agent))\n",
    "    path_list_radar.append('../data/rideshare_'+str(agent)+'_'+str(num_cars))\n",
    "    algo_list_radar.append(str(agent))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southern-colleague",
   "metadata": {},
   "source": [
    "# Step 6: Generate Figures\n",
    "\n",
    "Create a chart to compare the different heuristic functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "extra-wound",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/rideshare_SB PPO_10/trajectory.obj'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-8a03d26894f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0madditional_metric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mfig_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'rideshare_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_radar_plot'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.pdf'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m or_suite.plots.plot_radar_plots(path_list_radar, algo_list_radar,\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mfig_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfig_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0madditional_metric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/ORSuite/or_suite/plots.py\u001b[0m in \u001b[0;36mplot_radar_plots\u001b[0;34m(path_list, algo_list, fig_path, fig_name, additional_metric)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;31m# Calculates each additional metric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/trajectory.obj'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0madditional_metric\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/rideshare_SB PPO_10/trajectory.obj'"
     ]
    }
   ],
   "source": [
    "fig_path = '../figures/'\n",
    "fig_name = 'rideshare_'+'_line_plot'+'.pdf'\n",
    "or_suite.plots.plot_line_plots(path_list_line, algo_list_line, fig_path, fig_name, int(nEps / 40)+1)\n",
    "\n",
    "additional_metric = {}\n",
    "fig_name = 'rideshare_'+'_'+'_radar_plot'+'.pdf'\n",
    "or_suite.plots.plot_radar_plots(path_list_radar, algo_list_radar,\n",
    "fig_path, fig_name,\n",
    "additional_metric\n",
    ")\n",
    "\n",
    "# TODO: Import figures and display\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4c142dd05be15695d7d0f22ccf2092ca7b90c6a6af78118fc1f80db7c62802c3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
