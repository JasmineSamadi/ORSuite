{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# OR Suite \n",
    "\n",
    "Reinforcement learning (RL) is a natural model for problems involving real-time sequential decision making, including inventory control, resource allocation, ridesharing systems, and ambulance routing. In these models, an agent interacts with a system that has stochastic transitions and rewards, and aims to control the system by maximizing their cumulative rewards across the trajectory. Reinforcement learning has been shown in practice to be an effective technique for learning complex control policies."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Ridesharing Code Demo\n",
    "\n",
    "Reinforcement learning (RL) is a natural model for problems involving real-time sequential decision making. In these models, a principal interacts with a system having stochastic transitions and rewards and aims to control the system online (by exploring available actions using real-time feedback) or offline (by exploiting known properties of the system).\n",
    "\n",
    "This project revolves around providing a unified landscape on scaling reinforcement learning algorithms to operations research domains.\n",
    "\n",
    "In this notebook, we walk through the Ambulance Routing problem with a 1-dimensional reinforcement learning environment in the space $X = [0, 1]$. Each ambulance in the problem can be located anywhere in $X$, so the state space is $S = X^k$, where $k$ is the number of ambulances. For this example there will be only one ambulance, so $k = 1$.\n",
    "\n",
    "The default distribution for call arrivals is $Beta(5, 2)$ over $[0,1]$, however any probability distribution defined over the interval $[0,1]$ is valid. The probability distribution can also change with each timestep.\n",
    "\n",
    "For example, in a problem with two ambulances, imagine the ambulances are initially located at $0.4$ and $0.6$, and the distance function being used is the $\\ell_1$ norm. The agent could choose to move the ambulances to $0.342$ and $0.887$. If a call arrived at $0.115$, ambulance 1, which was at $0.342$, would respond to that call, and the state at the end of the iteration would be ambulance 1 at $0.115$ and ambulance 2 at $0.887$. The agent could then choose new locations to move the ambulances to, and the cycle would repeat."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Step 1: Package Installation\n",
    "First we import the necessary packages"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import or_suite\n",
    "import numpy as np\n",
    "import itertools as it\n",
    "\n",
    "import copy\n",
    "\n",
    "import os\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import gym"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Step 2: Pick problem parameters for the environment\n",
    "\n",
    "Here we use the ambulance metric environment as outlined in `or_suite/envs/ambulance/ambulance_metric.py`.  The package has default specifications for all of the environments in the file `or_suite/envs/env_configs.py`, and so we use one the default for the ambulance problem in a metric space.\n",
    "\n",
    "In addition, we need to specify the number of episodes for learning, and the number of iterations (in order to plot average results with confidence intervals)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "CONFIG =  or_suite.envs.env_configs.rideshare_graph_default_config\n",
    "\n",
    "epLen = CONFIG['epLen']\n",
    "nEps = 100\n",
    "numIters = 2\n",
    "\n",
    "np.random.seed(10000)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Step 3: Pick simulation parameters\n",
    "\n",
    "Next we need to specify parameters for the simulation. This includes setting a seed, the frequency to record the metrics, directory path for saving the data files, a deBug mode which prints the trajectory, etc."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "DEFAULT_SETTINGS = {'seed': 1, \n",
    "                    'recFreq': 1, \n",
    "                    'dirPath': '../data/rideshare/', \n",
    "                    'deBug': True, \n",
    "                    'nEps': nEps, \n",
    "                    'numIters': numIters, \n",
    "                    'saveTrajectory': False, \n",
    "                    'epLen' : epLen,\n",
    "                    'render': False,\n",
    "                    'pickle': False\n",
    "                    }\n",
    "\n",
    "starting_state = CONFIG['starting_state']\n",
    "num_cars = CONFIG['num_cars']\n",
    "num_nodes = len(starting_state)\n",
    "\n",
    "rideshare_env = gym.make('Rideshare-v1', config=CONFIG)\n",
    "mon_env = Monitor(rideshare_env)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Step 4: Pick list of algorithms\n",
    "\n",
    "We have several heuristics implemented for each of the environments defined, in addition to a `Random` policy, and some `RL discretization based` algorithms. \n",
    "\n",
    "The `Stable` agent only moves ambulances when responding to an incoming call and not in between calls. This means the policy $\\pi$ chosen by the agent for any given state $X$ will be $\\pi_h(X) = X$\n",
    "\n",
    "The `Median` agent takes a list of all past call arrivals sorted by arrival location, and partitions it into $k$ quantiles where $k$ is the number of ambulances. The algorithm then selects the middle data point in each quantile as the locations to station the ambulances."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "agents = { #'SB PPO': PPO(MlpPolicy, mon_env, gamma=1, verbose=0, n_steps=epLen),\n",
    "#'Random': or_suite.agents.rl.random.randomAgent(),\n",
    "'maxweightfixed' : or_suite.agents.rideshare.max_weight_fixed.maxWeightFixedAgent(CONFIG['epLen'], CONFIG, [1 for _ in range(num_nodes)]),\n",
    "'closestcar' : or_suite.agents.rideshare.closest_car.closetCarAgent(CONFIG['epLen'], CONFIG),\n",
    "'randomcar' : or_suite.agents.rideshare.random_car.randomCarAgent(CONFIG['epLen'], CONFIG)\n",
    "}"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'epLen'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/l5/43sgrxps7lbcv1k2xnpw4g5h0000gn/T/ipykernel_77665/394776022.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m agents = { #'SB PPO': PPO(MlpPolicy, mon_env, gamma=1, verbose=0, n_steps=epLen),\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#'Random': or_suite.agents.rl.random.randomAgent(),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;34m'maxweightfixed'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mor_suite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrideshare\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_weight_fixed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxWeightFixedAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_config\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mCONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epLen'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_nodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m'closestcar'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mor_suite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrideshare\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosest_car\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosetCarAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epLen'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCONFIG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m'randomcar'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mor_suite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrideshare\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_car\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandomCarAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epLen'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCONFIG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'epLen'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Step 5: Run Simulations\n",
    "\n",
    "Run the different heuristics in the environment"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "path_list_line = []\n",
    "algo_list_line = []\n",
    "path_list_radar = []\n",
    "algo_list_radar= []\n",
    "\n",
    "linspace_alpha = []\n",
    "\n",
    "param_list = [list(p) for p in it.product(np.linspace(0,1,4),repeat = len(starting_state))]\n",
    "\n",
    "for agent in agents:\n",
    "    print(agent)\n",
    "    DEFAULT_SETTINGS['dirPath'] = '../data/rideshare_'+str(agent)+'_'+str(num_cars)\n",
    "    if agent == 'max_weight_fixed':\n",
    "        or_suite.utils.run_single_algo_tune(rideshare_env,agents[agent], param_list, DEFAULT_SETTINGS)\n",
    "    elif agent == 'SB PPO':\n",
    "        or_suite.utils.run_single_sb_algo(mon_env, agents[agent], DEFAULT_SETTINGS)\n",
    "    else:\n",
    "        or_suite.utils.run_single_algo(rideshare_env, agents[agent], DEFAULT_SETTINGS)\n",
    "\n",
    "    path_list_line.append('../data/rideshare_'+str(agent)+'_'+str(num_cars))\n",
    "    algo_list_line.append(str(agent))\n",
    "    if agent != 'SB PPO':\n",
    "        path_list_radar.append('../data/rideshare_'+str(agent)+'_'+str(num_cars))\n",
    "        algo_list_radar.append(str(agent))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "maxweightfixed\n",
      "**************************************************\n",
      "Running experiment\n",
      "**************************************************\n",
      "Episode : 0\n",
      "state : [1 2 3 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "action : 3\n",
      "new state: [1 2 3 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [1 2 3 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "action : 3\n",
      "new state: [1 2 3 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [1 2 3 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "action : 3\n",
      "new state: [1 2 3 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "reward: 0.25\n",
      "epReward so far: -1.25\n",
      "state : [1 2 3 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "action : 2\n",
      "new state: [1 2 3 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "state : [1 2 3 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "action : 2\n",
      "new state: [1 3 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "reward: -0.75\n",
      "epReward so far: -2.75\n",
      "final state: [1 3 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "Episode : 1\n",
      "state : [1 3 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "action : 1\n",
      "new state: [1 3 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [1 3 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "action : 1\n",
      "new state: [1 3 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [1 3 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "action : 1\n",
      "new state: [1 3 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "reward: -0.75\n",
      "epReward so far: -2.25\n",
      "state : [1 3 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "action : 1\n",
      "new state: [1 3 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: -3.0\n",
      "state : [1 3 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "action : 1\n",
      "new state: [1 3 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "reward: 0.0\n",
      "epReward so far: -3.0\n",
      "final state: [1 3 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "Episode : 2\n",
      "state : [1 3 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "action : 1\n",
      "new state: [1 2 3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [1 2 3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "action : 2\n",
      "new state: [1 2 3 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [1 2 3 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "action : 2\n",
      "new state: [1 2 4 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "reward: -0.0\n",
      "epReward so far: -1.25\n",
      "state : [1 2 4 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "action : 2\n",
      "new state: [1 2 4 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "state : [1 2 4 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "action : 2\n",
      "new state: [1 2 4 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "reward: -0.75\n",
      "epReward so far: -2.75\n",
      "final state: [1 2 4 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "Episode : 3\n",
      "state : [1 2 4 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "action : 2\n",
      "new state: [1 2 4 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [1 2 4 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "action : 2\n",
      "new state: [1 2 3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "reward: -0.5\n",
      "epReward so far: -1.25\n",
      "state : [1 2 3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "action : 2\n",
      "new state: [1 2 2 3 2 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 3]\n",
      "reward: -0.5\n",
      "epReward so far: -1.75\n",
      "state : [1 2 2 3 2 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 3]\n",
      "action : 3\n",
      "new state: [1 2 3 2 3 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "reward: -0.5\n",
      "epReward so far: -2.25\n",
      "state : [1 2 3 2 3 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "action : 2\n",
      "new state: [1 2 3 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "reward: -0.75\n",
      "epReward so far: -3.0\n",
      "final state: [1 2 3 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "Episode : 4\n",
      "state : [1 2 3 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "action : 2\n",
      "new state: [1 2 2 4 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [1 2 2 4 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "action : 3\n",
      "new state: [1 2 2 4 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [1 2 2 4 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 3\n",
      "new state: [1 2 3 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "reward: 0.0\n",
      "epReward so far: -1.25\n",
      "state : [1 2 3 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "action : 3\n",
      "new state: [1 2 3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: -0.5\n",
      "epReward so far: -1.75\n",
      "state : [1 2 3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 2\n",
      "new state: [1 2 3 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: -0.75\n",
      "epReward so far: -2.5\n",
      "final state: [1 2 3 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "Episode : 5\n",
      "state : [1 2 3 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 2\n",
      "new state: [1 2 4 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [1 2 4 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "action : 2\n",
      "new state: [1 2 4 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [1 2 4 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "action : 2\n",
      "new state: [1 2 4 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "reward: 0.0\n",
      "epReward so far: -1.5\n",
      "state : [1 2 4 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "action : 2\n",
      "new state: [1 2 3 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "reward: -0.75\n",
      "epReward so far: -2.25\n",
      "state : [1 2 3 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "action : 3\n",
      "new state: [1 2 3 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: 0.25\n",
      "epReward so far: -2.0\n",
      "final state: [1 2 3 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "Episode : 6\n",
      "state : [1 2 3 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 2\n",
      "new state: [1 2 2 3 0 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [1 2 2 3 0 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "action : 3\n",
      "new state: [2 2 2 2 1 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.5\n",
      "state : [2 2 2 2 1 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 0]\n",
      "action : 0\n",
      "new state: [3 2 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.25\n",
      "state : [3 2 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "action : 0\n",
      "new state: [2 3 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: -0.5\n",
      "epReward so far: -0.75\n",
      "state : [2 3 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 1\n",
      "new state: [2 2 3 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "final state: [2 2 3 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "Episode : 7\n",
      "state : [2 2 3 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "action : 2\n",
      "new state: [2 2 3 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [2 2 3 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 2\n",
      "new state: [2 2 3 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [2 2 3 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "state : [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "action : 2\n",
      "new state: [2 2 2 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "reward: -0.5\n",
      "epReward so far: -2.5\n",
      "state : [2 2 2 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "action : 3\n",
      "new state: [3 2 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "reward: -0.75\n",
      "epReward so far: -3.25\n",
      "final state: [3 2 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "Episode : 8\n",
      "state : [3 2 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "action : 0\n",
      "new state: [2 3 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [2 3 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "action : 1\n",
      "new state: [2 3 2 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: -0.0\n",
      "epReward so far: -0.5\n",
      "state : [2 3 2 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 1\n",
      "new state: [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "action : 1\n",
      "new state: [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "state : [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "action : 1\n",
      "new state: [2 2 3 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "reward: 0.25\n",
      "epReward so far: -1.75\n",
      "final state: [2 2 3 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "Episode : 9\n",
      "state : [2 2 3 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "action : 2\n",
      "new state: [2 2 3 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 2 3 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 2\n",
      "new state: [3 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [3 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "action : 0\n",
      "new state: [2 2 3 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: 0.25\n",
      "epReward so far: -1.25\n",
      "state : [2 2 3 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 2\n",
      "new state: [2 2 3 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: -1.25\n",
      "state : [2 2 3 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 2\n",
      "new state: [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "final state: [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "Episode : 10\n",
      "state : [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "action : 1\n",
      "new state: [2 2 3 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [2 2 3 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 2\n",
      "new state: [2 2 2 2 2 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "reward: -0.5\n",
      "epReward so far: -0.25\n",
      "state : [2 2 2 2 2 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "action : 0\n",
      "new state: [1 2 3 2 2 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "reward: -0.5\n",
      "epReward so far: -0.75\n",
      "state : [1 2 3 2 2 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "action : 2\n",
      "new state: [1 2 4 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [1 2 4 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "action : 2\n",
      "new state: [1 2 4 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "reward: -0.5\n",
      "epReward so far: -2.0\n",
      "final state: [1 2 4 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "Episode : 11\n",
      "state : [1 2 4 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "action : 2\n",
      "new state: [1 2 4 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [1 2 4 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 2\n",
      "new state: [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "action : 1\n",
      "new state: [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "reward: -0.75\n",
      "epReward so far: -2.25\n",
      "state : [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "action : 1\n",
      "new state: [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "reward: -0.0\n",
      "epReward so far: -2.25\n",
      "state : [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "action : 1\n",
      "new state: [2 2 3 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: -0.5\n",
      "epReward so far: -2.75\n",
      "final state: [2 2 3 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "Episode : 12\n",
      "state : [2 2 3 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 2\n",
      "new state: [2 2 3 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 2 3 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "action : 2\n",
      "new state: [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "action : 2\n",
      "new state: [2 2 2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: -0.5\n",
      "epReward so far: -2.0\n",
      "state : [2 2 2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 3\n",
      "new state: [2 2 2 2 3 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 3]\n",
      "reward: 0.25\n",
      "epReward so far: -1.75\n",
      "state : [2 2 2 2 3 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 3]\n",
      "action : 0\n",
      "new state: [2 2 2 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "reward: -0.75\n",
      "epReward so far: -2.5\n",
      "final state: [2 2 2 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "Episode : 13\n",
      "state : [2 2 2 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "action : 3\n",
      "new state: [2 3 2 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [2 3 2 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 1\n",
      "new state: [2 2 3 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [2 2 3 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "action : 2\n",
      "new state: [2 3 2 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: -0.5\n",
      "epReward so far: -1.75\n",
      "state : [2 3 2 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 1\n",
      "new state: [2 3 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: -0.0\n",
      "epReward so far: -1.75\n",
      "state : [2 3 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 1\n",
      "new state: [2 2 2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "reward: -0.5\n",
      "epReward so far: -2.25\n",
      "final state: [2 2 2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "Episode : 14\n",
      "state : [2 2 2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "action : 3\n",
      "new state: [2 2 2 2 3 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 3]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [2 2 2 2 3 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 3]\n",
      "action : 0\n",
      "new state: [2 2 2 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [2 2 2 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 3\n",
      "new state: [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "state : [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "action : 1\n",
      "new state: [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "reward: -0.75\n",
      "epReward so far: -2.75\n",
      "state : [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "action : 1\n",
      "new state: [2 2 2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: -0.5\n",
      "epReward so far: -3.25\n",
      "final state: [2 2 2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "Episode : 15\n",
      "state : [2 2 2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 3\n",
      "new state: [2 2 2 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 2 2 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 3\n",
      "new state: [2 2 2 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: -0.5\n",
      "epReward so far: -1.25\n",
      "state : [2 2 2 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 3\n",
      "new state: [2 2 2 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "state : [2 2 2 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 3\n",
      "new state: [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "reward: 0.0\n",
      "epReward so far: -2.0\n",
      "state : [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "action : 0\n",
      "new state: [2 2 2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: -0.5\n",
      "epReward so far: -2.5\n",
      "final state: [2 2 2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "Episode : 16\n",
      "state : [2 2 2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 3\n",
      "new state: [2 2 2 2 3 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [2 2 2 2 3 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "action : 0\n",
      "new state: [1 2 2 3 0 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 3]\n",
      "reward: -0.5\n",
      "epReward so far: -0.25\n",
      "state : [1 2 2 3 0 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 3]\n",
      "action : 3\n",
      "new state: [1 2 3 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "reward: 0.0\n",
      "epReward so far: -0.25\n",
      "state : [1 2 3 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.0\n",
      "state : [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "action : 2\n",
      "new state: [2 2 2 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "reward: -0.5\n",
      "epReward so far: -1.5\n",
      "final state: [2 2 2 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "Episode : 17\n",
      "state : [2 2 2 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "action : 3\n",
      "new state: [2 2 2 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 2 2 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 3\n",
      "new state: [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "action : 1\n",
      "new state: [2 2 3 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "reward: -0.5\n",
      "epReward so far: -2.0\n",
      "state : [2 2 3 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "action : 2\n",
      "new state: [3 2 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: -0.75\n",
      "epReward so far: -2.75\n",
      "state : [3 2 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 0\n",
      "new state: [3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "reward: -0.75\n",
      "epReward so far: -3.5\n",
      "final state: [3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "Episode : 18\n",
      "state : [3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "action : 0\n",
      "new state: [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "action : 1\n",
      "new state: [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "reward: -0.0\n",
      "epReward so far: -0.75\n",
      "state : [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "action : 1\n",
      "new state: [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "action : 1\n",
      "new state: [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "reward: -0.75\n",
      "epReward so far: -2.25\n",
      "state : [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "action : 1\n",
      "new state: [2 2 3 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "reward: -0.5\n",
      "epReward so far: -2.75\n",
      "final state: [2 2 3 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "Episode : 19\n",
      "state : [2 2 3 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "action : 2\n",
      "new state: [2 2 3 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 2 3 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 2\n",
      "new state: [2 2 2 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: -0.5\n",
      "epReward so far: -1.25\n",
      "state : [2 2 2 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 3\n",
      "new state: [2 2 2 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: -0.0\n",
      "epReward so far: -1.25\n",
      "state : [2 2 2 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 3\n",
      "new state: [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "state : [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "action : 2\n",
      "new state: [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "reward: -0.75\n",
      "epReward so far: -2.75\n",
      "final state: [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "Episode : 20\n",
      "state : [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "action : 0\n",
      "new state: [2 2 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 2 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "action : 3\n",
      "new state: [2 2 2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "reward: -0.5\n",
      "epReward so far: -1.25\n",
      "state : [2 2 2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "action : 3\n",
      "new state: [2 2 2 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "state : [2 2 2 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 3\n",
      "new state: [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "reward: -0.75\n",
      "epReward so far: -2.75\n",
      "state : [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "action : 2\n",
      "new state: [2 2 2 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "reward: -0.5\n",
      "epReward so far: -3.25\n",
      "final state: [2 2 2 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "Episode : 21\n",
      "state : [2 2 2 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "action : 3\n",
      "new state: [2 2 2 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 2 2 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 3\n",
      "new state: [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "action : 2\n",
      "new state: [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "reward: -0.75\n",
      "epReward so far: -2.25\n",
      "state : [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "reward: 0.0\n",
      "epReward so far: -2.25\n",
      "state : [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "action : 2\n",
      "new state: [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "reward: -0.75\n",
      "epReward so far: -3.0\n",
      "final state: [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "Episode : 22\n",
      "state : [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "action : 1\n",
      "new state: [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "action : 1\n",
      "new state: [2 2 2 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: 0.25\n",
      "epReward so far: -0.5\n",
      "state : [2 2 2 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 3\n",
      "new state: [2 2 2 2 0 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "reward: 0.25\n",
      "epReward so far: -0.25\n",
      "state : [2 2 2 2 0 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "action : 0\n",
      "new state: [2 2 2 2 0 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 3]\n",
      "reward: -0.5\n",
      "epReward so far: -0.75\n",
      "state : [2 2 2 2 0 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 3]\n",
      "action : 0\n",
      "new state: [1 3 2 2 0 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "reward: -0.5\n",
      "epReward so far: -1.25\n",
      "final state: [1 3 2 2 0 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "Episode : 23\n",
      "state : [1 3 2 2 0 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "action : 1\n",
      "new state: [2 3 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 3 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 1\n",
      "new state: [2 2 2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "reward: -0.5\n",
      "epReward so far: -1.25\n",
      "state : [2 2 2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "action : 3\n",
      "new state: [2 2 2 2 3 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0]\n",
      "reward: 0.25\n",
      "epReward so far: -1.0\n",
      "state : [2 2 2 2 3 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0]\n",
      "action : 0\n",
      "new state: [2 2 2 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: -0.75\n",
      "epReward so far: -1.75\n",
      "state : [2 2 2 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 3\n",
      "new state: [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "reward: -0.75\n",
      "epReward so far: -2.5\n",
      "final state: [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "Episode : 24\n",
      "state : [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "action : 0\n",
      "new state: [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "action : 0\n",
      "new state: [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "action : 1\n",
      "new state: [2 2 2 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: -0.5\n",
      "epReward so far: -2.0\n",
      "state : [2 2 2 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 3\n",
      "new state: [2 2 2 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "reward: -0.75\n",
      "epReward so far: -2.75\n",
      "state : [2 2 2 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "action : 3\n",
      "new state: [3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "reward: -0.75\n",
      "epReward so far: -3.5\n",
      "final state: [3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "Episode : 25\n",
      "state : [3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "action : 0\n",
      "new state: [3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "action : 0\n",
      "new state: [3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "action : 0\n",
      "new state: [2 4 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "reward: -0.75\n",
      "epReward so far: -2.25\n",
      "state : [2 4 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "action : 1\n",
      "new state: [2 3 2 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: -0.5\n",
      "epReward so far: -2.75\n",
      "state : [2 3 2 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 1\n",
      "new state: [2 3 2 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: -3.5\n",
      "final state: [2 3 2 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "Episode : 26\n",
      "state : [2 3 2 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 1\n",
      "new state: [3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "action : 0\n",
      "new state: [3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "action : 0\n",
      "new state: [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "action : 1\n",
      "new state: [2 2 2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "reward: 0.25\n",
      "epReward so far: -0.5\n",
      "state : [2 2 2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "action : 3\n",
      "new state: [2 2 2 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "final state: [2 2 2 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "Episode : 27\n",
      "state : [2 2 2 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 2 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "action : 3\n",
      "new state: [2 2 2 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "reward: -0.5\n",
      "epReward so far: -1.25\n",
      "state : [2 2 2 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "action : 3\n",
      "new state: [2 2 2 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: -0.0\n",
      "epReward so far: -1.25\n",
      "state : [2 2 2 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 3\n",
      "new state: [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "state : [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "action : 0\n",
      "new state: [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "reward: -0.75\n",
      "epReward so far: -2.75\n",
      "final state: [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "Episode : 28\n",
      "state : [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "action : 0\n",
      "new state: [2 2 2 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [2 2 2 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 3\n",
      "new state: [2 2 2 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: 0.0\n",
      "epReward so far: -0.5\n",
      "state : [2 2 2 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 3\n",
      "new state: [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "action : 0\n",
      "new state: [2 2 2 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: -0.5\n",
      "epReward so far: -1.75\n",
      "state : [2 2 2 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 3\n",
      "new state: [2 2 2 2 0 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 1]\n",
      "reward: -0.5\n",
      "epReward so far: -2.25\n",
      "final state: [2 2 2 2 0 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 1]\n",
      "Episode : 29\n",
      "state : [2 2 2 2 0 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 1]\n",
      "action : 0\n",
      "new state: [2 2 2 2 1 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [2 2 2 2 1 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2]\n",
      "action : 0\n",
      "new state: [2 2 3 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: -0.75\n",
      "epReward so far: -0.5\n",
      "state : [2 2 3 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 2\n",
      "new state: [2 3 2 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: 0.25\n",
      "epReward so far: -0.25\n",
      "state : [2 3 2 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 1\n",
      "new state: [2 3 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: -0.75\n",
      "epReward so far: -1.0\n",
      "state : [2 3 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 1\n",
      "new state: [2 2 2 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: -0.5\n",
      "epReward so far: -1.5\n",
      "final state: [2 2 2 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "Episode : 30\n",
      "state : [2 2 2 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 3\n",
      "new state: [2 2 2 2 2 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 3]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [2 2 2 2 2 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 3]\n",
      "action : 0\n",
      "new state: [1 2 3 2 3 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 3]\n",
      "reward: -0.5\n",
      "epReward so far: -1.0\n",
      "state : [1 2 3 2 3 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 3]\n",
      "action : 2\n",
      "new state: [2 2 3 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.75\n",
      "state : [2 2 3 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "action : 2\n",
      "new state: [2 2 2 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "reward: 0.25\n",
      "epReward so far: -1.5\n",
      "state : [2 2 2 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "action : 3\n",
      "new state: [2 2 2 2 1 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1]\n",
      "reward: -0.5\n",
      "epReward so far: -2.0\n",
      "final state: [2 2 2 2 1 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1]\n",
      "Episode : 31\n",
      "state : [2 2 2 2 1 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1]\n",
      "action : 0\n",
      "new state: [1 4 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [1 4 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "action : 1\n",
      "new state: [1 4 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [1 4 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "action : 1\n",
      "new state: [1 4 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "reward: -0.75\n",
      "epReward so far: -2.25\n",
      "state : [1 4 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "action : 1\n",
      "new state: [1 3 2 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "reward: -0.5\n",
      "epReward so far: -2.75\n",
      "state : [1 3 2 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "action : 1\n",
      "new state: [1 3 2 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "reward: -0.75\n",
      "epReward so far: -3.5\n",
      "final state: [1 3 2 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "Episode : 32\n",
      "state : [1 3 2 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "action : 1\n",
      "new state: [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "action : 1\n",
      "new state: [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "action : 1\n",
      "new state: [2 2 2 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "reward: -0.5\n",
      "epReward so far: -2.0\n",
      "state : [2 2 2 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "action : 3\n",
      "new state: [2 2 2 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: -0.75\n",
      "epReward so far: -2.75\n",
      "state : [2 2 2 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 3\n",
      "new state: [3 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: -0.5\n",
      "epReward so far: -3.25\n",
      "final state: [3 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "Episode : 33\n",
      "state : [3 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 0\n",
      "new state: [3 2 2 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [3 2 2 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 0\n",
      "new state: [2 2 3 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "reward: 0.25\n",
      "epReward so far: -0.5\n",
      "state : [2 2 3 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "action : 2\n",
      "new state: [2 2 2 2 3 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 0]\n",
      "reward: -0.5\n",
      "epReward so far: -1.0\n",
      "state : [2 2 2 2 3 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 0]\n",
      "action : 0\n",
      "new state: [2 2 2 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.75\n",
      "state : [2 2 2 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 3\n",
      "new state: [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "reward: -0.0\n",
      "epReward so far: -1.75\n",
      "final state: [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "Episode : 34\n",
      "state : [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "action : 0\n",
      "new state: [2 2 2 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [2 2 2 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 3\n",
      "new state: [2 2 2 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: -0.5\n",
      "state : [2 2 2 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 3\n",
      "new state: [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "action : 0\n",
      "new state: [2 2 2 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "reward: -0.5\n",
      "epReward so far: -1.75\n",
      "state : [2 2 2 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "action : 3\n",
      "new state: [2 2 2 2 0 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "reward: -0.5\n",
      "epReward so far: -2.25\n",
      "final state: [2 2 2 2 0 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "Episode : 35\n",
      "state : [2 2 2 2 0 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "action : 0\n",
      "new state: [2 2 2 2 1 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [2 2 2 2 1 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1]\n",
      "action : 0\n",
      "new state: [2 3 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [2 3 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 1\n",
      "new state: [2 4 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "reward: 0.0\n",
      "epReward so far: -1.25\n",
      "state : [2 4 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "action : 1\n",
      "new state: [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "state : [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "action : 1\n",
      "new state: [2 2 2 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: -0.5\n",
      "epReward so far: -2.5\n",
      "final state: [2 2 2 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "Episode : 36\n",
      "state : [2 2 2 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 3\n",
      "new state: [2 2 2 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: 0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 2 2 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 3\n",
      "new state: [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "action : 2\n",
      "new state: [2 2 2 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [2 2 2 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 3\n",
      "new state: [2 2 3 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [2 2 3 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "action : 2\n",
      "new state: [4 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "final state: [4 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "Episode : 37\n",
      "state : [4 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "action : 0\n",
      "new state: [4 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [4 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "action : 0\n",
      "new state: [4 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [4 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "action : 0\n",
      "new state: [4 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "reward: -0.75\n",
      "epReward so far: -2.25\n",
      "state : [4 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "action : 0\n",
      "new state: [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "reward: -0.75\n",
      "epReward so far: -3.0\n",
      "state : [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "action : 0\n",
      "new state: [2 2 2 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: 0.25\n",
      "epReward so far: -2.75\n",
      "final state: [2 2 2 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "Episode : 38\n",
      "state : [2 2 2 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 3\n",
      "new state: [2 2 2 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "reward: 0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 2 2 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "action : 3\n",
      "new state: [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "action : 2\n",
      "new state: [2 2 2 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: -0.5\n",
      "epReward so far: -1.25\n",
      "state : [2 2 2 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 3\n",
      "new state: [2 2 2 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "reward: 0.0\n",
      "epReward so far: -1.25\n",
      "final state: [2 2 2 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "Episode : 39\n",
      "state : [2 2 2 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "action : 3\n",
      "new state: [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "reward: 0.0\n",
      "epReward so far: -0.75\n",
      "state : [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "action : 2\n",
      "new state: [2 2 2 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "reward: 0.25\n",
      "epReward so far: -0.5\n",
      "state : [2 2 2 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "action : 3\n",
      "new state: [2 2 2 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [2 2 2 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 3\n",
      "new state: [3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "final state: [3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "Episode : 40\n",
      "state : [3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "action : 0\n",
      "new state: [2 3 2 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [2 3 2 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 1\n",
      "new state: [2 3 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [2 3 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 1\n",
      "new state: [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "state : [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "action : 1\n",
      "new state: [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: -2.75\n",
      "state : [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "action : 1\n",
      "new state: [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "reward: 0.0\n",
      "epReward so far: -2.75\n",
      "final state: [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "Episode : 41\n",
      "state : [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "action : 1\n",
      "new state: [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "action : 1\n",
      "new state: [2 2 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [2 2 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "action : 3\n",
      "new state: [2 2 2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "reward: -0.5\n",
      "epReward so far: -2.0\n",
      "state : [2 2 2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "action : 3\n",
      "new state: [3 2 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: -0.75\n",
      "epReward so far: -2.75\n",
      "state : [3 2 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 0\n",
      "new state: [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "reward: -0.0\n",
      "epReward so far: -2.75\n",
      "final state: [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "Episode : 42\n",
      "state : [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "action : 0\n",
      "new state: [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "action : 0\n",
      "new state: [2 2 2 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "reward: 0.25\n",
      "epReward so far: -0.5\n",
      "state : [2 2 2 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "action : 3\n",
      "new state: [2 2 2 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [2 2 2 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "action : 3\n",
      "new state: [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "state : [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "action : 2\n",
      "new state: [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "reward: -0.75\n",
      "epReward so far: -2.75\n",
      "final state: [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "Episode : 43\n",
      "state : [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "reward: 0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "action : 2\n",
      "new state: [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "action : 2\n",
      "new state: [2 2 2 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: -0.5\n",
      "epReward so far: -1.25\n",
      "state : [2 2 2 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 3\n",
      "new state: [2 2 2 2 1 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "reward: -0.5\n",
      "epReward so far: -1.75\n",
      "state : [2 2 2 2 1 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "action : 0\n",
      "new state: [1 3 2 2 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 1]\n",
      "reward: -0.5\n",
      "epReward so far: -2.25\n",
      "final state: [1 3 2 2 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 1]\n",
      "Episode : 44\n",
      "state : [1 3 2 2 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 1]\n",
      "action : 1\n",
      "new state: [1 2 2 3 2 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 2]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [1 2 2 3 2 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 2]\n",
      "action : 3\n",
      "new state: [1 2 3 2 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "reward: 0.25\n",
      "epReward so far: -0.25\n",
      "state : [1 2 3 2 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "action : 2\n",
      "new state: [1 3 3 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "reward: -0.0\n",
      "epReward so far: -0.25\n",
      "state : [1 3 3 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "action : 1\n",
      "new state: [1 3 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "reward: -0.0\n",
      "epReward so far: -0.25\n",
      "state : [1 3 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "action : 2\n",
      "new state: [1 3 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.0\n",
      "final state: [1 3 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "Episode : 45\n",
      "state : [1 3 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "action : 2\n",
      "new state: [1 3 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [1 3 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "action : 2\n",
      "new state: [1 3 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "reward: -0.0\n",
      "epReward so far: -0.75\n",
      "state : [1 3 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "action : 2\n",
      "new state: [1 3 3 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: -0.5\n",
      "epReward so far: -1.25\n",
      "state : [1 3 3 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 1\n",
      "new state: [1 2 3 2 1 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "reward: -0.5\n",
      "epReward so far: -1.75\n",
      "state : [1 2 3 2 1 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "action : 2\n",
      "new state: [1 3 2 2 0 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 3]\n",
      "reward: 0.25\n",
      "epReward so far: -1.5\n",
      "final state: [1 3 2 2 0 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 3]\n",
      "Episode : 46\n",
      "state : [1 3 2 2 0 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 3]\n",
      "action : 1\n",
      "new state: [1 4 2 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [1 4 2 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "action : 1\n",
      "new state: [2 4 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [2 4 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "action : 1\n",
      "new state: [2 4 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "reward: -0.0\n",
      "epReward so far: -1.5\n",
      "state : [2 4 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "action : 1\n",
      "new state: [2 3 2 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "reward: -0.5\n",
      "epReward so far: -2.0\n",
      "state : [2 3 2 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "action : 1\n",
      "new state: [3 2 2 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "reward: -0.75\n",
      "epReward so far: -2.75\n",
      "final state: [3 2 2 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "Episode : 47\n",
      "state : [3 2 2 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "action : 0\n",
      "new state: [4 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [4 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "action : 0\n",
      "new state: [4 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [4 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "action : 0\n",
      "new state: [3 2 2 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "reward: -0.5\n",
      "epReward so far: -1.25\n",
      "state : [3 2 2 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "action : 0\n",
      "new state: [2 2 2 2 0 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 3]\n",
      "reward: 0.25\n",
      "epReward so far: -1.0\n",
      "state : [2 2 2 2 0 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 3]\n",
      "action : 0\n",
      "new state: [3 2 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "reward: -0.75\n",
      "epReward so far: -1.75\n",
      "final state: [3 2 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "Episode : 48\n",
      "state : [3 2 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "action : 0\n",
      "new state: [3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "action : 0\n",
      "new state: [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "action : 1\n",
      "new state: [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "reward: -0.75\n",
      "epReward so far: -2.25\n",
      "state : [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "action : 1\n",
      "new state: [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "reward: -0.75\n",
      "epReward so far: -3.0\n",
      "state : [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "action : 1\n",
      "new state: [2 2 3 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "reward: 0.25\n",
      "epReward so far: -2.75\n",
      "final state: [2 2 3 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "Episode : 49\n",
      "state : [2 2 3 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "action : 2\n",
      "new state: [2 2 2 2 2 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [2 2 2 2 2 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "action : 0\n",
      "new state: [1 2 3 2 2 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0]\n",
      "reward: 0.25\n",
      "epReward so far: -0.25\n",
      "state : [1 2 3 2 2 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0]\n",
      "action : 2\n",
      "new state: [2 2 3 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.0\n",
      "state : [2 2 3 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 2\n",
      "new state: [2 2 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "reward: -0.0\n",
      "epReward so far: -1.0\n",
      "state : [2 2 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "action : 2\n",
      "new state: [3 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "reward: -0.75\n",
      "epReward so far: -1.75\n",
      "final state: [3 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "Episode : 50\n",
      "state : [3 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "action : 0\n",
      "new state: [3 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [3 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "action : 0\n",
      "new state: [2 2 3 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "reward: 0.25\n",
      "epReward so far: -0.5\n",
      "state : [2 2 3 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "action : 2\n",
      "new state: [2 2 2 2 1 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "reward: 0.25\n",
      "epReward so far: -0.25\n",
      "state : [2 2 2 2 1 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "action : 0\n",
      "new state: [1 3 2 2 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 1]\n",
      "reward: -0.5\n",
      "epReward so far: -0.75\n",
      "state : [1 3 2 2 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 1]\n",
      "action : 1\n",
      "new state: [1 3 2 2 2 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 3]\n",
      "reward: -0.5\n",
      "epReward so far: -1.25\n",
      "final state: [1 3 2 2 2 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 3]\n",
      "Episode : 51\n",
      "state : [1 3 2 2 2 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 3]\n",
      "action : 1\n",
      "new state: [1 2 3 2 3 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [1 2 3 2 3 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "action : 2\n",
      "new state: [1 3 3 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: -0.75\n",
      "epReward so far: -0.5\n",
      "state : [1 3 3 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 1\n",
      "new state: [1 2 3 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: -0.5\n",
      "epReward so far: -1.0\n",
      "state : [1 2 3 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 2\n",
      "new state: [1 2 2 3 3 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "reward: 0.25\n",
      "epReward so far: -0.75\n",
      "state : [1 2 2 3 3 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "action : 3\n",
      "new state: [1 2 2 3 2 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "reward: -0.5\n",
      "epReward so far: -1.25\n",
      "final state: [1 2 2 3 2 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "Episode : 52\n",
      "state : [1 2 2 3 2 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "action : 3\n",
      "new state: [3 2 2 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [3 2 2 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 0\n",
      "new state: [3 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "reward: -0.0\n",
      "epReward so far: -0.75\n",
      "state : [3 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "action : 0\n",
      "new state: [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "action : 1\n",
      "new state: [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "reward: -0.75\n",
      "epReward so far: -2.25\n",
      "state : [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "action : 1\n",
      "new state: [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "reward: -0.75\n",
      "epReward so far: -3.0\n",
      "final state: [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Episode : 53\n",
      "state : [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "action : 1\n",
      "new state: [3 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [3 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "action : 0\n",
      "new state: [2 2 3 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: -0.5\n",
      "epReward so far: -1.25\n",
      "state : [2 2 3 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 2\n",
      "new state: [2 2 3 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: -0.0\n",
      "epReward so far: -1.25\n",
      "state : [2 2 3 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 2\n",
      "new state: [2 4 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "state : [2 4 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "action : 1\n",
      "new state: [2 4 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "reward: -0.75\n",
      "epReward so far: -2.75\n",
      "final state: [2 4 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "Episode : 54\n",
      "state : [2 4 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "action : 1\n",
      "new state: [2 4 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 4 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "action : 1\n",
      "new state: [2 4 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [2 4 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "action : 1\n",
      "new state: [2 4 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "reward: 0.0\n",
      "epReward so far: -1.5\n",
      "state : [2 4 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "action : 1\n",
      "new state: [2 4 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "reward: -0.0\n",
      "epReward so far: -1.5\n",
      "state : [2 4 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "action : 1\n",
      "new state: [2 3 2 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: -0.5\n",
      "epReward so far: -2.0\n",
      "final state: [2 3 2 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "Episode : 55\n",
      "state : [2 3 2 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 1\n",
      "new state: [2 3 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 3 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 1\n",
      "new state: [2 2 2 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "reward: -0.5\n",
      "epReward so far: -1.25\n",
      "state : [2 2 2 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "action : 3\n",
      "new state: [2 2 2 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "state : [2 2 2 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "action : 3\n",
      "new state: [2 2 3 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: -0.5\n",
      "epReward so far: -2.5\n",
      "state : [2 2 3 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 2\n",
      "new state: [2 2 2 2 1 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1]\n",
      "reward: -0.5\n",
      "epReward so far: -3.0\n",
      "final state: [2 2 2 2 1 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1]\n",
      "Episode : 56\n",
      "state : [2 2 2 2 1 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1]\n",
      "action : 0\n",
      "new state: [2 3 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 3 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 1\n",
      "new state: [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "action : 2\n",
      "new state: [2 2 2 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: 0.25\n",
      "epReward so far: -1.25\n",
      "state : [2 2 2 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 3\n",
      "new state: [2 2 2 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "state : [2 2 2 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 3\n",
      "new state: [2 3 2 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: -0.5\n",
      "epReward so far: -2.5\n",
      "final state: [2 3 2 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "Episode : 57\n",
      "state : [2 3 2 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 1\n",
      "new state: [2 3 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 3 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 1\n",
      "new state: [2 2 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [2 2 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "action : 3\n",
      "new state: [2 2 2 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: -0.5\n",
      "epReward so far: -2.0\n",
      "state : [2 2 2 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 3\n",
      "new state: [2 2 2 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: -0.75\n",
      "epReward so far: -2.75\n",
      "state : [2 2 2 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 3\n",
      "new state: [2 2 3 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: -0.5\n",
      "epReward so far: -3.25\n",
      "final state: [2 2 3 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "Episode : 58\n",
      "state : [2 2 3 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 2\n",
      "new state: [2 3 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 3 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 1\n",
      "new state: [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "action : 2\n",
      "new state: [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "reward: -0.0\n",
      "epReward so far: -1.5\n",
      "state : [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "action : 2\n",
      "new state: [2 2 2 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: -0.5\n",
      "epReward so far: -2.0\n",
      "state : [2 2 2 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 3\n",
      "new state: [2 2 2 2 2 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 3]\n",
      "reward: 0.25\n",
      "epReward so far: -1.75\n",
      "final state: [2 2 2 2 2 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 3]\n",
      "Episode : 59\n",
      "state : [2 2 2 2 2 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 3]\n",
      "action : 0\n",
      "new state: [2 2 3 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 2 3 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 2\n",
      "new state: [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "action : 1\n",
      "new state: [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "reward: -0.75\n",
      "epReward so far: -2.25\n",
      "state : [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "action : 1\n",
      "new state: [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "reward: -0.75\n",
      "epReward so far: -3.0\n",
      "state : [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "action : 1\n",
      "new state: [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "reward: -0.75\n",
      "epReward so far: -3.75\n",
      "final state: [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "Episode : 60\n",
      "state : [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "action : 1\n",
      "new state: [2 2 2 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [2 2 2 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 3\n",
      "new state: [2 2 2 2 0 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 3]\n",
      "reward: 0.25\n",
      "epReward so far: -0.25\n",
      "state : [2 2 2 2 0 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 3]\n",
      "action : 0\n",
      "new state: [3 2 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.0\n",
      "state : [3 2 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 0\n",
      "new state: [2 3 2 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: -0.5\n",
      "epReward so far: -1.5\n",
      "state : [2 3 2 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 1\n",
      "new state: [2 3 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: -0.75\n",
      "epReward so far: -2.25\n",
      "final state: [2 3 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "Episode : 61\n",
      "state : [2 3 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 1\n",
      "new state: [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "action : 1\n",
      "new state: [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "reward: 0.0\n",
      "epReward so far: -0.75\n",
      "state : [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "action : 1\n",
      "new state: [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "action : 1\n",
      "new state: [2 2 2 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "reward: 0.25\n",
      "epReward so far: -1.25\n",
      "state : [2 2 2 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "action : 3\n",
      "new state: [2 2 2 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "final state: [2 2 2 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "Episode : 62\n",
      "state : [2 2 2 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "action : 3\n",
      "new state: [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "action : 0\n",
      "new state: [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "action : 0\n",
      "new state: [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "reward: -0.0\n",
      "epReward so far: -1.5\n",
      "state : [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "action : 0\n",
      "new state: [2 2 2 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: -0.5\n",
      "epReward so far: -2.0\n",
      "state : [2 2 2 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 3\n",
      "new state: [2 2 2 2 1 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 3]\n",
      "reward: -0.5\n",
      "epReward so far: -2.5\n",
      "final state: [2 2 2 2 1 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 3]\n",
      "Episode : 63\n",
      "state : [2 2 2 2 1 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 3]\n",
      "action : 0\n",
      "new state: [2 3 2 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 3 2 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 1\n",
      "new state: [2 2 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [2 2 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "action : 2\n",
      "new state: [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "reward: -0.75\n",
      "epReward so far: -2.25\n",
      "state : [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "action : 2\n",
      "new state: [2 2 2 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: -0.5\n",
      "epReward so far: -2.75\n",
      "state : [2 2 2 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 3\n",
      "new state: [2 2 2 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: 0.0\n",
      "epReward so far: -2.75\n",
      "final state: [2 2 2 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "Episode : 64\n",
      "state : [2 2 2 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 3\n",
      "new state: [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "action : 1\n",
      "new state: [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "action : 1\n",
      "new state: [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "reward: -0.75\n",
      "epReward so far: -2.25\n",
      "state : [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "action : 1\n",
      "new state: [2 2 2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: 0.25\n",
      "epReward so far: -2.0\n",
      "state : [2 2 2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 3\n",
      "new state: [2 2 2 2 3 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "reward: 0.25\n",
      "epReward so far: -1.75\n",
      "final state: [2 2 2 2 3 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "Episode : 65\n",
      "state : [2 2 2 2 3 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "action : 0\n",
      "new state: [2 2 2 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 2 2 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 3\n",
      "new state: [2 2 3 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: -0.5\n",
      "epReward so far: -1.25\n",
      "state : [2 2 3 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 2\n",
      "new state: [2 2 2 2 0 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "reward: -0.5\n",
      "epReward so far: -1.75\n",
      "state : [2 2 2 2 0 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "action : 0\n",
      "new state: [2 2 2 2 1 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 1]\n",
      "reward: -0.5\n",
      "epReward so far: -2.25\n",
      "state : [2 2 2 2 1 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 1]\n",
      "action : 0\n",
      "new state: [1 3 2 2 1 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "reward: -0.5\n",
      "epReward so far: -2.75\n",
      "final state: [1 3 2 2 1 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "Episode : 66\n",
      "state : [1 3 2 2 1 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "action : 1\n",
      "new state: [1 3 2 2 1 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 2]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [1 3 2 2 1 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 2]\n",
      "action : 1\n",
      "new state: [1 4 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [1 4 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 1\n",
      "new state: [1 5 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "reward: 0.0\n",
      "epReward so far: -1.25\n",
      "state : [1 5 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "action : 1\n",
      "new state: [1 4 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: 0.25\n",
      "epReward so far: -1.0\n",
      "state : [1 4 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 1\n",
      "new state: [1 3 2 2 2 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "reward: -0.5\n",
      "epReward so far: -1.5\n",
      "final state: [1 3 2 2 2 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "Episode : 67\n",
      "state : [1 3 2 2 2 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "action : 1\n",
      "new state: [2 2 3 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 2 3 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 2\n",
      "new state: [2 2 2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: 0.25\n",
      "epReward so far: -0.5\n",
      "state : [2 2 2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 3\n",
      "new state: [2 2 2 2 3 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "reward: -0.5\n",
      "epReward so far: -1.0\n",
      "state : [2 2 2 2 3 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "action : 0\n",
      "new state: [2 2 2 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.75\n",
      "state : [2 2 2 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 3\n",
      "new state: [3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "reward: -0.75\n",
      "epReward so far: -2.5\n",
      "final state: [3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "Episode : 68\n",
      "state : [3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "action : 0\n",
      "new state: [2 3 2 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [2 3 2 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "action : 1\n",
      "new state: [2 2 2 2 3 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 3]\n",
      "reward: -0.5\n",
      "epReward so far: -0.25\n",
      "state : [2 2 2 2 3 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 3]\n",
      "action : 0\n",
      "new state: [1 2 2 3 3 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.0\n",
      "state : [1 2 2 3 3 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "action : 3\n",
      "new state: [1 3 2 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 3 2 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 1\n",
      "new state: [1 3 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "final state: [1 3 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "Episode : 69\n",
      "state : [1 3 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "action : 3\n",
      "new state: [1 3 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [1 3 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "action : 3\n",
      "new state: [1 3 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [1 3 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "action : 1\n",
      "new state: [1 3 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "reward: -0.75\n",
      "epReward so far: -2.25\n",
      "state : [1 3 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "action : 1\n",
      "new state: [1 3 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "reward: -0.75\n",
      "epReward so far: -3.0\n",
      "state : [1 3 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "action : 1\n",
      "new state: [1 2 3 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: 0.25\n",
      "epReward so far: -2.75\n",
      "final state: [1 2 3 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "Episode : 70\n",
      "state : [1 2 3 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 2\n",
      "new state: [1 2 3 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [1 2 3 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 2\n",
      "new state: [1 2 2 4 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "reward: 0.25\n",
      "epReward so far: -0.5\n",
      "state : [1 2 2 4 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "action : 3\n",
      "new state: [1 2 2 4 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [1 2 2 4 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 3\n",
      "new state: [1 2 2 4 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "reward: -0.5\n",
      "epReward so far: -1.75\n",
      "state : [1 2 2 4 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "action : 3\n",
      "new state: [1 2 2 4 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "reward: -0.75\n",
      "epReward so far: -2.5\n",
      "final state: [1 2 2 4 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "Episode : 71\n",
      "state : [1 2 2 4 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "action : 3\n",
      "new state: [1 2 3 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [1 2 3 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "action : 3\n",
      "new state: [1 2 3 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [1 2 3 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "action : 3\n",
      "new state: [1 3 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "reward: -0.75\n",
      "epReward so far: -2.25\n",
      "state : [1 3 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "action : 1\n",
      "new state: [1 2 3 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "reward: -0.5\n",
      "epReward so far: -2.75\n",
      "state : [1 2 3 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "action : 2\n",
      "new state: [1 2 2 3 3 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "reward: -0.5\n",
      "epReward so far: -3.25\n",
      "final state: [1 2 2 3 3 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "Episode : 72\n",
      "state : [1 2 2 3 3 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "action : 3\n",
      "new state: [1 2 2 4 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [1 2 2 4 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 3\n",
      "new state: [1 2 3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: -0.5\n",
      "epReward so far: -1.25\n",
      "state : [1 2 3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 2\n",
      "new state: [1 2 2 3 2 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "reward: 0.25\n",
      "epReward so far: -1.0\n",
      "state : [1 2 2 3 2 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "action : 3\n",
      "new state: [1 2 3 2 2 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 0]\n",
      "reward: -0.5\n",
      "epReward so far: -1.5\n",
      "state : [1 2 3 2 2 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 0]\n",
      "action : 2\n",
      "new state: [2 2 2 2 2 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "reward: -0.5\n",
      "epReward so far: -2.0\n",
      "final state: [2 2 2 2 2 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "Episode : 73\n",
      "state : [2 2 2 2 2 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "action : 0\n",
      "new state: [2 2 3 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: 0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 2 3 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 2\n",
      "new state: [3 2 2 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [3 2 2 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 0\n",
      "new state: [2 2 2 2 3 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 0]\n",
      "reward: -0.5\n",
      "epReward so far: -1.0\n",
      "state : [2 2 2 2 3 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 0]\n",
      "action : 0\n",
      "new state: [1 2 2 3 0 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "reward: -0.5\n",
      "epReward so far: -1.5\n",
      "state : [1 2 2 3 0 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "action : 3\n",
      "new state: [2 2 3 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: -0.75\n",
      "epReward so far: -2.25\n",
      "final state: [2 2 3 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "Episode : 74\n",
      "state : [2 2 3 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 2\n",
      "new state: [3 2 2 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [3 2 2 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 0\n",
      "new state: [3 2 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [3 2 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "action : 0\n",
      "new state: [3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "state : [3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "action : 0\n",
      "new state: [3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "reward: -0.0\n",
      "epReward so far: -2.0\n",
      "state : [3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "action : 0\n",
      "new state: [2 3 2 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: 0.25\n",
      "epReward so far: -1.75\n",
      "final state: [2 3 2 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "Episode : 75\n",
      "state : [2 3 2 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 1\n",
      "new state: [2 3 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 3 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 1\n",
      "new state: [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "action : 1\n",
      "new state: [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "reward: -0.75\n",
      "epReward so far: -2.25\n",
      "state : [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "action : 1\n",
      "new state: [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "reward: -0.75\n",
      "epReward so far: -3.0\n",
      "state : [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "action : 1\n",
      "new state: [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "reward: -0.75\n",
      "epReward so far: -3.75\n",
      "final state: [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "Episode : 76\n",
      "state : [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "action : 1\n",
      "new state: [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "action : 1\n",
      "new state: [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "action : 1\n",
      "new state: [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "reward: -0.75\n",
      "epReward so far: -2.25\n",
      "state : [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "action : 1\n",
      "new state: [2 2 2 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: 0.25\n",
      "epReward so far: -2.0\n",
      "state : [2 2 2 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 3\n",
      "new state: [2 2 2 2 2 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0]\n",
      "reward: -0.5\n",
      "epReward so far: -2.5\n",
      "final state: [2 2 2 2 2 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0]\n",
      "Episode : 77\n",
      "state : [2 2 2 2 2 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0]\n",
      "action : 0\n",
      "new state: [1 2 3 2 0 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [1 2 3 2 0 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2]\n",
      "action : 2\n",
      "new state: [1 2 4 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: -0.5\n",
      "state : [1 2 4 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 2\n",
      "new state: [2 2 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "reward: 0.0\n",
      "epReward so far: -0.5\n",
      "state : [2 2 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "action : 2\n",
      "new state: [2 2 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [2 2 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "action : 2\n",
      "new state: [2 2 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "final state: [2 2 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "Episode : 78\n",
      "state : [2 2 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "action : 2\n",
      "new state: [2 2 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 2 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "action : 2\n",
      "new state: [2 2 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [2 2 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "action : 2\n",
      "new state: [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "reward: -0.75\n",
      "epReward so far: -2.25\n",
      "state : [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "action : 1\n",
      "new state: [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: -3.0\n",
      "state : [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "action : 2\n",
      "new state: [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "reward: -0.75\n",
      "epReward so far: -3.75\n",
      "final state: [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "Episode : 79\n",
      "state : [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "action : 2\n",
      "new state: [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "reward: -0.75\n",
      "epReward so far: -2.25\n",
      "state : [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "action : 2\n",
      "new state: [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "reward: -0.75\n",
      "epReward so far: -3.0\n",
      "state : [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "reward: -0.75\n",
      "epReward so far: -3.75\n",
      "final state: [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "Episode : 80\n",
      "state : [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "action : 2\n",
      "new state: [2 2 2 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [2 2 2 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 3\n",
      "new state: [2 2 2 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [2 2 2 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 3\n",
      "new state: [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "reward: 0.0\n",
      "epReward so far: -1.25\n",
      "state : [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "reward: 0.0\n",
      "epReward so far: -1.25\n",
      "final state: [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "Episode : 81\n",
      "state : [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "action : 2\n",
      "new state: [2 2 2 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [2 2 2 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "action : 3\n",
      "new state: [2 2 2 2 0 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2]\n",
      "reward: -0.5\n",
      "epReward so far: -0.25\n",
      "state : [2 2 2 2 0 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2]\n",
      "action : 0\n",
      "new state: [2 2 3 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "reward: -0.75\n",
      "epReward so far: -1.0\n",
      "state : [2 2 3 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "action : 2\n",
      "new state: [2 2 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.75\n",
      "state : [2 2 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "action : 2\n",
      "new state: [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "reward: -0.75\n",
      "epReward so far: -2.5\n",
      "final state: [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "Episode : 82\n",
      "state : [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "action : 2\n",
      "new state: [2 2 2 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [2 2 2 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 3\n",
      "new state: [2 3 2 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [2 3 2 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "action : 1\n",
      "new state: [3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "state : [3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "action : 0\n",
      "new state: [3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "reward: -0.75\n",
      "epReward so far: -2.75\n",
      "state : [3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "action : 0\n",
      "new state: [2 3 2 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: -0.5\n",
      "epReward so far: -3.25\n",
      "final state: [2 3 2 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "Episode : 83\n",
      "state : [2 3 2 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 1\n",
      "new state: [2 2 2 2 1 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [2 2 2 2 1 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "action : 0\n",
      "new state: [2 3 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [2 3 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 1\n",
      "new state: [2 3 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "reward: -0.5\n",
      "epReward so far: -1.75\n",
      "state : [2 3 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "action : 1\n",
      "new state: [2 2 2 2 2 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "reward: 0.25\n",
      "epReward so far: -1.5\n",
      "state : [2 2 2 2 2 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "action : 0\n",
      "new state: [1 2 3 2 2 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "reward: 0.25\n",
      "epReward so far: -1.25\n",
      "final state: [1 2 3 2 2 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "Episode : 84\n",
      "state : [1 2 3 2 2 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "action : 2\n",
      "new state: [1 2 4 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [1 2 4 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 2\n",
      "new state: [1 2 5 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [1 2 5 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "action : 2\n",
      "new state: [1 2 4 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: -0.5\n",
      "epReward so far: -2.0\n",
      "state : [1 2 4 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 2\n",
      "new state: [1 2 3 2 2 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 3]\n",
      "reward: 0.25\n",
      "epReward so far: -1.75\n",
      "state : [1 2 3 2 2 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 3]\n",
      "action : 2\n",
      "new state: [1 2 3 2 3 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 2]\n",
      "reward: 0.25\n",
      "epReward so far: -1.5\n",
      "final state: [1 2 3 2 3 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 2]\n",
      "Episode : 85\n",
      "state : [1 2 3 2 3 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 2]\n",
      "action : 2\n",
      "new state: [1 2 2 3 3 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [1 2 2 3 3 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2]\n",
      "action : 3\n",
      "new state: [1 2 3 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [1 2 3 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 2\n",
      "new state: [1 2 4 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "state : [1 2 4 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "action : 2\n",
      "new state: [1 2 3 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: -0.5\n",
      "epReward so far: -2.5\n",
      "state : [1 2 3 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 2\n",
      "new state: [1 2 3 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "reward: 0.0\n",
      "epReward so far: -2.5\n",
      "final state: [1 2 3 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "Episode : 86\n",
      "state : [1 2 3 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "action : 2\n",
      "new state: [1 3 2 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [1 3 2 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 1\n",
      "new state: [1 3 2 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [1 3 2 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 1\n",
      "new state: [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "state : [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "action : 2\n",
      "new state: [2 2 2 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "reward: -0.5\n",
      "epReward so far: -2.5\n",
      "state : [2 2 2 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "action : 3\n",
      "new state: [2 2 2 2 2 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 3]\n",
      "reward: 0.25\n",
      "epReward so far: -2.25\n",
      "final state: [2 2 2 2 2 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 3]\n",
      "Episode : 87\n",
      "state : [2 2 2 2 2 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 3]\n",
      "action : 0\n",
      "new state: [1 2 3 2 3 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 3]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [1 2 3 2 3 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 3]\n",
      "action : 2\n",
      "new state: [2 2 2 2 3 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.5\n",
      "state : [2 2 2 2 3 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "action : 0\n",
      "new state: [1 2 2 3 1 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 3]\n",
      "reward: -0.5\n",
      "epReward so far: 0.0\n",
      "state : [1 2 2 3 1 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 3]\n",
      "action : 3\n",
      "new state: [1 2 2 4 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: 0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 2 2 4 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 3\n",
      "new state: [1 3 2 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "final state: [1 3 2 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "Episode : 88\n",
      "state : [1 3 2 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "action : 1\n",
      "new state: [1 2 2 3 2 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 3]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [1 2 2 3 2 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 3]\n",
      "action : 3\n",
      "new state: [1 2 3 2 3 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 2]\n",
      "reward: -0.5\n",
      "epReward so far: -0.25\n",
      "state : [1 2 3 2 3 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 2]\n",
      "action : 2\n",
      "new state: [2 2 2 2 3 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 3]\n",
      "reward: -0.5\n",
      "epReward so far: -0.75\n",
      "state : [2 2 2 2 3 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 3]\n",
      "action : 0\n",
      "new state: [1 2 2 3 3 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 3]\n",
      "reward: -0.5\n",
      "epReward so far: -1.25\n",
      "state : [1 2 2 3 3 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 3]\n",
      "action : 3\n",
      "new state: [1 2 3 2 3 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 3]\n",
      "reward: -0.5\n",
      "epReward so far: -1.75\n",
      "final state: [1 2 3 2 3 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 3]\n",
      "Episode : 89\n",
      "state : [1 2 3 2 3 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 3]\n",
      "action : 2\n",
      "new state: [1 2 3 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [1 2 3 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 2\n",
      "new state: [1 2 3 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [1 2 3 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "action : 3\n",
      "new state: [1 2 3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: -0.5\n",
      "epReward so far: -2.0\n",
      "state : [1 2 3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 2\n",
      "new state: [1 2 2 3 2 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "reward: -0.5\n",
      "epReward so far: -2.5\n",
      "state : [1 2 2 3 2 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "action : 3\n",
      "new state: [1 2 3 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: -0.0\n",
      "epReward so far: -2.5\n",
      "final state: [1 2 3 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "Episode : 90\n",
      "state : [1 2 3 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 2\n",
      "new state: [1 2 4 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [1 2 4 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "action : 2\n",
      "new state: [1 2 4 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [1 2 4 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "action : 2\n",
      "new state: [1 2 3 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: 0.25\n",
      "epReward so far: -1.25\n",
      "state : [1 2 3 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 2\n",
      "new state: [1 2 3 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: -0.0\n",
      "epReward so far: -1.25\n",
      "state : [1 2 3 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 2\n",
      "new state: [1 3 2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "reward: 0.25\n",
      "epReward so far: -1.0\n",
      "final state: [1 3 2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "Episode : 91\n",
      "state : [1 3 2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "action : 1\n",
      "new state: [1 2 2 3 3 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 0]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [1 2 2 3 3 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 0]\n",
      "action : 3\n",
      "new state: [1 2 2 3 0 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 3]\n",
      "reward: 0.25\n",
      "epReward so far: -0.25\n",
      "state : [1 2 2 3 0 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 3]\n",
      "action : 3\n",
      "new state: [2 2 2 2 0 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0]\n",
      "reward: -0.5\n",
      "epReward so far: -0.75\n",
      "state : [2 2 2 2 0 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0]\n",
      "action : 0\n",
      "new state: [2 2 2 2 0 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 2]\n",
      "reward: -0.5\n",
      "epReward so far: -1.25\n",
      "state : [2 2 2 2 0 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 2]\n",
      "action : 0\n",
      "new state: [2 2 2 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "final state: [2 2 2 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "Episode : 92\n",
      "state : [2 2 2 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 3\n",
      "new state: [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "action : 0\n",
      "new state: [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "action : 0\n",
      "new state: [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "reward: -0.75\n",
      "epReward so far: -2.25\n",
      "state : [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "action : 0\n",
      "new state: [2 2 2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: -0.5\n",
      "epReward so far: -2.75\n",
      "state : [2 2 2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 3\n",
      "new state: [2 2 2 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: -0.75\n",
      "epReward so far: -3.5\n",
      "final state: [2 2 2 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "Episode : 93\n",
      "state : [2 2 2 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 3\n",
      "new state: [2 2 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 2 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "reward: 0.0\n",
      "epReward so far: -0.75\n",
      "state : [2 2 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "reward: 0.0\n",
      "epReward so far: -0.75\n",
      "state : [2 2 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [2 2 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "action : 3\n",
      "new state: [2 2 2 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "reward: 0.25\n",
      "epReward so far: -1.25\n",
      "final state: [2 2 2 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "Episode : 94\n",
      "state : [2 2 2 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "action : 3\n",
      "new state: [2 2 2 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 2 2 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 3\n",
      "new state: [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "action : 0\n",
      "new state: [2 2 2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: 0.25\n",
      "epReward so far: -1.25\n",
      "state : [2 2 2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 3\n",
      "new state: [2 2 2 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "state : [2 2 2 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "action : 3\n",
      "new state: [2 2 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "reward: -0.75\n",
      "epReward so far: -2.75\n",
      "final state: [2 2 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "Episode : 95\n",
      "state : [2 2 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "action : 3\n",
      "new state: [2 2 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 2 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [2 2 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "action : 3\n",
      "new state: [2 2 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "reward: -0.75\n",
      "epReward so far: -2.25\n",
      "state : [2 2 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "action : 3\n",
      "new state: [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "reward: -0.75\n",
      "epReward so far: -3.0\n",
      "state : [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "action : 0\n",
      "new state: [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "reward: -0.75\n",
      "epReward so far: -3.75\n",
      "final state: [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "Episode : 96\n",
      "state : [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "action : 0\n",
      "new state: [2 2 2 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [2 2 2 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 3\n",
      "new state: [2 2 2 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: 0.0\n",
      "epReward so far: -0.5\n",
      "state : [2 2 2 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 3\n",
      "new state: [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "action : 1\n",
      "new state: [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "state : [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "action : 1\n",
      "new state: [2 2 2 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: -0.5\n",
      "epReward so far: -2.5\n",
      "final state: [2 2 2 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "Episode : 97\n",
      "state : [2 2 2 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 3\n",
      "new state: [2 2 2 2 2 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [2 2 2 2 2 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "action : 0\n",
      "new state: [2 2 3 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [2 2 3 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 2\n",
      "new state: [3 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "state : [3 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "action : 0\n",
      "new state: [3 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "reward: -0.75\n",
      "epReward so far: -2.75\n",
      "state : [3 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "action : 0\n",
      "new state: [2 2 3 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "reward: 0.25\n",
      "epReward so far: -2.5\n",
      "final state: [2 2 3 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "Episode : 98\n",
      "state : [2 2 3 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "action : 2\n",
      "new state: [2 2 3 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 2 3 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 2\n",
      "new state: [2 3 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: -0.5\n",
      "epReward so far: -1.25\n",
      "state : [2 3 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 1\n",
      "new state: [2 3 2 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "state : [2 3 2 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 1\n",
      "new state: [2 2 3 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "reward: -0.5\n",
      "epReward so far: -2.5\n",
      "state : [2 2 3 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "action : 2\n",
      "new state: [2 2 2 2 1 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "reward: -0.5\n",
      "epReward so far: -3.0\n",
      "final state: [2 2 2 2 1 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "Episode : 99\n",
      "state : [2 2 2 2 1 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "action : 0\n",
      "new state: [2 3 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 3 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 1\n",
      "new state: [2 2 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 2 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "action : 3\n",
      "new state: [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "action : 0\n",
      "new state: [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "reward: 0.0\n",
      "epReward so far: -1.5\n",
      "state : [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "action : 0\n",
      "new state: [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "reward: -0.75\n",
      "epReward so far: -2.25\n",
      "final state: [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "Episode : 0\n",
      "state : [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "action : 0\n",
      "new state: [2 2 2 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [2 2 2 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 3\n",
      "new state: [2 2 2 2 2 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 0]\n",
      "reward: -0.5\n",
      "epReward so far: -0.25\n",
      "state : [2 2 2 2 2 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 0]\n",
      "action : 0\n",
      "new state: [2 2 3 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.0\n",
      "state : [2 2 3 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 2\n",
      "new state: [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.75\n",
      "state : [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "action : 2\n",
      "new state: [2 2 2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "reward: 0.25\n",
      "epReward so far: -1.5\n",
      "final state: [2 2 2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "Episode : 1\n",
      "state : [2 2 2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "action : 3\n",
      "new state: [2 2 2 2 3 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [2 2 2 2 3 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0]\n",
      "action : 0\n",
      "new state: [1 2 2 3 0 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "reward: -0.5\n",
      "epReward so far: -0.25\n",
      "state : [1 2 2 3 0 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "action : 3\n",
      "new state: [2 2 2 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: -0.75\n",
      "epReward so far: -1.0\n",
      "state : [2 2 2 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 3\n",
      "new state: [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "reward: -0.75\n",
      "epReward so far: -1.75\n",
      "state : [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "action : 0\n",
      "new state: [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "reward: -0.75\n",
      "epReward so far: -2.5\n",
      "final state: [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "Episode : 2\n",
      "state : [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "action : 0\n",
      "new state: [2 2 2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [2 2 2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 3\n",
      "new state: [2 2 2 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "reward: -0.75\n",
      "epReward so far: -0.5\n",
      "state : [2 2 2 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [2 2 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "action : 3\n",
      "new state: [2 2 2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "reward: -0.5\n",
      "epReward so far: -1.75\n",
      "state : [2 2 2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "action : 3\n",
      "new state: [2 2 2 2 3 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 1]\n",
      "reward: -0.5\n",
      "epReward so far: -2.25\n",
      "final state: [2 2 2 2 3 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 1]\n",
      "Episode : 3\n",
      "state : [2 2 2 2 3 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 1]\n",
      "action : 0\n",
      "new state: [2 2 2 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 2 2 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 3\n",
      "new state: [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "action : 1\n",
      "new state: [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "reward: -0.0\n",
      "epReward so far: -1.5\n",
      "state : [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "action : 1\n",
      "new state: [2 2 2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: 0.25\n",
      "epReward so far: -1.25\n",
      "state : [2 2 2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 3\n",
      "new state: [2 2 2 2 3 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 3]\n",
      "reward: -0.5\n",
      "epReward so far: -1.75\n",
      "final state: [2 2 2 2 3 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 3]\n",
      "Episode : 4\n",
      "state : [2 2 2 2 3 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 3]\n",
      "action : 0\n",
      "new state: [2 2 2 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 2 2 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 3\n",
      "new state: [2 2 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [2 2 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "action : 3\n",
      "new state: [2 2 2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: -0.5\n",
      "epReward so far: -2.0\n",
      "state : [2 2 2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 3\n",
      "new state: [2 2 2 2 3 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2]\n",
      "reward: -0.5\n",
      "epReward so far: -2.5\n",
      "state : [2 2 2 2 3 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2]\n",
      "action : 0\n",
      "new state: [1 2 3 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: -0.75\n",
      "epReward so far: -3.25\n",
      "final state: [1 2 3 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "Episode : 5\n",
      "state : [1 2 3 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 2\n",
      "new state: [1 2 3 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [1 2 3 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 2\n",
      "new state: [1 2 2 3 3 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 2]\n",
      "reward: -0.5\n",
      "epReward so far: -0.25\n",
      "state : [1 2 2 3 3 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 2]\n",
      "action : 3\n",
      "new state: [1 2 2 4 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: -0.0\n",
      "epReward so far: -0.25\n",
      "state : [1 2 2 4 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 3\n",
      "new state: [1 2 3 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.0\n",
      "state : [1 2 3 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 2\n",
      "new state: [1 2 3 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "final state: [1 2 3 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "Episode : 6\n",
      "state : [1 2 3 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "action : 2\n",
      "new state: [1 3 2 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [1 3 2 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "action : 1\n",
      "new state: [1 2 2 3 0 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 1]\n",
      "reward: 0.25\n",
      "epReward so far: -0.25\n",
      "state : [1 2 2 3 0 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 1]\n",
      "action : 3\n",
      "new state: [2 2 2 2 1 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "reward: -0.5\n",
      "epReward so far: -0.75\n",
      "state : [2 2 2 2 1 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "action : 0\n",
      "new state: [2 2 2 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [2 2 2 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "action : 3\n",
      "new state: [3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "reward: -0.75\n",
      "epReward so far: -2.25\n",
      "final state: [3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "Episode : 7\n",
      "state : [3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "action : 0\n",
      "new state: [3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "action : 0\n",
      "new state: [2 3 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: 0.25\n",
      "epReward so far: -0.5\n",
      "state : [2 3 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 1\n",
      "new state: [2 2 2 2 2 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 3]\n",
      "reward: -0.5\n",
      "epReward so far: -1.0\n",
      "state : [2 2 2 2 2 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 3]\n",
      "action : 0\n",
      "new state: [1 2 3 2 3 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 3]\n",
      "reward: -0.5\n",
      "epReward so far: -1.5\n",
      "state : [1 2 3 2 3 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 3]\n",
      "action : 2\n",
      "new state: [1 2 2 4 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: -0.75\n",
      "epReward so far: -2.25\n",
      "final state: [1 2 2 4 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "Episode : 8\n",
      "state : [1 2 2 4 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 3\n",
      "new state: [1 2 2 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [1 2 2 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "action : 3\n",
      "new state: [1 2 3 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [1 2 3 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "action : 3\n",
      "new state: [1 2 3 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "reward: -0.75\n",
      "epReward so far: -2.25\n",
      "state : [1 2 3 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "action : 3\n",
      "new state: [1 2 3 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "reward: -0.75\n",
      "epReward so far: -3.0\n",
      "state : [1 2 3 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "action : 3\n",
      "new state: [1 2 3 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "reward: -0.75\n",
      "epReward so far: -3.75\n",
      "final state: [1 2 3 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "Episode : 9\n",
      "state : [1 2 3 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "action : 3\n",
      "new state: [1 2 3 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [1 2 3 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 2\n",
      "new state: [1 2 3 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: -0.75\n",
      "epReward so far: -0.5\n",
      "state : [1 2 3 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 2\n",
      "new state: [2 2 2 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: 0.25\n",
      "epReward so far: -0.25\n",
      "state : [2 2 2 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 3\n",
      "new state: [2 2 2 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "reward: -0.75\n",
      "epReward so far: -1.0\n",
      "state : [2 2 2 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "action : 3\n",
      "new state: [3 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "reward: -0.5\n",
      "epReward so far: -1.5\n",
      "final state: [3 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "Episode : 10\n",
      "state : [3 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "action : 0\n",
      "new state: [3 2 2 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "reward: 0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 2 2 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "action : 0\n",
      "new state: [2 2 3 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [2 2 3 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "action : 2\n",
      "new state: [3 2 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.5\n",
      "state : [3 2 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 0\n",
      "new state: [2 4 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [2 4 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "action : 1\n",
      "new state: [3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "final state: [3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "Episode : 11\n",
      "state : [3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "action : 0\n",
      "new state: [3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "action : 0\n",
      "new state: [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "action : 1\n",
      "new state: [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "reward: -0.75\n",
      "epReward so far: -2.25\n",
      "state : [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "action : 1\n",
      "new state: [2 2 3 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "reward: 0.25\n",
      "epReward so far: -2.0\n",
      "state : [2 2 3 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "action : 2\n",
      "new state: [2 2 2 2 2 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2]\n",
      "reward: -0.5\n",
      "epReward so far: -2.5\n",
      "final state: [2 2 2 2 2 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2]\n",
      "Episode : 12\n",
      "state : [2 2 2 2 2 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2]\n",
      "action : 0\n",
      "new state: [2 2 3 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 2 3 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "action : 2\n",
      "new state: [2 3 2 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "reward: -0.5\n",
      "epReward so far: -1.25\n",
      "state : [2 3 2 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "action : 1\n",
      "new state: [2 3 2 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "state : [2 3 2 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 1\n",
      "new state: [3 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "reward: -0.5\n",
      "epReward so far: -2.5\n",
      "state : [3 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "action : 0\n",
      "new state: [2 2 2 2 2 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0]\n",
      "reward: -0.5\n",
      "epReward so far: -3.0\n",
      "final state: [2 2 2 2 2 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0]\n",
      "Episode : 13\n",
      "state : [2 2 2 2 2 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0]\n",
      "action : 0\n",
      "new state: [2 2 3 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 2 3 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 2\n",
      "new state: [3 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [3 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "action : 0\n",
      "new state: [2 2 3 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: 0.25\n",
      "epReward so far: -1.25\n",
      "state : [2 2 3 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 2\n",
      "new state: [2 3 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "state : [2 3 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "action : 1\n",
      "new state: [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "reward: -0.0\n",
      "epReward so far: -2.0\n",
      "final state: [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "Episode : 14\n",
      "state : [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "action : 1\n",
      "new state: [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "action : 1\n",
      "new state: [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "action : 1\n",
      "new state: [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "reward: -0.75\n",
      "epReward so far: -2.25\n",
      "state : [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "action : 1\n",
      "new state: [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "reward: -0.75\n",
      "epReward so far: -3.0\n",
      "state : [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "action : 2\n",
      "new state: [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "reward: -0.75\n",
      "epReward so far: -3.75\n",
      "final state: [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "Episode : 15\n",
      "state : [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "action : 0\n",
      "new state: [2 2 2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [2 2 2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "action : 3\n",
      "new state: [2 2 2 2 3 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 0]\n",
      "reward: -0.5\n",
      "epReward so far: -1.0\n",
      "state : [2 2 2 2 3 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 0]\n",
      "action : 0\n",
      "new state: [2 2 2 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.75\n",
      "state : [2 2 2 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 3\n",
      "new state: [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "reward: 0.0\n",
      "epReward so far: -1.75\n",
      "state : [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "action : 1\n",
      "new state: [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "reward: -0.75\n",
      "epReward so far: -2.5\n",
      "final state: [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "Episode : 16\n",
      "state : [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "action : 2\n",
      "new state: [2 2 2 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [2 2 2 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 3\n",
      "new state: [2 2 2 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: -0.75\n",
      "epReward so far: -0.5\n",
      "state : [2 2 2 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 3\n",
      "new state: [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "reward: 0.0\n",
      "epReward so far: -0.5\n",
      "state : [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "action : 1\n",
      "new state: [2 2 2 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: -0.5\n",
      "epReward so far: -1.0\n",
      "state : [2 2 2 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 3\n",
      "new state: [2 2 2 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: 0.0\n",
      "epReward so far: -1.0\n",
      "final state: [2 2 2 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "Episode : 17\n",
      "state : [2 2 2 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 3\n",
      "new state: [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "action : 0\n",
      "new state: [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "action : 0\n",
      "new state: [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "reward: -0.75\n",
      "epReward so far: -2.25\n",
      "state : [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "action : 1\n",
      "new state: [2 2 2 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: 0.25\n",
      "epReward so far: -2.0\n",
      "state : [2 2 2 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 3\n",
      "new state: [2 2 2 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: 0.0\n",
      "epReward so far: -2.0\n",
      "final state: [2 2 2 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "Episode : 18\n",
      "state : [2 2 2 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 3\n",
      "new state: [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "reward: 0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "action : 0\n",
      "new state: [2 2 2 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [2 2 2 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "action : 3\n",
      "new state: [2 2 2 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.25\n",
      "state : [2 2 2 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 3\n",
      "new state: [2 3 2 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "reward: -0.5\n",
      "epReward so far: -0.25\n",
      "state : [2 3 2 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "action : 1\n",
      "new state: [2 2 2 2 0 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.0\n",
      "final state: [2 2 2 2 0 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 1]\n",
      "Episode : 19\n",
      "state : [2 2 2 2 0 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 1]\n",
      "action : 0\n",
      "new state: [2 2 2 2 1 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 1]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [2 2 2 2 1 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 1]\n",
      "action : 0\n",
      "new state: [1 2 3 2 1 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 3]\n",
      "reward: -0.5\n",
      "epReward so far: -1.0\n",
      "state : [1 2 3 2 1 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 3]\n",
      "action : 2\n",
      "new state: [1 3 2 2 3 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 1]\n",
      "reward: -0.5\n",
      "epReward so far: -1.5\n",
      "state : [1 3 2 2 3 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 1]\n",
      "action : 1\n",
      "new state: [1 4 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: -0.75\n",
      "epReward so far: -2.25\n",
      "state : [1 4 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 1\n",
      "new state: [1 3 2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "reward: -0.5\n",
      "epReward so far: -2.75\n",
      "final state: [1 3 2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "Episode : 20\n",
      "state : [1 3 2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "action : 1\n",
      "new state: [1 3 2 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 3 2 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "action : 1\n",
      "new state: [1 2 2 4 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [1 2 2 4 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "action : 3\n",
      "new state: [1 2 2 4 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [1 2 2 4 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "action : 3\n",
      "new state: [1 3 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "state : [1 3 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "action : 3\n",
      "new state: [1 3 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "reward: -0.75\n",
      "epReward so far: -2.75\n",
      "final state: [1 3 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "Episode : 21\n",
      "state : [1 3 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "action : 3\n",
      "new state: [1 3 2 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [1 3 2 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 1\n",
      "new state: [1 3 2 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: -0.75\n",
      "epReward so far: -0.5\n",
      "state : [1 3 2 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 1\n",
      "new state: [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "action : 1\n",
      "new state: [2 2 2 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "reward: 0.25\n",
      "epReward so far: -1.0\n",
      "state : [2 2 2 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "action : 3\n",
      "new state: [2 2 2 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: -0.0\n",
      "epReward so far: -1.0\n",
      "final state: [2 2 2 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "Episode : 22\n",
      "state : [2 2 2 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 3\n",
      "new state: [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "reward: 0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "action : 2\n",
      "new state: [2 2 2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [2 2 2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "action : 3\n",
      "new state: [2 2 2 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [2 2 2 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "reward: 0.0\n",
      "epReward so far: -1.25\n",
      "state : [2 2 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "final state: [2 2 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "Episode : 23\n",
      "state : [2 2 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 2 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [2 2 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "action : 3\n",
      "new state: [2 2 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "reward: -0.75\n",
      "epReward so far: -2.25\n",
      "state : [2 2 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "action : 3\n",
      "new state: [2 2 2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "reward: -0.5\n",
      "epReward so far: -2.75\n",
      "state : [2 2 2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "action : 3\n",
      "new state: [2 2 2 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "reward: -0.75\n",
      "epReward so far: -3.5\n",
      "final state: [2 2 2 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "Episode : 24\n",
      "state : [2 2 2 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "action : 3\n",
      "new state: [2 2 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 2 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "action : 3\n",
      "new state: [2 2 2 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: 0.25\n",
      "epReward so far: -0.5\n",
      "state : [2 2 2 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 3\n",
      "new state: [2 2 2 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: -0.0\n",
      "epReward so far: -0.5\n",
      "state : [2 2 2 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 3\n",
      "new state: [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "action : 0\n",
      "new state: [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "final state: [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "Episode : 25\n",
      "state : [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "action : 0\n",
      "new state: [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "action : 0\n",
      "new state: [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "action : 0\n",
      "new state: [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: -2.25\n",
      "state : [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "action : 0\n",
      "new state: [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "reward: -0.75\n",
      "epReward so far: -3.0\n",
      "state : [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "action : 0\n",
      "new state: [2 2 2 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: -0.5\n",
      "epReward so far: -3.5\n",
      "final state: [2 2 2 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "Episode : 26\n",
      "state : [2 2 2 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 3\n",
      "new state: [2 2 2 2 2 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [2 2 2 2 2 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "action : 0\n",
      "new state: [2 2 3 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.25\n",
      "state : [2 2 3 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 2\n",
      "new state: [2 2 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.5\n",
      "state : [2 2 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "action : 2\n",
      "new state: [2 2 3 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: -0.5\n",
      "epReward so far: -1.0\n",
      "state : [2 2 3 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 2\n",
      "new state: [2 2 3 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.75\n",
      "final state: [2 2 3 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "Episode : 27\n",
      "state : [2 2 3 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "action : 2\n",
      "new state: [2 2 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 2 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "action : 2\n",
      "new state: [2 2 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [2 2 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "action : 2\n",
      "new state: [2 2 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "reward: -0.75\n",
      "epReward so far: -2.25\n",
      "state : [2 2 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "action : 2\n",
      "new state: [2 2 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "reward: -0.75\n",
      "epReward so far: -3.0\n",
      "state : [2 2 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "action : 2\n",
      "new state: [2 2 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "reward: -0.75\n",
      "epReward so far: -3.75\n",
      "final state: [2 2 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "Episode : 28\n",
      "state : [2 2 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "action : 2\n",
      "new state: [2 2 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 2 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "action : 2\n",
      "new state: [2 2 3 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [2 2 3 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 2\n",
      "new state: [2 2 2 2 2 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0]\n",
      "reward: -0.5\n",
      "epReward so far: -1.0\n",
      "state : [2 2 2 2 2 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0]\n",
      "action : 0\n",
      "new state: [2 2 3 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: -0.75\n",
      "epReward so far: -1.75\n",
      "state : [2 2 3 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 2\n",
      "new state: [2 2 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "reward: 0.0\n",
      "epReward so far: -1.75\n",
      "final state: [2 2 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "Episode : 29\n",
      "state : [2 2 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "action : 2\n",
      "new state: [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "action : 2\n",
      "new state: [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "action : 2\n",
      "new state: [2 2 2 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: -0.5\n",
      "epReward so far: -2.0\n",
      "state : [2 2 2 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 3\n",
      "new state: [2 2 2 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "reward: 0.0\n",
      "epReward so far: -2.0\n",
      "state : [2 2 2 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "action : 3\n",
      "new state: [2 2 3 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: -0.5\n",
      "epReward so far: -2.5\n",
      "final state: [2 2 3 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "Episode : 30\n",
      "state : [2 2 3 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 2\n",
      "new state: [2 2 2 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 2 2 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 3\n",
      "new state: [3 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [3 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "action : 0\n",
      "new state: [3 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "reward: -0.75\n",
      "epReward so far: -2.25\n",
      "state : [3 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "action : 0\n",
      "new state: [2 2 3 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: -0.5\n",
      "epReward so far: -2.75\n",
      "state : [2 2 3 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 2\n",
      "new state: [2 2 3 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: 0.0\n",
      "epReward so far: -2.75\n",
      "final state: [2 2 3 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "Episode : 31\n",
      "state : [2 2 3 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 2\n",
      "new state: [3 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [3 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "action : 0\n",
      "new state: [3 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [3 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "action : 0\n",
      "new state: [3 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "reward: -0.75\n",
      "epReward so far: -2.25\n",
      "state : [3 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "action : 0\n",
      "new state: [2 2 3 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: -0.5\n",
      "epReward so far: -2.75\n",
      "state : [2 2 3 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 2\n",
      "new state: [2 2 3 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: -0.0\n",
      "epReward so far: -2.75\n",
      "final state: [2 2 3 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "Episode : 32\n",
      "state : [2 2 3 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 2\n",
      "new state: [2 3 2 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [2 3 2 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "action : 1\n",
      "new state: [2 3 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.25\n",
      "state : [2 3 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 1\n",
      "new state: [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.5\n",
      "state : [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "action : 1\n",
      "new state: [2 2 2 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "reward: -0.5\n",
      "epReward so far: -1.0\n",
      "state : [2 2 2 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "action : 3\n",
      "new state: [2 2 2 2 1 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0]\n",
      "reward: 0.25\n",
      "epReward so far: -0.75\n",
      "final state: [2 2 2 2 1 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0]\n",
      "Episode : 33\n",
      "state : [2 2 2 2 1 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0]\n",
      "action : 0\n",
      "new state: [2 3 2 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 3 2 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "action : 1\n",
      "new state: [3 2 2 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "reward: 0.25\n",
      "epReward so far: -0.5\n",
      "state : [3 2 2 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "action : 0\n",
      "new state: [2 2 2 2 0 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "reward: -0.5\n",
      "epReward so far: -1.0\n",
      "state : [2 2 2 2 0 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "action : 0\n",
      "new state: [2 2 2 2 1 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 2]\n",
      "reward: -0.5\n",
      "epReward so far: -1.5\n",
      "state : [2 2 2 2 1 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 2]\n",
      "action : 0\n",
      "new state: [3 2 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "reward: -0.75\n",
      "epReward so far: -2.25\n",
      "final state: [3 2 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "Episode : 34\n",
      "state : [3 2 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "action : 0\n",
      "new state: [2 3 2 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [2 3 2 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 1\n",
      "new state: [2 2 2 2 1 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 3]\n",
      "reward: -0.5\n",
      "epReward so far: -0.25\n",
      "state : [2 2 2 2 1 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 3]\n",
      "action : 0\n",
      "new state: [1 3 2 2 3 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.0\n",
      "state : [1 3 2 2 3 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 1]\n",
      "action : 1\n",
      "new state: [1 2 3 2 3 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [1 2 3 2 3 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "action : 2\n",
      "new state: [1 2 3 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "final state: [1 2 3 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "Episode : 35\n",
      "state : [1 2 3 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "action : 2\n",
      "new state: [1 3 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [1 3 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "action : 1\n",
      "new state: [1 2 3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "reward: -0.5\n",
      "epReward so far: -1.25\n",
      "state : [1 2 3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "action : 2\n",
      "new state: [1 2 3 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "state : [1 2 3 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 2\n",
      "new state: [1 2 3 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "reward: -0.5\n",
      "epReward so far: -2.5\n",
      "state : [1 2 3 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "action : 2\n",
      "new state: [2 2 2 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "reward: -0.75\n",
      "epReward so far: -3.25\n",
      "final state: [2 2 2 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "Episode : 36\n",
      "state : [2 2 2 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "action : 3\n",
      "new state: [2 3 2 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [2 3 2 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "action : 1\n",
      "new state: [2 2 2 2 0 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 1]\n",
      "reward: 0.25\n",
      "epReward so far: -0.25\n",
      "state : [2 2 2 2 0 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 1]\n",
      "action : 0\n",
      "new state: [3 2 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: -0.25\n",
      "state : [3 2 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 0\n",
      "new state: [2 2 2 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.0\n",
      "state : [2 2 2 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "action : 3\n",
      "new state: [2 2 2 2 2 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "final state: [2 2 2 2 2 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "Episode : 37\n",
      "state : [2 2 2 2 2 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "action : 0\n",
      "new state: [1 2 3 2 0 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 1]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [1 2 3 2 0 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 1]\n",
      "action : 2\n",
      "new state: [1 3 3 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [1 3 3 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 1\n",
      "new state: [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "state : [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "action : 1\n",
      "new state: [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "reward: -0.75\n",
      "epReward so far: -2.75\n",
      "state : [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "action : 1\n",
      "new state: [2 2 3 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: -0.5\n",
      "epReward so far: -3.25\n",
      "final state: [2 2 3 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "Episode : 38\n",
      "state : [2 2 3 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 2\n",
      "new state: [2 2 2 2 2 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [2 2 2 2 2 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0]\n",
      "action : 0\n",
      "new state: [2 2 3 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.5\n",
      "state : [2 2 3 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "action : 2\n",
      "new state: [3 2 2 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: -0.5\n",
      "epReward so far: -1.0\n",
      "state : [3 2 2 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 0\n",
      "new state: [3 2 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "reward: -0.75\n",
      "epReward so far: -1.75\n",
      "state : [3 2 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "action : 0\n",
      "new state: [3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "reward: -0.75\n",
      "epReward so far: -2.5\n",
      "final state: [3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "Episode : 39\n",
      "state : [3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "action : 0\n",
      "new state: [2 3 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [2 3 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "action : 1\n",
      "new state: [2 3 2 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: -0.0\n",
      "epReward so far: -0.5\n",
      "state : [2 3 2 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 1\n",
      "new state: [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "action : 1\n",
      "new state: [2 2 3 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: -0.5\n",
      "epReward so far: -1.75\n",
      "state : [2 2 3 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 2\n",
      "new state: [2 2 2 2 0 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 3]\n",
      "reward: -0.5\n",
      "epReward so far: -2.25\n",
      "final state: [2 2 2 2 0 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 3]\n",
      "Episode : 40\n",
      "state : [2 2 2 2 0 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 3]\n",
      "action : 0\n",
      "new state: [2 2 2 2 3 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 2]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [2 2 2 2 3 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 2]\n",
      "action : 0\n",
      "new state: [1 2 3 2 3 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "reward: -0.5\n",
      "epReward so far: -1.0\n",
      "state : [1 2 3 2 3 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "action : 2\n",
      "new state: [1 2 2 3 0 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2]\n",
      "reward: 0.25\n",
      "epReward so far: -0.75\n",
      "state : [1 2 2 3 0 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2]\n",
      "action : 3\n",
      "new state: [1 2 4 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [1 2 4 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "action : 2\n",
      "new state: [2 2 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "reward: -0.75\n",
      "epReward so far: -2.25\n",
      "final state: [2 2 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "Episode : 41\n",
      "state : [2 2 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "action : 2\n",
      "new state: [2 2 3 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [2 2 3 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "action : 2\n",
      "new state: [2 2 3 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: -0.0\n",
      "epReward so far: -0.5\n",
      "state : [2 2 3 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 2\n",
      "new state: [3 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [3 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "action : 0\n",
      "new state: [2 2 3 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: -0.5\n",
      "epReward so far: -1.75\n",
      "state : [2 2 3 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 2\n",
      "new state: [2 2 3 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: 0.0\n",
      "epReward so far: -1.75\n",
      "final state: [2 2 3 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "Episode : 42\n",
      "state : [2 2 3 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 2\n",
      "new state: [2 3 2 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [2 3 2 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 1\n",
      "new state: [2 3 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.5\n",
      "state : [2 3 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 1\n",
      "new state: [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "action : 1\n",
      "new state: [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "state : [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "action : 1\n",
      "new state: [2 2 2 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: 0.25\n",
      "epReward so far: -1.75\n",
      "final state: [2 2 2 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "Episode : 43\n",
      "state : [2 2 2 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 3\n",
      "new state: [2 2 2 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 2 2 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "action : 3\n",
      "new state: [4 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [4 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "action : 0\n",
      "new state: [3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "reward: -0.75\n",
      "epReward so far: -2.25\n",
      "state : [3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "action : 0\n",
      "new state: [3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "reward: -0.75\n",
      "epReward so far: -3.0\n",
      "state : [3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "action : 0\n",
      "new state: [2 3 2 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "reward: 0.25\n",
      "epReward so far: -2.75\n",
      "final state: [2 3 2 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "Episode : 44\n",
      "state : [2 3 2 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "action : 1\n",
      "new state: [2 3 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 3 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "action : 1\n",
      "new state: [2 3 2 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: -0.5\n",
      "epReward so far: -1.25\n",
      "state : [2 3 2 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 1\n",
      "new state: [2 3 2 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "state : [2 3 2 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 1\n",
      "new state: [3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "reward: 0.0\n",
      "epReward so far: -2.0\n",
      "state : [3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "action : 0\n",
      "new state: [3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "reward: -0.75\n",
      "epReward so far: -2.75\n",
      "final state: [3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "Episode : 45\n",
      "state : [3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "action : 0\n",
      "new state: [2 3 2 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [2 3 2 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 1\n",
      "new state: [2 3 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: -0.75\n",
      "epReward so far: -0.5\n",
      "state : [2 3 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 1\n",
      "new state: [2 3 2 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: -0.5\n",
      "epReward so far: -1.0\n",
      "state : [2 3 2 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 1\n",
      "new state: [2 3 2 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: -0.0\n",
      "epReward so far: -1.0\n",
      "state : [2 3 2 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 1\n",
      "new state: [3 2 2 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "reward: -0.5\n",
      "epReward so far: -1.5\n",
      "final state: [3 2 2 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "Episode : 46\n",
      "state : [3 2 2 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "action : 0\n",
      "new state: [2 2 2 2 1 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [2 2 2 2 1 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "action : 0\n",
      "new state: [1 3 2 2 2 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1]\n",
      "reward: -0.5\n",
      "epReward so far: -1.0\n",
      "state : [1 3 2 2 2 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1]\n",
      "action : 1\n",
      "new state: [2 3 2 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: 0.0\n",
      "epReward so far: -1.0\n",
      "state : [2 3 2 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 1\n",
      "new state: [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.75\n",
      "state : [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "action : 1\n",
      "new state: [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "reward: -0.75\n",
      "epReward so far: -2.5\n",
      "final state: [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "Episode : 47\n",
      "state : [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "action : 1\n",
      "new state: [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "action : 1\n",
      "new state: [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "action : 1\n",
      "new state: [2 2 3 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: -0.5\n",
      "epReward so far: -2.0\n",
      "state : [2 2 3 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 2\n",
      "new state: [2 2 2 2 1 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0]\n",
      "reward: -0.5\n",
      "epReward so far: -2.5\n",
      "state : [2 2 2 2 1 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0]\n",
      "action : 0\n",
      "new state: [1 3 2 2 0 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1]\n",
      "reward: -0.5\n",
      "epReward so far: -3.0\n",
      "final state: [1 3 2 2 0 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1]\n",
      "Episode : 48\n",
      "state : [1 3 2 2 0 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1]\n",
      "action : 1\n",
      "new state: [1 3 2 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: 0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 3 2 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 1\n",
      "new state: [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "action : 2\n",
      "new state: [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "reward: -0.0\n",
      "epReward so far: -0.75\n",
      "state : [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "action : 2\n",
      "new state: [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: -0.75\n",
      "state : [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "final state: [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "Episode : 49\n",
      "state : [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "action : 2\n",
      "new state: [2 2 2 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [2 2 2 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 3\n",
      "new state: [2 2 2 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [2 2 2 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "action : 3\n",
      "new state: [2 2 3 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "reward: -0.5\n",
      "epReward so far: -1.75\n",
      "state : [2 2 3 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "action : 2\n",
      "new state: [2 2 3 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "reward: -0.0\n",
      "epReward so far: -1.75\n",
      "state : [2 2 3 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "action : 2\n",
      "new state: [2 2 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "reward: -0.75\n",
      "epReward so far: -2.5\n",
      "final state: [2 2 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Episode : 50\n",
      "state : [2 2 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "action : 2\n",
      "new state: [2 2 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 2 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "action : 2\n",
      "new state: [2 2 3 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: -0.5\n",
      "epReward so far: -1.25\n",
      "state : [2 2 3 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 2\n",
      "new state: [2 2 2 2 2 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "reward: 0.25\n",
      "epReward so far: -1.0\n",
      "state : [2 2 2 2 2 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "action : 0\n",
      "new state: [2 2 3 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "reward: 0.0\n",
      "epReward so far: -1.0\n",
      "state : [2 2 3 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "action : 2\n",
      "new state: [3 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.75\n",
      "final state: [3 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "Episode : 51\n",
      "state : [3 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "action : 0\n",
      "new state: [3 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [3 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "action : 0\n",
      "new state: [2 2 3 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "reward: -0.5\n",
      "epReward so far: -1.25\n",
      "state : [2 2 3 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "action : 2\n",
      "new state: [2 2 2 2 0 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 3]\n",
      "reward: -0.5\n",
      "epReward so far: -1.75\n",
      "state : [2 2 2 2 0 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 3]\n",
      "action : 0\n",
      "new state: [3 2 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: -0.75\n",
      "epReward so far: -2.5\n",
      "state : [3 2 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 0\n",
      "new state: [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "reward: -0.75\n",
      "epReward so far: -3.25\n",
      "final state: [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "Episode : 52\n",
      "state : [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "action : 1\n",
      "new state: [2 2 2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [2 2 2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "action : 3\n",
      "new state: [3 2 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "reward: -0.75\n",
      "epReward so far: -0.5\n",
      "state : [3 2 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "action : 0\n",
      "new state: [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "reward: 0.0\n",
      "epReward so far: -0.5\n",
      "state : [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "action : 0\n",
      "new state: [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "action : 0\n",
      "new state: [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "final state: [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "Episode : 53\n",
      "state : [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "action : 0\n",
      "new state: [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "action : 1\n",
      "new state: [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "action : 1\n",
      "new state: [2 2 2 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "reward: 0.25\n",
      "epReward so far: -1.25\n",
      "state : [2 2 2 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "action : 3\n",
      "new state: [2 2 2 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "state : [2 2 2 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 3\n",
      "new state: [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "reward: -0.75\n",
      "epReward so far: -2.75\n",
      "final state: [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "Episode : 54\n",
      "state : [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "action : 0\n",
      "new state: [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "action : 0\n",
      "new state: [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "action : 0\n",
      "new state: [2 2 2 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "reward: -0.5\n",
      "epReward so far: -2.0\n",
      "state : [2 2 2 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "action : 3\n",
      "new state: [3 2 2 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "reward: -0.75\n",
      "epReward so far: -2.75\n",
      "state : [3 2 2 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "action : 0\n",
      "new state: [4 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "reward: -0.75\n",
      "epReward so far: -3.5\n",
      "final state: [4 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "Episode : 55\n",
      "state : [4 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "action : 0\n",
      "new state: [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "action : 0\n",
      "new state: [2 2 2 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: -0.5\n",
      "epReward so far: -1.25\n",
      "state : [2 2 2 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 3\n",
      "new state: [2 2 3 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "state : [2 2 3 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "action : 2\n",
      "new state: [4 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "reward: -0.75\n",
      "epReward so far: -2.75\n",
      "state : [4 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "action : 0\n",
      "new state: [3 2 2 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "reward: -0.5\n",
      "epReward so far: -3.25\n",
      "final state: [3 2 2 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "Episode : 56\n",
      "state : [3 2 2 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "action : 0\n",
      "new state: [2 2 2 2 3 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 3]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [2 2 2 2 3 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 3]\n",
      "action : 0\n",
      "new state: [1 2 2 3 3 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "reward: -0.5\n",
      "epReward so far: -1.0\n",
      "state : [1 2 2 3 3 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "action : 3\n",
      "new state: [1 2 2 4 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: -0.75\n",
      "epReward so far: -1.75\n",
      "state : [1 2 2 4 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 3\n",
      "new state: [1 2 2 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "reward: -0.0\n",
      "epReward so far: -1.75\n",
      "state : [1 2 2 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "action : 3\n",
      "new state: [1 2 2 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "reward: -0.75\n",
      "epReward so far: -2.5\n",
      "final state: [1 2 2 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "Episode : 57\n",
      "state : [1 2 2 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "action : 3\n",
      "new state: [1 2 2 4 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [1 2 2 4 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "action : 3\n",
      "new state: [1 2 2 4 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [1 2 2 4 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "action : 3\n",
      "new state: [1 2 3 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: 0.25\n",
      "epReward so far: -1.0\n",
      "state : [1 2 3 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 2\n",
      "new state: [1 2 2 3 0 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "reward: -0.5\n",
      "epReward so far: -1.5\n",
      "state : [1 2 2 3 0 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "action : 3\n",
      "new state: [2 2 2 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: -0.75\n",
      "epReward so far: -2.25\n",
      "final state: [2 2 2 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "Episode : 58\n",
      "state : [2 2 2 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 3\n",
      "new state: [2 3 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [2 3 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 1\n",
      "new state: [2 3 2 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: 0.0\n",
      "epReward so far: -0.5\n",
      "state : [2 3 2 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 1\n",
      "new state: [2 2 3 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "reward: -0.5\n",
      "epReward so far: -1.0\n",
      "state : [2 2 3 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "action : 2\n",
      "new state: [2 2 3 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "reward: -0.0\n",
      "epReward so far: -1.0\n",
      "state : [2 2 3 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "action : 2\n",
      "new state: [3 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "reward: -0.0\n",
      "epReward so far: -1.0\n",
      "final state: [3 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "Episode : 59\n",
      "state : [3 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "action : 0\n",
      "new state: [3 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [3 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "action : 0\n",
      "new state: [3 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [3 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "action : 0\n",
      "new state: [2 2 3 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "reward: -0.5\n",
      "epReward so far: -2.0\n",
      "state : [2 2 3 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "action : 2\n",
      "new state: [2 2 2 2 3 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 3]\n",
      "reward: -0.5\n",
      "epReward so far: -2.5\n",
      "state : [2 2 2 2 3 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 3]\n",
      "action : 0\n",
      "new state: [1 2 2 3 3 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1]\n",
      "reward: -0.5\n",
      "epReward so far: -3.0\n",
      "final state: [1 2 2 3 3 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1]\n",
      "Episode : 60\n",
      "state : [1 2 2 3 3 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1]\n",
      "action : 3\n",
      "new state: [2 2 2 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 2 2 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "reward: 0.0\n",
      "epReward so far: -0.75\n",
      "state : [2 2 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "action : 3\n",
      "new state: [2 2 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [2 2 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "action : 3\n",
      "new state: [2 2 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "reward: -0.75\n",
      "epReward so far: -2.25\n",
      "state : [2 2 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "reward: -0.75\n",
      "epReward so far: -3.0\n",
      "final state: [2 2 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "Episode : 61\n",
      "state : [2 2 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "action : 3\n",
      "new state: [2 2 2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [2 2 2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "action : 3\n",
      "new state: [2 2 2 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [2 2 2 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 3\n",
      "new state: [2 2 2 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: 0.25\n",
      "epReward so far: -1.0\n",
      "state : [2 2 2 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 3\n",
      "new state: [2 2 2 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "reward: -0.75\n",
      "epReward so far: -1.75\n",
      "state : [2 2 2 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "action : 3\n",
      "new state: [2 2 3 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: -0.5\n",
      "epReward so far: -2.25\n",
      "final state: [2 2 3 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "Episode : 62\n",
      "state : [2 2 3 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 2\n",
      "new state: [2 2 2 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 2 2 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 3\n",
      "new state: [2 2 3 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "reward: -0.5\n",
      "epReward so far: -1.25\n",
      "state : [2 2 3 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "action : 2\n",
      "new state: [2 2 3 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "state : [2 2 3 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "action : 2\n",
      "new state: [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "reward: -0.75\n",
      "epReward so far: -2.75\n",
      "state : [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "reward: -0.75\n",
      "epReward so far: -3.5\n",
      "final state: [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "Episode : 63\n",
      "state : [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "action : 2\n",
      "new state: [2 2 2 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [2 2 2 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "action : 3\n",
      "new state: [2 2 2 2 1 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 2]\n",
      "reward: 0.25\n",
      "epReward so far: -0.25\n",
      "state : [2 2 2 2 1 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 2]\n",
      "action : 0\n",
      "new state: [1 3 2 2 2 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "reward: -0.5\n",
      "epReward so far: -0.75\n",
      "state : [1 3 2 2 2 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "action : 1\n",
      "new state: [2 3 2 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [2 3 2 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "action : 1\n",
      "new state: [2 2 3 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "reward: 0.25\n",
      "epReward so far: -1.25\n",
      "final state: [2 2 3 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "Episode : 64\n",
      "state : [2 2 3 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "action : 2\n",
      "new state: [2 2 2 2 2 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 3]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [2 2 2 2 2 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 3]\n",
      "action : 0\n",
      "new state: [2 2 3 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [2 2 3 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "action : 2\n",
      "new state: [2 2 3 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "reward: -0.5\n",
      "epReward so far: -1.75\n",
      "state : [2 2 3 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "action : 2\n",
      "new state: [2 2 3 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: -0.75\n",
      "epReward so far: -2.5\n",
      "state : [2 2 3 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 2\n",
      "new state: [3 2 2 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: -0.5\n",
      "epReward so far: -3.0\n",
      "final state: [3 2 2 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "Episode : 65\n",
      "state : [3 2 2 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 0\n",
      "new state: [3 2 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [3 2 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 0\n",
      "new state: [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "action : 0\n",
      "new state: [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "reward: -0.0\n",
      "epReward so far: -1.5\n",
      "state : [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "action : 0\n",
      "new state: [2 2 2 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: -0.5\n",
      "epReward so far: -2.0\n",
      "state : [2 2 2 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 3\n",
      "new state: [2 2 2 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: 0.0\n",
      "epReward so far: -2.0\n",
      "final state: [2 2 2 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "Episode : 66\n",
      "state : [2 2 2 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 3\n",
      "new state: [2 3 2 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [2 3 2 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 1\n",
      "new state: [2 3 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "reward: 0.0\n",
      "epReward so far: 0.25\n",
      "state : [2 3 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "action : 1\n",
      "new state: [2 3 2 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: -0.5\n",
      "epReward so far: -0.25\n",
      "state : [2 3 2 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 1\n",
      "new state: [2 3 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: -0.75\n",
      "epReward so far: -1.0\n",
      "state : [2 3 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 1\n",
      "new state: [2 3 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: -0.5\n",
      "epReward so far: -1.5\n",
      "final state: [2 3 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "Episode : 67\n",
      "state : [2 3 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 1\n",
      "new state: [2 3 2 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 3 2 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 1\n",
      "new state: [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "reward: 0.0\n",
      "epReward so far: -0.75\n",
      "state : [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "action : 1\n",
      "new state: [2 2 3 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: -0.5\n",
      "epReward so far: -1.25\n",
      "state : [2 2 3 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 2\n",
      "new state: [2 2 3 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: 0.0\n",
      "epReward so far: -1.25\n",
      "state : [2 2 3 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 2\n",
      "new state: [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "reward: -0.0\n",
      "epReward so far: -1.25\n",
      "final state: [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "Episode : 68\n",
      "state : [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "action : 1\n",
      "new state: [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "action : 1\n",
      "new state: [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "action : 1\n",
      "new state: [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "reward: -0.75\n",
      "epReward so far: -2.25\n",
      "state : [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "action : 1\n",
      "new state: [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "reward: -0.75\n",
      "epReward so far: -3.0\n",
      "state : [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "action : 1\n",
      "new state: [2 2 3 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "reward: -0.5\n",
      "epReward so far: -3.5\n",
      "final state: [2 2 3 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "Episode : 69\n",
      "state : [2 2 3 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "action : 2\n",
      "new state: [2 2 2 2 3 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [2 2 2 2 3 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "action : 0\n",
      "new state: [1 2 2 3 2 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0]\n",
      "reward: 0.25\n",
      "epReward so far: -0.25\n",
      "state : [1 2 2 3 2 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0]\n",
      "action : 3\n",
      "new state: [2 2 2 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: -0.75\n",
      "epReward so far: -1.0\n",
      "state : [2 2 2 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 3\n",
      "new state: [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "reward: -0.0\n",
      "epReward so far: -1.0\n",
      "state : [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "action : 2\n",
      "new state: [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.75\n",
      "final state: [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "Episode : 70\n",
      "state : [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "action : 2\n",
      "new state: [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "action : 2\n",
      "new state: [2 2 2 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "reward: -0.5\n",
      "epReward so far: -1.25\n",
      "state : [2 2 2 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "action : 3\n",
      "new state: [2 2 2 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "state : [2 2 2 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "action : 3\n",
      "new state: [2 2 3 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: -0.5\n",
      "epReward so far: -2.5\n",
      "state : [2 2 3 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 2\n",
      "new state: [2 2 3 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "reward: -0.75\n",
      "epReward so far: -3.25\n",
      "final state: [2 2 3 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "Episode : 71\n",
      "state : [2 2 3 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "action : 2\n",
      "new state: [2 2 3 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [2 2 3 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "action : 2\n",
      "new state: [2 2 2 2 1 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "reward: -0.5\n",
      "epReward so far: -0.25\n",
      "state : [2 2 2 2 1 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "action : 0\n",
      "new state: [2 3 2 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "reward: -0.75\n",
      "epReward so far: -1.0\n",
      "state : [2 3 2 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "action : 1\n",
      "new state: [3 2 2 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "reward: -0.5\n",
      "epReward so far: -1.5\n",
      "state : [3 2 2 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "action : 0\n",
      "new state: [3 2 2 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: 0.0\n",
      "epReward so far: -1.5\n",
      "final state: [3 2 2 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "Episode : 72\n",
      "state : [3 2 2 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 0\n",
      "new state: [4 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [4 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "action : 0\n",
      "new state: [3 2 2 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [3 2 2 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 0\n",
      "new state: [3 2 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [3 2 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "action : 0\n",
      "new state: [3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "reward: 0.0\n",
      "epReward so far: -1.25\n",
      "state : [3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "action : 0\n",
      "new state: [2 3 2 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: -0.5\n",
      "epReward so far: -1.75\n",
      "final state: [2 3 2 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "Episode : 73\n",
      "state : [2 3 2 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 1\n",
      "new state: [2 3 2 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 3 2 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 1\n",
      "new state: [3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "reward: -0.0\n",
      "epReward so far: -0.75\n",
      "state : [3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "action : 0\n",
      "new state: [2 3 2 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: -0.5\n",
      "epReward so far: -1.25\n",
      "state : [2 3 2 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 1\n",
      "new state: [2 3 2 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: -1.25\n",
      "state : [2 3 2 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 1\n",
      "new state: [3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "final state: [3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "Episode : 74\n",
      "state : [3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "action : 0\n",
      "new state: [2 3 2 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [2 3 2 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "action : 1\n",
      "new state: [2 2 2 2 0 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 1]\n",
      "reward: -0.5\n",
      "epReward so far: -1.0\n",
      "state : [2 2 2 2 0 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 1]\n",
      "action : 0\n",
      "new state: [2 2 2 2 1 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0]\n",
      "reward: 0.25\n",
      "epReward so far: -0.75\n",
      "state : [2 2 2 2 1 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0]\n",
      "action : 0\n",
      "new state: [1 3 2 2 1 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0]\n",
      "reward: -0.5\n",
      "epReward so far: -1.25\n",
      "state : [1 3 2 2 1 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0]\n",
      "action : 1\n",
      "new state: [1 3 2 2 0 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "reward: 0.25\n",
      "epReward so far: -1.0\n",
      "final state: [1 3 2 2 0 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "Episode : 75\n",
      "state : [1 3 2 2 0 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "action : 1\n",
      "new state: [2 3 2 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 3 2 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 1\n",
      "new state: [3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "action : 0\n",
      "new state: [3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "reward: -0.75\n",
      "epReward so far: -2.25\n",
      "state : [3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "action : 0\n",
      "new state: [3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "reward: -0.75\n",
      "epReward so far: -3.0\n",
      "state : [3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "action : 0\n",
      "new state: [2 3 2 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: 0.25\n",
      "epReward so far: -2.75\n",
      "final state: [2 3 2 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "Episode : 76\n",
      "state : [2 3 2 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 1\n",
      "new state: [2 3 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 3 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 1\n",
      "new state: [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "action : 1\n",
      "new state: [2 2 2 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "reward: 0.25\n",
      "epReward so far: -1.25\n",
      "state : [2 2 2 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "action : 3\n",
      "new state: [2 2 2 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "state : [2 2 2 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 3\n",
      "new state: [3 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: 0.25\n",
      "epReward so far: -1.75\n",
      "final state: [3 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "Episode : 77\n",
      "state : [3 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 0\n",
      "new state: [2 2 2 2 2 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [2 2 2 2 2 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "action : 0\n",
      "new state: [1 2 3 2 1 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 0]\n",
      "reward: -0.5\n",
      "epReward so far: -0.25\n",
      "state : [1 2 3 2 1 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 0]\n",
      "action : 2\n",
      "new state: [1 2 2 3 1 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "reward: -0.5\n",
      "epReward so far: -0.75\n",
      "state : [1 2 2 3 1 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "action : 3\n",
      "new state: [1 3 2 2 1 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 3]\n",
      "reward: 0.25\n",
      "epReward so far: -0.5\n",
      "state : [1 3 2 2 1 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 3]\n",
      "action : 1\n",
      "new state: [2 3 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "final state: [2 3 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "Episode : 78\n",
      "state : [2 3 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 1\n",
      "new state: [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "action : 1\n",
      "new state: [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "action : 1\n",
      "new state: [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "reward: -0.0\n",
      "epReward so far: -1.5\n",
      "state : [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "action : 1\n",
      "new state: [2 2 2 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "reward: -0.5\n",
      "epReward so far: -2.0\n",
      "state : [2 2 2 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "action : 3\n",
      "new state: [2 2 2 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: -0.75\n",
      "epReward so far: -2.75\n",
      "final state: [2 2 2 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "Episode : 79\n",
      "state : [2 2 2 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 3\n",
      "new state: [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "action : 1\n",
      "new state: [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "action : 1\n",
      "new state: [2 2 2 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [2 2 2 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 3\n",
      "new state: [2 2 2 2 1 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 1]\n",
      "reward: -0.5\n",
      "epReward so far: -1.0\n",
      "state : [2 2 2 2 1 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 1]\n",
      "action : 0\n",
      "new state: [2 3 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "reward: -0.0\n",
      "epReward so far: -1.0\n",
      "final state: [2 3 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "Episode : 80\n",
      "state : [2 3 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "action : 1\n",
      "new state: [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "action : 1\n",
      "new state: [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "action : 1\n",
      "new state: [2 2 2 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: -0.5\n",
      "epReward so far: -1.25\n",
      "state : [2 2 2 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 3\n",
      "new state: [2 2 2 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: 0.0\n",
      "epReward so far: -1.25\n",
      "state : [2 2 2 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 3\n",
      "new state: [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "reward: 0.0\n",
      "epReward so far: -1.25\n",
      "final state: [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "Episode : 81\n",
      "state : [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "action : 2\n",
      "new state: [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "action : 2\n",
      "new state: [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "action : 2\n",
      "new state: [2 2 2 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: 0.25\n",
      "epReward so far: -0.5\n",
      "state : [2 2 2 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 3\n",
      "new state: [2 2 3 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [2 2 3 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "action : 2\n",
      "new state: [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "final state: [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "Episode : 82\n",
      "state : [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "action : 1\n",
      "new state: [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "action : 1\n",
      "new state: [2 2 3 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: -0.5\n",
      "epReward so far: -1.25\n",
      "state : [2 2 3 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 2\n",
      "new state: [2 2 2 2 1 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "reward: -0.5\n",
      "epReward so far: -1.75\n",
      "state : [2 2 2 2 1 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "action : 0\n",
      "new state: [1 3 2 2 1 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "reward: -0.5\n",
      "epReward so far: -2.25\n",
      "state : [1 3 2 2 1 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "action : 1\n",
      "new state: [1 3 3 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: -3.0\n",
      "final state: [1 3 3 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "Episode : 83\n",
      "state : [1 3 3 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 1\n",
      "new state: [1 4 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "reward: 0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 4 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "action : 1\n",
      "new state: [1 4 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 4 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "action : 1\n",
      "new state: [1 3 3 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [1 3 3 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 1\n",
      "new state: [1 2 3 2 1 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "reward: -0.5\n",
      "epReward so far: -1.0\n",
      "state : [1 2 3 2 1 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "action : 2\n",
      "new state: [1 3 2 2 1 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "reward: -0.5\n",
      "epReward so far: -1.5\n",
      "final state: [1 3 2 2 1 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "Episode : 84\n",
      "state : [1 3 2 2 1 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "action : 1\n",
      "new state: [2 3 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 3 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 1\n",
      "new state: [2 3 2 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: -0.5\n",
      "epReward so far: -1.25\n",
      "state : [2 3 2 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 1\n",
      "new state: [2 2 2 2 3 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 0]\n",
      "reward: -0.5\n",
      "epReward so far: -1.75\n",
      "state : [2 2 2 2 3 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 0]\n",
      "action : 0\n",
      "new state: [2 2 2 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: -0.75\n",
      "epReward so far: -2.5\n",
      "state : [2 2 2 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 3\n",
      "new state: [2 2 3 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: -0.5\n",
      "epReward so far: -3.0\n",
      "final state: [2 2 3 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "Episode : 85\n",
      "state : [2 2 3 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 2\n",
      "new state: [2 2 2 2 2 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [2 2 2 2 2 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 0]\n",
      "action : 0\n",
      "new state: [2 2 3 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.5\n",
      "state : [2 2 3 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 2\n",
      "new state: [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "action : 1\n",
      "new state: [2 2 2 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: 0.25\n",
      "epReward so far: -1.0\n",
      "state : [2 2 2 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 3\n",
      "new state: [2 2 2 2 0 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "reward: -0.5\n",
      "epReward so far: -1.5\n",
      "final state: [2 2 2 2 0 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "Episode : 86\n",
      "state : [2 2 2 2 0 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "action : 0\n",
      "new state: [3 2 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [3 2 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "action : 0\n",
      "new state: [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "action : 0\n",
      "new state: [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "reward: 0.0\n",
      "epReward so far: -1.5\n",
      "state : [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "action : 0\n",
      "new state: [2 2 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "reward: -0.75\n",
      "epReward so far: -2.25\n",
      "state : [2 2 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "action : 3\n",
      "new state: [2 2 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "reward: -0.75\n",
      "epReward so far: -3.0\n",
      "final state: [2 2 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "Episode : 87\n",
      "state : [2 2 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "action : 3\n",
      "new state: [2 2 2 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [2 2 2 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "action : 3\n",
      "new state: [2 2 2 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [2 2 2 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 3\n",
      "new state: [2 3 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "reward: 0.25\n",
      "epReward so far: -1.0\n",
      "state : [2 3 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "action : 1\n",
      "new state: [2 2 2 2 2 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "reward: 0.25\n",
      "epReward so far: -0.75\n",
      "state : [2 2 2 2 2 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "action : 0\n",
      "new state: [2 2 3 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "reward: -0.0\n",
      "epReward so far: -0.75\n",
      "final state: [2 2 3 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "Episode : 88\n",
      "state : [2 2 3 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "action : 2\n",
      "new state: [2 2 3 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [2 2 3 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 2\n",
      "new state: [2 2 3 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [2 2 3 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 2\n",
      "new state: [2 2 3 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: 0.25\n",
      "epReward so far: -1.0\n",
      "state : [2 2 3 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 2\n",
      "new state: [2 2 2 2 0 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 2]\n",
      "reward: 0.25\n",
      "epReward so far: -0.75\n",
      "state : [2 2 2 2 0 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 2]\n",
      "action : 0\n",
      "new state: [2 2 2 2 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "reward: -0.5\n",
      "epReward so far: -1.25\n",
      "final state: [2 2 2 2 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "Episode : 89\n",
      "state : [2 2 2 2 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "action : 0\n",
      "new state: [2 2 2 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 2 2 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "action : 3\n",
      "new state: [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "action : 2\n",
      "new state: [2 2 2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: 0.25\n",
      "epReward so far: -0.5\n",
      "state : [2 2 2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 3\n",
      "new state: [2 2 2 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [2 2 2 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 3\n",
      "new state: [2 2 2 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: 0.25\n",
      "epReward so far: -1.0\n",
      "final state: [2 2 2 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "Episode : 90\n",
      "state : [2 2 2 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 3\n",
      "new state: [2 2 2 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 2 2 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 3\n",
      "new state: [2 3 2 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "reward: -0.5\n",
      "epReward so far: -1.25\n",
      "state : [2 3 2 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "action : 1\n",
      "new state: [2 2 2 2 3 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 3]\n",
      "reward: 0.25\n",
      "epReward so far: -1.0\n",
      "state : [2 2 2 2 3 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 3]\n",
      "action : 0\n",
      "new state: [1 2 2 4 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.75\n",
      "state : [1 2 2 4 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "action : 3\n",
      "new state: [1 2 3 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: -2.5\n",
      "final state: [1 2 3 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "Episode : 91\n",
      "state : [1 2 3 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "action : 3\n",
      "new state: [1 2 3 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [1 2 3 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "action : 3\n",
      "new state: [1 2 3 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: -0.5\n",
      "epReward so far: -1.25\n",
      "state : [1 2 3 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 2\n",
      "new state: [1 2 3 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "state : [1 2 3 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 2\n",
      "new state: [1 3 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "reward: -0.75\n",
      "epReward so far: -2.75\n",
      "state : [1 3 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "action : 1\n",
      "new state: [1 3 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "reward: -0.75\n",
      "epReward so far: -3.5\n",
      "final state: [1 3 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "Episode : 92\n",
      "state : [1 3 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "action : 1\n",
      "new state: [1 2 3 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [1 2 3 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "action : 3\n",
      "new state: [1 2 3 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "reward: -0.5\n",
      "epReward so far: -1.25\n",
      "state : [1 2 3 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "action : 2\n",
      "new state: [1 2 2 3 1 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "reward: -0.5\n",
      "epReward so far: -1.75\n",
      "state : [1 2 2 3 1 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "action : 3\n",
      "new state: [1 3 2 2 2 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 1]\n",
      "reward: -0.5\n",
      "epReward so far: -2.25\n",
      "state : [1 3 2 2 2 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 1]\n",
      "action : 1\n",
      "new state: [1 2 3 2 2 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 2]\n",
      "reward: -0.5\n",
      "epReward so far: -2.75\n",
      "final state: [1 2 3 2 2 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 2]\n",
      "Episode : 93\n",
      "state : [1 2 3 2 2 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 2]\n",
      "action : 2\n",
      "new state: [1 2 3 2 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [1 2 3 2 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "action : 2\n",
      "new state: [1 3 3 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [1 3 3 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "action : 1\n",
      "new state: [2 2 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "state : [2 2 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "action : 2\n",
      "new state: [2 2 3 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: -0.5\n",
      "epReward so far: -2.5\n",
      "state : [2 2 3 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 2\n",
      "new state: [2 3 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: -0.75\n",
      "epReward so far: -3.25\n",
      "final state: [2 3 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "Episode : 94\n",
      "state : [2 3 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 1\n",
      "new state: [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "action : 1\n",
      "new state: [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "action : 1\n",
      "new state: [2 2 3 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "reward: -0.5\n",
      "epReward so far: -2.0\n",
      "state : [2 2 3 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "action : 2\n",
      "new state: [2 2 2 2 1 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "reward: 0.25\n",
      "epReward so far: -1.75\n",
      "state : [2 2 2 2 1 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "action : 0\n",
      "new state: [1 3 2 2 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "reward: -0.5\n",
      "epReward so far: -2.25\n",
      "final state: [1 3 2 2 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "Episode : 95\n",
      "state : [1 3 2 2 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "action : 1\n",
      "new state: [1 4 2 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [1 4 2 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "action : 1\n",
      "new state: [1 3 3 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "reward: -0.5\n",
      "epReward so far: -1.25\n",
      "state : [1 3 3 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "action : 1\n",
      "new state: [1 2 3 2 0 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "reward: 0.25\n",
      "epReward so far: -1.0\n",
      "state : [1 2 3 2 0 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "action : 2\n",
      "new state: [2 2 3 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.75\n",
      "state : [2 2 3 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "action : 2\n",
      "new state: [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "reward: -0.75\n",
      "epReward so far: -2.5\n",
      "final state: [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "Episode : 96\n",
      "state : [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "action : 2\n",
      "new state: [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "action : 2\n",
      "new state: [2 2 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 2 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "action : 3\n",
      "new state: [2 2 2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: -0.5\n",
      "epReward so far: -1.25\n",
      "state : [2 2 2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 3\n",
      "new state: [2 2 2 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "reward: 0.0\n",
      "epReward so far: -1.25\n",
      "state : [2 2 2 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "action : 3\n",
      "new state: [2 2 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "final state: [2 2 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "Episode : 97\n",
      "state : [2 2 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 2 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "action : 3\n",
      "new state: [2 2 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [2 2 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "reward: -0.75\n",
      "epReward so far: -2.25\n",
      "state : [2 2 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "reward: 0.0\n",
      "epReward so far: -2.25\n",
      "state : [2 2 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "reward: -0.0\n",
      "epReward so far: -2.25\n",
      "final state: [2 2 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "Episode : 98\n",
      "state : [2 2 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "action : 3\n",
      "new state: [2 2 2 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [2 2 2 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "action : 3\n",
      "new state: [2 2 2 2 2 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "reward: -0.5\n",
      "epReward so far: -0.25\n",
      "state : [2 2 2 2 2 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "action : 0\n",
      "new state: [2 2 3 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: 0.0\n",
      "epReward so far: -0.25\n",
      "state : [2 2 3 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 2\n",
      "new state: [2 3 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: -0.5\n",
      "epReward so far: -0.75\n",
      "state : [2 3 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 1\n",
      "new state: [2 3 2 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: 0.0\n",
      "epReward so far: -0.75\n",
      "final state: [2 3 2 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "Episode : 99\n",
      "state : [2 3 2 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 1\n",
      "new state: [2 2 3 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [2 2 3 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 2\n",
      "new state: [2 2 2 2 3 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 3]\n",
      "reward: -0.5\n",
      "epReward so far: -1.0\n",
      "state : [2 2 2 2 3 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 3]\n",
      "action : 0\n",
      "new state: [1 2 2 3 3 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "reward: -0.5\n",
      "epReward so far: -1.5\n",
      "state : [1 2 2 3 3 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "action : 3\n",
      "new state: [1 2 3 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "reward: -0.75\n",
      "epReward so far: -2.25\n",
      "state : [1 2 3 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "action : 2\n",
      "new state: [1 2 3 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "reward: -0.75\n",
      "epReward so far: -3.0\n",
      "final state: [1 2 3 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "**************************************************\n",
      "Experiment complete\n",
      "**************************************************\n",
      "**************************************************\n",
      "Saving data\n",
      "**************************************************\n",
      "[[ 0.00000000e+00  0.00000000e+00 -2.75000000e+00  3.16490000e+04\n",
      "  -4.51936251e+00]\n",
      " [ 1.00000000e+00  0.00000000e+00 -3.00000000e+00  1.49350000e+04\n",
      "  -4.57349173e+00]\n",
      " [ 2.00000000e+00  0.00000000e+00 -2.75000000e+00  1.50720000e+04\n",
      "  -3.93461712e+00]\n",
      " [ 3.00000000e+00  0.00000000e+00 -3.00000000e+00  1.63460000e+04\n",
      "  -4.46601994e+00]\n",
      " [ 4.00000000e+00  0.00000000e+00 -2.50000000e+00  1.54540000e+04\n",
      "  -4.54144208e+00]\n",
      " [ 5.00000000e+00  0.00000000e+00 -2.00000000e+00  1.35010000e+04\n",
      "  -4.50196587e+00]\n",
      " [ 6.00000000e+00  0.00000000e+00 -1.50000000e+00  1.36290000e+04\n",
      "  -4.77881860e+00]\n",
      " [ 7.00000000e+00  0.00000000e+00 -3.25000000e+00  1.39990000e+04\n",
      "  -4.54030187e+00]\n",
      " [ 8.00000000e+00  0.00000000e+00 -1.75000000e+00  1.58560000e+04\n",
      "  -4.64186114e+00]\n",
      " [ 9.00000000e+00  0.00000000e+00 -2.00000000e+00  1.68960000e+04\n",
      "  -4.94640399e+00]\n",
      " [ 1.00000000e+01  0.00000000e+00 -2.00000000e+00  1.59010000e+04\n",
      "  -4.53315492e+00]\n",
      " [ 1.10000000e+01  0.00000000e+00 -2.75000000e+00  1.73050000e+04\n",
      "  -4.47345337e+00]\n",
      " [ 1.20000000e+01  0.00000000e+00 -2.50000000e+00  1.55880000e+04\n",
      "  -4.35799521e+00]\n",
      " [ 1.30000000e+01  0.00000000e+00 -2.25000000e+00  4.86300000e+03\n",
      "  -4.53920759e+00]\n",
      " [ 1.40000000e+01  0.00000000e+00 -3.25000000e+00  1.60160000e+04\n",
      "  -4.52322107e+00]\n",
      " [ 1.50000000e+01  0.00000000e+00 -2.50000000e+00  6.15700000e+03\n",
      "  -4.48810627e+00]\n",
      " [ 1.60000000e+01  0.00000000e+00 -1.50000000e+00  1.20350000e+04\n",
      "  -4.57162251e+00]\n",
      " [ 1.70000000e+01  0.00000000e+00 -3.50000000e+00  5.97600000e+03\n",
      "  -4.27435518e+00]\n",
      " [ 1.80000000e+01  0.00000000e+00 -2.75000000e+00  1.41290000e+04\n",
      "  -4.42159073e+00]\n",
      " [ 1.90000000e+01  0.00000000e+00 -2.75000000e+00  3.83300000e+03\n",
      "  -4.23389290e+00]\n",
      " [ 2.00000000e+01  0.00000000e+00 -3.25000000e+00  1.39690000e+04\n",
      "  -3.94423499e+00]\n",
      " [ 2.10000000e+01  0.00000000e+00 -3.00000000e+00  4.84000000e+03\n",
      "  -4.21706166e+00]\n",
      " [ 2.20000000e+01  0.00000000e+00 -1.25000000e+00  3.39900000e+03\n",
      "  -4.48189047e+00]\n",
      " [ 2.30000000e+01  0.00000000e+00 -2.50000000e+00  1.83120000e+04\n",
      "  -4.06996871e+00]\n",
      " [ 2.40000000e+01  0.00000000e+00 -3.50000000e+00  3.83200000e+03\n",
      "  -3.87491962e+00]\n",
      " [ 2.50000000e+01  0.00000000e+00 -3.50000000e+00  5.88100000e+03\n",
      "  -4.14914972e+00]\n",
      " [ 2.60000000e+01  0.00000000e+00 -1.25000000e+00  5.88400000e+03\n",
      "  -3.82393134e+00]\n",
      " [ 2.70000000e+01  0.00000000e+00 -2.75000000e+00  5.03300000e+03\n",
      "  -4.45744772e+00]\n",
      " [ 2.80000000e+01  0.00000000e+00 -2.25000000e+00  7.70000000e+03\n",
      "  -4.54842342e+00]\n",
      " [ 2.90000000e+01  0.00000000e+00 -1.50000000e+00  1.14520000e+04\n",
      "  -4.60229043e+00]\n",
      " [ 3.00000000e+01  0.00000000e+00 -2.00000000e+00  8.73700000e+03\n",
      "  -4.18678021e+00]\n",
      " [ 3.10000000e+01  0.00000000e+00 -3.50000000e+00  5.93700000e+03\n",
      "  -3.92182314e+00]\n",
      " [ 3.20000000e+01  0.00000000e+00 -3.25000000e+00  8.07200000e+03\n",
      "  -3.35003701e+00]\n",
      " [ 3.30000000e+01  0.00000000e+00 -1.75000000e+00  7.23100000e+03\n",
      "  -3.52149956e+00]\n",
      " [ 3.40000000e+01  0.00000000e+00 -2.25000000e+00  7.95000000e+03\n",
      "  -4.40279828e+00]\n",
      " [ 3.50000000e+01  0.00000000e+00 -2.50000000e+00  9.07100000e+03\n",
      "  -1.12416473e+00]\n",
      " [ 3.60000000e+01  0.00000000e+00 -2.00000000e+00  5.59500000e+03\n",
      "  -4.95245911e+00]\n",
      " [ 3.70000000e+01  0.00000000e+00 -2.75000000e+00  8.83800000e+03\n",
      "  -4.86446690e+00]\n",
      " [ 3.80000000e+01  0.00000000e+00 -1.25000000e+00  6.39400000e+03\n",
      "  -5.00329611e+00]\n",
      " [ 3.90000000e+01  0.00000000e+00 -2.00000000e+00  7.29400000e+03\n",
      "  -4.98145459e+00]\n",
      " [ 4.00000000e+01  0.00000000e+00 -2.75000000e+00  9.23100000e+03\n",
      "  -4.74088815e+00]\n",
      " [ 4.10000000e+01  0.00000000e+00 -2.75000000e+00  7.73500000e+03\n",
      "  -4.85686370e+00]\n",
      " [ 4.20000000e+01  0.00000000e+00 -2.75000000e+00  1.56720000e+04\n",
      "  -4.94905700e+00]\n",
      " [ 4.30000000e+01  0.00000000e+00 -2.25000000e+00  8.74100000e+03\n",
      "  -4.87437273e+00]\n",
      " [ 4.40000000e+01  0.00000000e+00 -1.00000000e+00  9.91800000e+03\n",
      "  -4.80133529e+00]\n",
      " [ 4.50000000e+01  0.00000000e+00 -1.50000000e+00  6.69500000e+03\n",
      "  -4.94078544e+00]\n",
      " [ 4.60000000e+01  0.00000000e+00 -2.75000000e+00  1.70460000e+04\n",
      "  -4.87555931e+00]\n",
      " [ 4.70000000e+01  0.00000000e+00 -1.75000000e+00  1.15660000e+04\n",
      "  -5.04457233e+00]\n",
      " [ 4.80000000e+01  0.00000000e+00 -2.75000000e+00  5.56900000e+03\n",
      "  -5.01249765e+00]\n",
      " [ 4.90000000e+01  0.00000000e+00 -1.75000000e+00  1.36860000e+04\n",
      "  -5.01310693e+00]\n",
      " [ 5.00000000e+01  0.00000000e+00 -1.25000000e+00  9.88700000e+03\n",
      "  -5.13536795e+00]\n",
      " [ 5.10000000e+01  0.00000000e+00 -1.25000000e+00  1.47810000e+04\n",
      "  -4.92401834e+00]\n",
      " [ 5.20000000e+01  0.00000000e+00 -3.00000000e+00  5.40100000e+03\n",
      "  -5.22083731e+00]\n",
      " [ 5.30000000e+01  0.00000000e+00 -2.75000000e+00  7.04000000e+03\n",
      "  -2.30010956e+00]\n",
      " [ 5.40000000e+01  0.00000000e+00 -2.00000000e+00  1.25960000e+04\n",
      "  -4.91865429e+00]\n",
      " [ 5.50000000e+01  0.00000000e+00 -3.00000000e+00  5.85300000e+03\n",
      "  -4.44286919e+00]\n",
      " [ 5.60000000e+01  0.00000000e+00 -2.50000000e+00  1.11420000e+04\n",
      "  -5.03166987e+00]\n",
      " [ 5.70000000e+01  0.00000000e+00 -3.25000000e+00  7.18400000e+03\n",
      "  -4.92677696e+00]\n",
      " [ 5.80000000e+01  0.00000000e+00 -1.75000000e+00  9.30200000e+03\n",
      "  -4.83232817e+00]\n",
      " [ 5.90000000e+01  0.00000000e+00 -3.75000000e+00  1.03140000e+04\n",
      "  -5.06200107e+00]\n",
      " [ 6.00000000e+01  0.00000000e+00 -2.25000000e+00  1.02860000e+04\n",
      "  -4.97608522e+00]\n",
      " [ 6.10000000e+01  0.00000000e+00 -2.00000000e+00  1.14150000e+04\n",
      "  -5.27305773e+00]\n",
      " [ 6.20000000e+01  0.00000000e+00 -2.50000000e+00  9.09300000e+03\n",
      "  -5.12589226e+00]\n",
      " [ 6.30000000e+01  0.00000000e+00 -2.75000000e+00  1.18320000e+04\n",
      "  -5.31299286e+00]\n",
      " [ 6.40000000e+01  0.00000000e+00 -1.75000000e+00  1.03200000e+04\n",
      "  -5.20299297e+00]\n",
      " [ 6.50000000e+01  0.00000000e+00 -2.75000000e+00  4.88800000e+03\n",
      "  -5.11579016e+00]\n",
      " [ 6.60000000e+01  0.00000000e+00 -1.50000000e+00  1.19160000e+04\n",
      "  -5.15281868e+00]\n",
      " [ 6.70000000e+01  0.00000000e+00 -2.50000000e+00  1.16630000e+04\n",
      "  -5.20590152e+00]\n",
      " [ 6.80000000e+01  0.00000000e+00 -7.50000000e-01  7.04300000e+03\n",
      "  -5.24608610e+00]\n",
      " [ 6.90000000e+01  0.00000000e+00 -2.75000000e+00  6.78500000e+03\n",
      "  -5.20087116e+00]\n",
      " [ 7.00000000e+01  0.00000000e+00 -2.50000000e+00  1.58320000e+04\n",
      "  -5.06388523e+00]\n",
      " [ 7.10000000e+01  0.00000000e+00 -3.25000000e+00  7.04900000e+03\n",
      "  -5.14784263e+00]\n",
      " [ 7.20000000e+01  0.00000000e+00 -2.00000000e+00  2.16280000e+04\n",
      "  -2.61101285e+00]\n",
      " [ 7.30000000e+01  0.00000000e+00 -2.25000000e+00  4.78150000e+04\n",
      "  -5.02937157e+00]\n",
      " [ 7.40000000e+01  0.00000000e+00 -1.75000000e+00  1.39820000e+04\n",
      "  -5.46933148e+00]\n",
      " [ 7.50000000e+01  0.00000000e+00 -3.75000000e+00  1.31060000e+04\n",
      "  -5.36360894e+00]\n",
      " [ 7.60000000e+01  0.00000000e+00 -2.50000000e+00  1.31030000e+04\n",
      "  -5.45612348e+00]\n",
      " [ 7.70000000e+01  0.00000000e+00 -2.00000000e+00  6.88400000e+03\n",
      "  -5.00489495e+00]\n",
      " [ 7.80000000e+01  0.00000000e+00 -3.75000000e+00  1.12980000e+04\n",
      "  -5.25719024e+00]\n",
      " [ 7.90000000e+01  0.00000000e+00 -3.75000000e+00  1.05210000e+04\n",
      "  -5.17854250e+00]\n",
      " [ 8.00000000e+01  0.00000000e+00 -1.25000000e+00  1.30990000e+04\n",
      "  -5.23215391e+00]\n",
      " [ 8.10000000e+01  0.00000000e+00 -2.50000000e+00  1.34950000e+04\n",
      "  -4.59372131e+00]\n",
      " [ 8.20000000e+01  0.00000000e+00 -3.25000000e+00  1.16080000e+04\n",
      "  -4.97843726e+00]\n",
      " [ 8.30000000e+01  0.00000000e+00 -1.25000000e+00  1.01970000e+04\n",
      "  -4.60956770e+00]\n",
      " [ 8.40000000e+01  0.00000000e+00 -1.50000000e+00  8.84500000e+03\n",
      "  -4.82633182e+00]\n",
      " [ 8.50000000e+01  0.00000000e+00 -2.50000000e+00  5.02000000e+03\n",
      "  -4.06535839e+00]\n",
      " [ 8.60000000e+01  0.00000000e+00 -2.25000000e+00  1.80540000e+04\n",
      "  -4.35464911e+00]\n",
      " [ 8.70000000e+01  0.00000000e+00 -5.00000000e-01  7.12600000e+03\n",
      "  -3.63309729e+00]\n",
      " [ 8.80000000e+01  0.00000000e+00 -1.75000000e+00  1.48610000e+04\n",
      "  -3.60332126e+00]\n",
      " [ 8.90000000e+01  0.00000000e+00 -2.50000000e+00  1.31010000e+04\n",
      "  -4.40305147e+00]\n",
      " [ 9.00000000e+01  0.00000000e+00 -1.00000000e+00  6.13400000e+03\n",
      "  -1.34446183e+00]\n",
      " [ 9.10000000e+01  0.00000000e+00 -2.00000000e+00  7.95100000e+03\n",
      "  -5.10637925e+00]\n",
      " [ 9.20000000e+01  0.00000000e+00 -3.50000000e+00  1.68720000e+04\n",
      "  -5.19358743e+00]\n",
      " [ 9.30000000e+01  0.00000000e+00 -1.25000000e+00  8.94900000e+03\n",
      "  -5.28989007e+00]\n",
      " [ 9.40000000e+01  0.00000000e+00 -2.75000000e+00  8.56100000e+03\n",
      "  -5.23179693e+00]\n",
      " [ 9.50000000e+01  0.00000000e+00 -3.75000000e+00  1.22410000e+04\n",
      "  -5.17204925e+00]\n",
      " [ 9.60000000e+01  0.00000000e+00 -2.50000000e+00  1.31310000e+04\n",
      "  -5.12472912e+00]\n",
      " [ 9.70000000e+01  0.00000000e+00 -2.50000000e+00  1.11660000e+04\n",
      "  -5.04675756e+00]\n",
      " [ 9.80000000e+01  0.00000000e+00 -3.00000000e+00  1.24850000e+04\n",
      "  -4.91357847e+00]\n",
      " [ 9.90000000e+01  0.00000000e+00 -2.25000000e+00  1.04040000e+04\n",
      "  -5.06181285e+00]\n",
      " [ 0.00000000e+00  1.00000000e+00 -1.50000000e+00  1.47650000e+04\n",
      "  -5.24157060e+00]\n",
      " [ 1.00000000e+00  1.00000000e+00 -2.50000000e+00  1.11510000e+04\n",
      "  -5.02718760e+00]\n",
      " [ 2.00000000e+00  1.00000000e+00 -2.25000000e+00  5.81000000e+03\n",
      "  -5.14665369e+00]\n",
      " [ 3.00000000e+00  1.00000000e+00 -1.75000000e+00  1.49830000e+04\n",
      "  -5.06796718e+00]\n",
      " [ 4.00000000e+00  1.00000000e+00 -3.25000000e+00  1.27510000e+04\n",
      "  -4.86304653e+00]\n",
      " [ 5.00000000e+00  1.00000000e+00 -7.50000000e-01  6.73300000e+03\n",
      "  -4.74348574e+00]\n",
      " [ 6.00000000e+00  1.00000000e+00 -2.25000000e+00  1.38540000e+04\n",
      "  -4.80261251e+00]\n",
      " [ 7.00000000e+00  1.00000000e+00 -2.25000000e+00  1.31970000e+04\n",
      "  -5.20772912e+00]\n",
      " [ 8.00000000e+00  1.00000000e+00 -3.75000000e+00  6.02600000e+03\n",
      "  -4.79754231e+00]\n",
      " [ 9.00000000e+00  1.00000000e+00 -1.50000000e+00  1.44100000e+04\n",
      "  -2.00904178e+00]\n",
      " [ 1.00000000e+01  1.00000000e+00 -2.00000000e+00  7.74800000e+03\n",
      "  -4.85212182e+00]\n",
      " [ 1.10000000e+01  1.00000000e+00 -2.50000000e+00  9.33400000e+03\n",
      "  -4.57116151e+00]\n",
      " [ 1.20000000e+01  1.00000000e+00 -3.00000000e+00  7.62900000e+03\n",
      "  -4.96256500e+00]\n",
      " [ 1.30000000e+01  1.00000000e+00 -2.00000000e+00  8.07800000e+03\n",
      "  -5.11265654e+00]\n",
      " [ 1.40000000e+01  1.00000000e+00 -3.75000000e+00  1.12820000e+04\n",
      "  -5.04820465e+00]\n",
      " [ 1.50000000e+01  1.00000000e+00 -2.50000000e+00  1.19410000e+04\n",
      "  -5.12894692e+00]\n",
      " [ 1.60000000e+01  1.00000000e+00 -1.00000000e+00  9.63300000e+03\n",
      "  -5.09293157e+00]\n",
      " [ 1.70000000e+01  1.00000000e+00 -2.00000000e+00  1.08530000e+04\n",
      "  -5.14763754e+00]\n",
      " [ 1.80000000e+01  1.00000000e+00  0.00000000e+00  1.24790000e+04\n",
      "  -5.07820728e+00]\n",
      " [ 1.90000000e+01  1.00000000e+00 -2.75000000e+00  1.26690000e+04\n",
      "  -5.20117400e+00]\n",
      " [ 2.00000000e+01  1.00000000e+00 -2.75000000e+00  1.37090000e+04\n",
      "  -5.12164733e+00]\n",
      " [ 2.10000000e+01  1.00000000e+00 -1.00000000e+00  1.49650000e+04\n",
      "  -5.23367252e+00]\n",
      " [ 2.20000000e+01  1.00000000e+00 -2.00000000e+00  1.25390000e+04\n",
      "  -5.06565959e+00]\n",
      " [ 2.30000000e+01  1.00000000e+00 -3.50000000e+00  1.39760000e+04\n",
      "  -5.11666456e+00]\n",
      " [ 2.40000000e+01  1.00000000e+00 -2.00000000e+00  8.10300000e+03\n",
      "  -5.23855659e+00]\n",
      " [ 2.50000000e+01  1.00000000e+00 -3.50000000e+00  1.59120000e+04\n",
      "  -5.12248688e+00]\n",
      " [ 2.60000000e+01  1.00000000e+00 -1.75000000e+00  6.97300000e+03\n",
      "  -4.89579007e+00]\n",
      " [ 2.70000000e+01  1.00000000e+00 -3.75000000e+00  6.72000000e+03\n",
      "  -2.35754893e+00]\n",
      " [ 2.80000000e+01  1.00000000e+00 -1.75000000e+00  1.62980000e+04\n",
      "  -5.00671006e+00]\n",
      " [ 2.90000000e+01  1.00000000e+00 -2.50000000e+00  1.44840000e+04\n",
      "  -5.43545633e+00]\n",
      " [ 3.00000000e+01  1.00000000e+00 -2.75000000e+00  1.12550000e+04\n",
      "  -5.11749993e+00]\n",
      " [ 3.10000000e+01  1.00000000e+00 -2.75000000e+00  1.00240000e+04\n",
      "  -5.22863427e+00]\n",
      " [ 3.20000000e+01  1.00000000e+00 -7.50000000e-01  1.31000000e+04\n",
      "  -5.19526359e+00]\n",
      " [ 3.30000000e+01  1.00000000e+00 -2.25000000e+00  1.80130000e+04\n",
      "  -4.75613210e+00]\n",
      " [ 3.40000000e+01  1.00000000e+00 -1.25000000e+00  1.56350000e+04\n",
      "  -4.35995228e+00]\n",
      " [ 3.50000000e+01  1.00000000e+00 -3.25000000e+00  8.89500000e+03\n",
      "  -4.74856363e+00]\n",
      " [ 3.60000000e+01  1.00000000e+00 -5.00000000e-01  8.73900000e+03\n",
      "  -5.16902784e+00]\n",
      " [ 3.70000000e+01  1.00000000e+00 -3.25000000e+00  1.64390000e+04\n",
      "  -4.98902048e+00]\n",
      " [ 3.80000000e+01  1.00000000e+00 -2.50000000e+00  1.25900000e+04\n",
      "  -4.80107425e+00]\n",
      " [ 3.90000000e+01  1.00000000e+00 -2.25000000e+00  6.46200000e+03\n",
      "  -4.93076412e+00]\n",
      " [ 4.00000000e+01  1.00000000e+00 -2.25000000e+00  1.51900000e+04\n",
      "  -5.08836004e+00]\n",
      " [ 4.10000000e+01  1.00000000e+00 -1.75000000e+00  8.45300000e+03\n",
      "  -5.12001024e+00]\n",
      " [ 4.20000000e+01  1.00000000e+00 -1.75000000e+00  1.66540000e+04\n",
      "  -5.13841110e+00]\n",
      " [ 4.30000000e+01  1.00000000e+00 -2.75000000e+00  8.63200000e+03\n",
      "  -5.18750812e+00]\n",
      " [ 4.40000000e+01  1.00000000e+00 -2.75000000e+00  1.50790000e+04\n",
      "  -5.14800673e+00]\n",
      " [ 4.50000000e+01  1.00000000e+00 -1.50000000e+00  7.15500000e+03\n",
      "  -5.06705864e+00]\n",
      " [ 4.60000000e+01  1.00000000e+00 -2.50000000e+00  1.65400000e+04\n",
      "  -2.64823552e+00]\n",
      " [ 4.70000000e+01  1.00000000e+00 -3.00000000e+00  3.39810000e+04\n",
      "  -5.15616365e+00]\n",
      " [ 4.80000000e+01  1.00000000e+00 -1.50000000e+00  9.82000000e+03\n",
      "  -5.15314855e+00]\n",
      " [ 4.90000000e+01  1.00000000e+00 -2.50000000e+00  7.89500000e+03\n",
      "  -5.10555311e+00]\n",
      " [ 5.00000000e+01  1.00000000e+00 -1.75000000e+00  1.55490000e+04\n",
      "  -5.12152745e+00]\n",
      " [ 5.10000000e+01  1.00000000e+00 -3.25000000e+00  1.03280000e+04\n",
      "  -5.25044024e+00]\n",
      " [ 5.20000000e+01  1.00000000e+00 -2.00000000e+00  1.28760000e+04\n",
      "  -5.23068218e+00]\n",
      " [ 5.30000000e+01  1.00000000e+00 -2.75000000e+00  9.50400000e+03\n",
      "  -5.46887897e+00]\n",
      " [ 5.40000000e+01  1.00000000e+00 -3.50000000e+00  6.83900000e+03\n",
      "  -5.15331353e+00]\n",
      " [ 5.50000000e+01  1.00000000e+00 -3.25000000e+00  1.25050000e+04\n",
      "  -4.91361092e+00]\n",
      " [ 5.60000000e+01  1.00000000e+00 -2.50000000e+00  9.38100000e+03\n",
      "  -5.26855792e+00]\n",
      " [ 5.70000000e+01  1.00000000e+00 -2.25000000e+00  1.07820000e+04\n",
      "  -4.80843867e+00]\n",
      " [ 5.80000000e+01  1.00000000e+00 -1.00000000e+00  1.44080000e+04\n",
      "  -5.05569616e+00]\n",
      " [ 5.90000000e+01  1.00000000e+00 -3.00000000e+00  6.80500000e+03\n",
      "  -5.10370661e+00]\n",
      " [ 6.00000000e+01  1.00000000e+00 -3.00000000e+00  1.60150000e+04\n",
      "  -5.05457446e+00]\n",
      " [ 6.10000000e+01  1.00000000e+00 -2.25000000e+00  1.73350000e+04\n",
      "  -5.02127847e+00]\n",
      " [ 6.20000000e+01  1.00000000e+00 -3.50000000e+00  7.51300000e+03\n",
      "  -5.11186477e+00]\n",
      " [ 6.30000000e+01  1.00000000e+00 -1.25000000e+00  1.41410000e+04\n",
      "  -4.92054805e+00]\n",
      " [ 6.40000000e+01  1.00000000e+00 -3.00000000e+00  7.24600000e+03\n",
      "  -2.62483335e+00]\n",
      " [ 6.50000000e+01  1.00000000e+00 -2.00000000e+00  1.51240000e+04\n",
      "  -5.17769683e+00]\n",
      " [ 6.60000000e+01  1.00000000e+00 -1.50000000e+00  7.68200000e+03\n",
      "  -4.94288870e+00]\n",
      " [ 6.70000000e+01  1.00000000e+00 -1.25000000e+00  7.51000000e+03\n",
      "  -5.02070031e+00]\n",
      " [ 6.80000000e+01  1.00000000e+00 -3.50000000e+00  1.78160000e+04\n",
      "  -5.19358743e+00]\n",
      " [ 6.90000000e+01  1.00000000e+00 -1.75000000e+00  5.65410000e+04\n",
      "  -5.09764112e+00]\n",
      " [ 7.00000000e+01  1.00000000e+00 -3.25000000e+00  1.72240000e+04\n",
      "  -5.41821359e+00]\n",
      " [ 7.10000000e+01  1.00000000e+00 -1.50000000e+00  1.30990000e+04\n",
      "  -5.48397854e+00]\n",
      " [ 7.20000000e+01  1.00000000e+00 -1.75000000e+00  1.31000000e+04\n",
      "  -5.38340037e+00]\n",
      " [ 7.30000000e+01  1.00000000e+00 -2.00000000e+00  6.64700000e+03\n",
      "  -5.24274267e+00]\n",
      " [ 7.40000000e+01  1.00000000e+00 -1.00000000e+00  1.31000000e+04\n",
      "  -5.40677512e+00]\n",
      " [ 7.50000000e+01  1.00000000e+00 -2.75000000e+00  1.18080000e+04\n",
      "  -4.93772193e+00]\n",
      " [ 7.60000000e+01  1.00000000e+00 -1.75000000e+00  7.79900000e+03\n",
      "  -5.11416262e+00]\n",
      " [ 7.70000000e+01  1.00000000e+00 -1.25000000e+00  1.15580000e+04\n",
      "  -4.97812564e+00]\n",
      " [ 7.80000000e+01  1.00000000e+00 -2.75000000e+00  1.33980000e+04\n",
      "  -5.08430966e+00]\n",
      " [ 7.90000000e+01  1.00000000e+00 -1.00000000e+00  1.41110000e+04\n",
      "  -4.75715852e+00]\n",
      " [ 8.00000000e+01  1.00000000e+00 -1.25000000e+00  1.03160000e+04\n",
      "  -4.93645931e+00]\n",
      " [ 8.10000000e+01  1.00000000e+00 -2.00000000e+00  1.28130000e+04\n",
      "  -4.75482978e+00]\n",
      " [ 8.20000000e+01  1.00000000e+00 -3.00000000e+00  1.26880000e+04\n",
      "  -4.72460958e+00]\n",
      " [ 8.30000000e+01  1.00000000e+00 -1.50000000e+00  1.39830000e+04\n",
      "  -2.69047180e+00]\n",
      " [ 8.40000000e+01  1.00000000e+00 -3.00000000e+00  9.33400000e+03\n",
      "  -5.05162664e+00]\n",
      " [ 8.50000000e+01  1.00000000e+00 -1.50000000e+00  1.46530000e+04\n",
      "  -4.98263625e+00]\n",
      " [ 8.60000000e+01  1.00000000e+00 -3.00000000e+00  9.15900000e+03\n",
      "  -5.03859666e+00]\n",
      " [ 8.70000000e+01  1.00000000e+00 -7.50000000e-01  1.00290000e+04\n",
      "  -5.03335134e+00]\n",
      " [ 8.80000000e+01  1.00000000e+00 -1.25000000e+00  1.68220000e+04\n",
      "  -4.93480038e+00]\n",
      " [ 8.90000000e+01  1.00000000e+00 -1.00000000e+00  1.25240000e+04\n",
      "  -5.29553270e+00]\n",
      " [ 9.00000000e+01  1.00000000e+00 -2.50000000e+00  1.71360000e+04\n",
      "  -5.09293157e+00]\n",
      " [ 9.10000000e+01  1.00000000e+00 -3.50000000e+00  5.12370000e+04\n",
      "  -5.05397673e+00]\n",
      " [ 9.20000000e+01  1.00000000e+00 -2.75000000e+00  1.84970000e+04\n",
      "  -5.42392820e+00]\n",
      " [ 9.30000000e+01  1.00000000e+00 -3.25000000e+00  7.50200000e+03\n",
      "  -5.21691755e+00]\n",
      " [ 9.40000000e+01  1.00000000e+00 -2.25000000e+00  9.48700000e+03\n",
      "  -5.27347627e+00]\n",
      " [ 9.50000000e+01  1.00000000e+00 -2.50000000e+00  1.20470000e+04\n",
      "  -4.88025695e+00]\n",
      " [ 9.60000000e+01  1.00000000e+00 -2.00000000e+00  9.15600000e+03\n",
      "  -5.12136764e+00]\n",
      " [ 9.70000000e+01  1.00000000e+00 -2.25000000e+00  1.01840000e+04\n",
      "  -4.92924646e+00]\n",
      " [ 9.80000000e+01  1.00000000e+00 -7.50000000e-01  1.26190000e+04\n",
      "  -5.06770210e+00]\n",
      " [ 9.90000000e+01  1.00000000e+00 -3.00000000e+00  1.12200000e+04\n",
      "  -4.52616885e+00]]\n",
      "Writing to file data.csv\n",
      "**************************************************\n",
      "Data save complete\n",
      "**************************************************\n",
      "closestcar\n",
      "**************************************************\n",
      "Running experiment\n",
      "**************************************************\n",
      "Episode : 0\n",
      "state : [1 2 3 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "action : 2\n",
      "new state: [1 2 3 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "reward: 0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 2 3 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "action : 0\n",
      "new state: [1 2 3 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "reward: 0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 2 3 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "action : 3\n",
      "new state: [1 2 3 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [1 2 3 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "action : 0\n",
      "new state: [0 2 3 3 1 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.5\n",
      "state : [0 2 3 3 1 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0]\n",
      "action : 1\n",
      "new state: [0 2 3 3 0 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.75\n",
      "final state: [0 2 3 3 0 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "Episode : 1\n",
      "state : [0 2 3 3 0 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "action : 2\n",
      "new state: [0 3 2 3 0 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [0 3 2 3 0 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "action : 2\n",
      "new state: [1 3 1 3 0 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.5\n",
      "state : [1 3 1 3 0 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "action : 2\n",
      "new state: [2 3 0 3 0 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.75\n",
      "state : [2 3 0 3 0 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "action : 0\n",
      "new state: [3 3 0 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: 0.0\n",
      "state : [3 3 0 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 1\n",
      "new state: [4 3 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "reward: 0.0\n",
      "epReward so far: 0.0\n",
      "final state: [4 3 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "Episode : 2\n",
      "state : [4 3 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "action : 0\n",
      "new state: [4 3 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [4 3 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "action : 1\n",
      "new state: [4 2 0 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [4 2 0 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "action : 0\n",
      "new state: [3 2 0 3 3 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 1]\n",
      "reward: -0.5\n",
      "epReward so far: -0.25\n",
      "state : [3 2 0 3 3 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 1]\n",
      "action : 0\n",
      "new state: [2 2 0 4 1 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 3]\n",
      "reward: 0.25\n",
      "epReward so far: 0.0\n",
      "state : [2 2 0 4 1 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 3]\n",
      "action : 1\n",
      "new state: [2 2 0 4 1 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "final state: [2 2 0 4 1 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0]\n",
      "Episode : 3\n",
      "state : [2 2 0 4 1 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0]\n",
      "action : 1\n",
      "new state: [2 2 0 4 0 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [2 2 0 4 0 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 2]\n",
      "action : 3\n",
      "new state: [2 2 0 5 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.25\n",
      "state : [2 2 0 5 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "action : 1\n",
      "new state: [3 1 0 5 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "reward: 0.25\n",
      "epReward so far: 0.5\n",
      "state : [3 1 0 5 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "action : 1\n",
      "new state: [3 0 0 5 3 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.75\n",
      "state : [3 0 0 5 3 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "action : 0\n",
      "new state: [3 0 0 6 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "reward: 0.0\n",
      "epReward so far: 0.75\n",
      "final state: [3 0 0 6 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "Episode : 4\n",
      "state : [3 0 0 6 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "action : 0\n",
      "new state: [2 0 0 7 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [2 0 0 7 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "action : 0\n",
      "new state: [1 0 0 7 2 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 3]\n",
      "reward: 0.25\n",
      "epReward so far: -0.25\n",
      "state : [1 0 0 7 2 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 3]\n",
      "action : 3\n",
      "new state: [1 0 1 7 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: 0.0\n",
      "epReward so far: -0.25\n",
      "state : [1 0 1 7 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 0\n",
      "new state: [1 1 1 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "reward: -0.0\n",
      "epReward so far: -0.25\n",
      "state : [1 1 1 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "action : 3\n",
      "new state: [1 1 1 6 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: 0.25\n",
      "epReward so far: 0.0\n",
      "final state: [1 1 1 6 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "Episode : 5\n",
      "state : [1 1 1 6 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 0\n",
      "new state: [0 1 1 6 2 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [0 1 1 6 2 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 2]\n",
      "action : 3\n",
      "new state: [0 1 2 5 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.5\n",
      "state : [0 1 2 5 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2]\n",
      "action : 2\n",
      "new state: [0 1 2 6 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: 0.0\n",
      "epReward so far: 0.5\n",
      "state : [0 1 2 6 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 3\n",
      "new state: [0 1 3 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "reward: 0.0\n",
      "epReward so far: 0.5\n",
      "state : [0 1 3 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "action : 3\n",
      "new state: [0 1 3 5 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.75\n",
      "final state: [0 1 3 5 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "Episode : 6\n",
      "state : [0 1 3 5 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 2\n",
      "new state: [0 1 2 5 0 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [0 1 2 5 0 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "action : 3\n",
      "new state: [1 1 2 4 1 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.5\n",
      "state : [1 1 2 4 1 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 0]\n",
      "action : 3\n",
      "new state: [2 1 2 3 1 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.75\n",
      "state : [2 1 2 3 1 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "action : 1\n",
      "new state: [2 1 2 3 2 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2]\n",
      "reward: 0.25\n",
      "epReward so far: 1.0\n",
      "state : [2 1 2 3 2 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2]\n",
      "action : 2\n",
      "new state: [3 1 2 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "reward: -0.0\n",
      "epReward so far: 1.0\n",
      "final state: [3 1 2 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "Episode : 7\n",
      "state : [3 1 2 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "action : 1\n",
      "new state: [3 1 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 1 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "action : 0\n",
      "new state: [2 1 3 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [2 1 3 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 0\n",
      "new state: [1 1 3 3 3 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.5\n",
      "state : [1 1 3 3 3 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "action : 3\n",
      "new state: [1 1 3 4 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.5\n",
      "state : [1 1 3 4 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "action : 0\n",
      "new state: [1 1 4 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.5\n",
      "final state: [1 1 4 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "Episode : 8\n",
      "state : [1 1 4 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "action : 1\n",
      "new state: [1 1 4 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 1 4 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "action : 1\n",
      "new state: [1 1 4 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 1 4 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "action : 2\n",
      "new state: [1 1 3 4 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [1 1 3 4 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "action : 0\n",
      "new state: [0 1 3 4 0 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.5\n",
      "state : [0 1 3 4 0 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0]\n",
      "action : 1\n",
      "new state: [1 0 3 4 0 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.75\n",
      "final state: [1 0 3 4 0 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "Episode : 9\n",
      "state : [1 0 3 4 0 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "action : 0\n",
      "new state: [1 1 3 4 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [1 1 3 4 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 3\n",
      "new state: [2 1 3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "reward: 0.25\n",
      "epReward so far: -0.5\n",
      "state : [2 1 3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "action : 0\n",
      "new state: [1 1 3 3 2 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2]\n",
      "reward: 0.25\n",
      "epReward so far: -0.25\n",
      "state : [1 1 3 3 2 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2]\n",
      "action : 2\n",
      "new state: [1 1 4 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: -0.25\n",
      "state : [1 1 4 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 0\n",
      "new state: [0 2 4 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.0\n",
      "final state: [0 2 4 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "Episode : 10\n",
      "state : [0 2 4 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "action : 1\n",
      "new state: [0 1 4 3 2 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [0 1 4 3 2 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "action : 1\n",
      "new state: [0 0 5 3 2 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "reward: -0.5\n",
      "epReward so far: -0.25\n",
      "state : [0 0 5 3 2 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "action : 2\n",
      "new state: [0 0 5 3 2 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "reward: -0.5\n",
      "epReward so far: -0.75\n",
      "state : [0 0 5 3 2 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "action : 2\n",
      "new state: [0 0 6 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [0 0 6 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "action : 2\n",
      "new state: [0 0 6 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "reward: -0.5\n",
      "epReward so far: -2.0\n",
      "final state: [0 0 6 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "Episode : 11\n",
      "state : [0 0 6 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "action : 3\n",
      "new state: [0 0 6 2 0 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [0 0 6 2 0 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1]\n",
      "action : 2\n",
      "new state: [1 1 5 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "reward: -0.75\n",
      "epReward so far: -0.5\n",
      "state : [1 1 5 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "action : 0\n",
      "new state: [2 1 5 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "reward: 0.0\n",
      "epReward so far: -0.5\n",
      "state : [2 1 5 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "action : 1\n",
      "new state: [2 1 5 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "reward: -0.0\n",
      "epReward so far: -0.5\n",
      "state : [2 1 5 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "action : 0\n",
      "new state: [1 1 5 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: 0.25\n",
      "epReward so far: -0.25\n",
      "final state: [1 1 5 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "Episode : 12\n",
      "state : [1 1 5 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 3\n",
      "new state: [1 1 5 1 3 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [1 1 5 1 3 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0]\n",
      "action : 1\n",
      "new state: [1 0 5 2 0 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 3]\n",
      "reward: 0.25\n",
      "epReward so far: 0.5\n",
      "state : [1 0 5 2 0 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 3]\n",
      "action : 0\n",
      "new state: [0 1 5 2 0 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "reward: -0.5\n",
      "epReward so far: 0.0\n",
      "state : [0 1 5 2 0 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "action : 3\n",
      "new state: [1 1 5 1 1 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 3]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [1 1 5 1 1 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 3]\n",
      "action : 1\n",
      "new state: [1 0 5 2 1 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.5\n",
      "final state: [1 0 5 2 1 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 1]\n",
      "Episode : 13\n",
      "state : [1 0 5 2 1 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 1]\n",
      "action : 2\n",
      "new state: [1 1 4 2 1 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [1 1 4 2 1 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2]\n",
      "action : 2\n",
      "new state: [1 1 4 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "reward: 0.0\n",
      "epReward so far: 0.25\n",
      "state : [1 1 4 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "action : 1\n",
      "new state: [1 2 4 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.25\n",
      "state : [1 2 4 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "action : 1\n",
      "new state: [1 2 4 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.25\n",
      "state : [1 2 4 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "action : 0\n",
      "new state: [1 2 4 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.25\n",
      "final state: [1 2 4 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "Episode : 14\n",
      "state : [1 2 4 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "action : 0\n",
      "new state: [0 2 4 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [0 2 4 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 2\n",
      "new state: [0 2 3 3 1 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.5\n",
      "state : [0 2 3 3 1 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "action : 2\n",
      "new state: [0 3 2 3 0 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.75\n",
      "state : [0 3 2 3 0 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "action : 1\n",
      "new state: [0 3 2 4 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: -0.75\n",
      "epReward so far: 0.0\n",
      "state : [0 3 2 4 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 2\n",
      "new state: [1 3 1 4 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "final state: [1 3 1 4 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "Episode : 15\n",
      "state : [1 3 1 4 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 0\n",
      "new state: [0 3 1 4 3 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [0 3 1 4 3 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "action : 2\n",
      "new state: [0 3 0 5 0 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 3]\n",
      "reward: 0.25\n",
      "epReward so far: 0.5\n",
      "state : [0 3 0 5 0 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 3]\n",
      "action : 1\n",
      "new state: [0 3 0 6 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: -0.75\n",
      "epReward so far: -0.25\n",
      "state : [0 3 0 6 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 3\n",
      "new state: [1 3 0 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "reward: 0.0\n",
      "epReward so far: -0.25\n",
      "state : [1 3 0 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "action : 0\n",
      "new state: [0 3 0 6 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: -0.5\n",
      "epReward so far: -0.75\n",
      "final state: [0 3 0 6 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "Episode : 16\n",
      "state : [0 3 0 6 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 3\n",
      "new state: [0 3 0 5 3 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [0 3 0 5 3 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "action : 1\n",
      "new state: [0 2 0 6 0 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 3]\n",
      "reward: -0.5\n",
      "epReward so far: -0.25\n",
      "state : [0 2 0 6 0 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 3]\n",
      "action : 3\n",
      "new state: [0 2 1 6 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "reward: 0.0\n",
      "epReward so far: -0.25\n",
      "state : [0 2 1 6 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "action : 1\n",
      "new state: [1 1 1 6 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.0\n",
      "state : [1 1 1 6 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 3\n",
      "new state: [1 1 1 5 2 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 3]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "final state: [1 1 1 5 2 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 3]\n",
      "Episode : 17\n",
      "state : [1 1 1 5 2 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 3]\n",
      "action : 1\n",
      "new state: [1 0 2 5 3 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [1 0 2 5 3 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2]\n",
      "action : 2\n",
      "new state: [1 1 2 5 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "reward: 0.0\n",
      "epReward so far: 0.25\n",
      "state : [1 1 2 5 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "action : 2\n",
      "new state: [1 1 1 6 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.5\n",
      "state : [1 1 1 6 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "action : 0\n",
      "new state: [1 1 1 6 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: 0.0\n",
      "epReward so far: 0.5\n",
      "state : [1 1 1 6 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 3\n",
      "new state: [1 2 1 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "reward: 0.0\n",
      "epReward so far: 0.5\n",
      "final state: [1 2 1 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "Episode : 18\n",
      "state : [1 2 1 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "action : 2\n",
      "new state: [1 2 1 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "reward: 0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 2 1 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "action : 1\n",
      "new state: [1 2 1 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 2 1 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "action : 0\n",
      "new state: [1 2 1 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "reward: 0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 2 1 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "action : 0\n",
      "new state: [0 2 1 6 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [0 2 1 6 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 2\n",
      "new state: [0 2 0 6 1 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.5\n",
      "final state: [0 2 0 6 1 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0]\n",
      "Episode : 19\n",
      "state : [0 2 0 6 1 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0]\n",
      "action : 1\n",
      "new state: [0 2 0 6 0 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [0 2 0 6 0 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 2]\n",
      "action : 3\n",
      "new state: [0 2 0 7 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.25\n",
      "state : [0 2 0 7 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 3\n",
      "new state: [1 2 0 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.25\n",
      "state : [1 2 0 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "action : 0\n",
      "new state: [1 2 0 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "reward: -0.75\n",
      "epReward so far: -0.5\n",
      "state : [1 2 0 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "action : 0\n",
      "new state: [1 2 0 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "reward: 0.0\n",
      "epReward so far: -0.5\n",
      "final state: [1 2 0 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "Episode : 20\n",
      "state : [1 2 0 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "action : 3\n",
      "new state: [1 2 0 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 2 0 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "action : 1\n",
      "new state: [1 2 0 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 2 0 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "action : 0\n",
      "new state: [0 2 0 7 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [0 2 0 7 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 1\n",
      "new state: [0 1 1 7 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.5\n",
      "state : [0 1 1 7 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "action : 1\n",
      "new state: [0 2 1 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "reward: -0.0\n",
      "epReward so far: -0.5\n",
      "final state: [0 2 1 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Episode : 21\n",
      "state : [0 2 1 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "action : 1\n",
      "new state: [0 2 1 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [0 2 1 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "action : 2\n",
      "new state: [0 2 0 7 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: 0.25\n",
      "epReward so far: -0.5\n",
      "state : [0 2 0 7 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 3\n",
      "new state: [0 2 0 6 0 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2]\n",
      "reward: 0.25\n",
      "epReward so far: -0.25\n",
      "state : [0 2 0 6 0 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2]\n",
      "action : 1\n",
      "new state: [1 2 0 6 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.0\n",
      "state : [1 2 0 6 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 1\n",
      "new state: [1 3 0 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "reward: -0.0\n",
      "epReward so far: -1.0\n",
      "final state: [1 3 0 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "Episode : 22\n",
      "state : [1 3 0 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "action : 3\n",
      "new state: [1 3 0 5 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [1 3 0 5 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "action : 1\n",
      "new state: [1 2 0 5 2 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.5\n",
      "state : [1 2 0 5 2 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "action : 3\n",
      "new state: [1 2 1 4 1 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.75\n",
      "state : [1 2 1 4 1 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "action : 2\n",
      "new state: [2 2 0 4 1 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 3]\n",
      "reward: 0.25\n",
      "epReward so far: 1.0\n",
      "state : [2 2 0 4 1 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 3]\n",
      "action : 0\n",
      "new state: [1 3 0 4 3 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "reward: -0.5\n",
      "epReward so far: 0.5\n",
      "final state: [1 3 0 4 3 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "Episode : 23\n",
      "state : [1 3 0 4 3 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "action : 0\n",
      "new state: [2 3 0 4 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 3 0 4 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 0\n",
      "new state: [2 3 0 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "reward: -0.0\n",
      "epReward so far: -0.75\n",
      "state : [2 3 0 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "action : 3\n",
      "new state: [2 3 0 4 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "reward: 0.25\n",
      "epReward so far: -0.5\n",
      "state : [2 3 0 4 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "action : 1\n",
      "new state: [2 2 0 4 0 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2]\n",
      "reward: 0.25\n",
      "epReward so far: -0.25\n",
      "state : [2 2 0 4 0 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2]\n",
      "action : 0\n",
      "new state: [3 2 0 4 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.0\n",
      "final state: [3 2 0 4 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "Episode : 24\n",
      "state : [3 2 0 4 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 0\n",
      "new state: [4 2 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [4 2 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "action : 1\n",
      "new state: [4 2 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "reward: 0.0\n",
      "epReward so far: -0.75\n",
      "state : [4 2 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "action : 0\n",
      "new state: [3 2 0 4 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: -0.5\n",
      "epReward so far: -1.25\n",
      "state : [3 2 0 4 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 0\n",
      "new state: [3 2 0 4 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "state : [3 2 0 4 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "action : 0\n",
      "new state: [3 3 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "reward: 0.0\n",
      "epReward so far: -2.0\n",
      "final state: [3 3 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "Episode : 25\n",
      "state : [3 3 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "action : 3\n",
      "new state: [3 3 0 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [3 3 0 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 3\n",
      "new state: [3 3 0 2 2 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.5\n",
      "state : [3 3 0 2 2 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1]\n",
      "action : 1\n",
      "new state: [3 3 1 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: 0.0\n",
      "epReward so far: 0.5\n",
      "state : [3 3 1 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 2\n",
      "new state: [3 4 0 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.75\n",
      "state : [3 4 0 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 0\n",
      "new state: [2 4 0 2 0 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1]\n",
      "reward: 0.25\n",
      "epReward so far: 1.0\n",
      "final state: [2 4 0 2 0 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1]\n",
      "Episode : 26\n",
      "state : [2 4 0 2 0 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1]\n",
      "action : 1\n",
      "new state: [3 4 0 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 4 0 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 0\n",
      "new state: [3 4 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 4 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "action : 3\n",
      "new state: [3 4 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 4 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "action : 1\n",
      "new state: [3 3 1 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [3 3 1 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "action : 1\n",
      "new state: [3 2 1 2 3 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 3]\n",
      "reward: 0.25\n",
      "epReward so far: 0.5\n",
      "final state: [3 2 1 2 3 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 3]\n",
      "Episode : 27\n",
      "state : [3 2 1 2 3 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 3]\n",
      "action : 2\n",
      "new state: [3 2 0 3 3 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [3 2 0 3 3 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "action : 0\n",
      "new state: [2 2 1 3 3 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 0]\n",
      "reward: -0.5\n",
      "epReward so far: -0.25\n",
      "state : [2 2 1 3 3 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 0]\n",
      "action : 3\n",
      "new state: [2 2 1 4 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: -0.0\n",
      "epReward so far: -0.25\n",
      "state : [2 2 1 4 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 1\n",
      "new state: [3 2 1 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "reward: 0.0\n",
      "epReward so far: -0.25\n",
      "state : [3 2 1 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "action : 3\n",
      "new state: [3 2 1 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.0\n",
      "final state: [3 2 1 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "Episode : 28\n",
      "state : [3 2 1 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 2\n",
      "new state: [3 2 1 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 2 1 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 3\n",
      "new state: [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "reward: 0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "action : 2\n",
      "new state: [3 2 1 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [3 2 1 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 2\n",
      "new state: [3 2 0 3 3 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.5\n",
      "state : [3 2 0 3 3 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "action : 0\n",
      "new state: [2 2 0 4 0 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 1]\n",
      "reward: -0.5\n",
      "epReward so far: 0.0\n",
      "final state: [2 2 0 4 0 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 1]\n",
      "Episode : 29\n",
      "state : [2 2 0 4 0 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 1]\n",
      "action : 0\n",
      "new state: [2 2 0 4 0 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [2 2 0 4 0 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2]\n",
      "action : 0\n",
      "new state: [2 2 1 4 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: -0.75\n",
      "epReward so far: -0.5\n",
      "state : [2 2 1 4 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 2\n",
      "new state: [2 3 0 4 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: 0.25\n",
      "epReward so far: -0.25\n",
      "state : [2 3 0 4 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 3\n",
      "new state: [2 3 0 3 3 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.0\n",
      "state : [2 3 0 3 3 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 2]\n",
      "action : 3\n",
      "new state: [2 3 0 4 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "final state: [2 3 0 4 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "Episode : 30\n",
      "state : [2 3 0 4 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 0\n",
      "new state: [1 4 0 4 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [1 4 0 4 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 0\n",
      "new state: [0 4 0 4 0 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 3]\n",
      "reward: -0.5\n",
      "epReward so far: -1.0\n",
      "state : [0 4 0 4 0 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 3]\n",
      "action : 1\n",
      "new state: [1 4 0 4 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.75\n",
      "state : [1 4 0 4 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "action : 0\n",
      "new state: [1 4 0 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "reward: -0.75\n",
      "epReward so far: -2.5\n",
      "state : [1 4 0 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "action : 1\n",
      "new state: [1 3 0 5 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: 0.25\n",
      "epReward so far: -2.25\n",
      "final state: [1 3 0 5 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "Episode : 31\n",
      "state : [1 3 0 5 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 1\n",
      "new state: [1 3 0 5 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 3 0 5 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "action : 0\n",
      "new state: [0 3 0 6 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [0 3 0 6 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 3\n",
      "new state: [0 3 0 5 1 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.5\n",
      "state : [0 3 0 5 1 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "action : 1\n",
      "new state: [0 3 0 5 0 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 0]\n",
      "reward: -0.5\n",
      "epReward so far: 0.0\n",
      "state : [0 3 0 5 0 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 0]\n",
      "action : 3\n",
      "new state: [0 3 1 4 0 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "final state: [0 3 1 4 0 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "Episode : 32\n",
      "state : [0 3 1 4 0 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "action : 1\n",
      "new state: [1 3 1 4 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [1 3 1 4 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 3\n",
      "new state: [2 3 1 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: 0.25\n",
      "epReward so far: -0.5\n",
      "state : [2 3 1 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 2\n",
      "new state: [2 3 0 3 2 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "reward: 0.25\n",
      "epReward so far: -0.25\n",
      "state : [2 3 0 3 2 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "action : 0\n",
      "new state: [2 3 1 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: 0.0\n",
      "epReward so far: -0.25\n",
      "state : [2 3 1 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 0\n",
      "new state: [2 3 1 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.0\n",
      "final state: [2 3 1 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "Episode : 33\n",
      "state : [2 3 1 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 2\n",
      "new state: [2 3 1 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: 0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 3 1 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 0\n",
      "new state: [1 3 2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [1 3 2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "action : 3\n",
      "new state: [1 3 2 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.25\n",
      "state : [1 3 2 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "action : 3\n",
      "new state: [1 3 2 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.5\n",
      "state : [1 3 2 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 3\n",
      "new state: [1 3 2 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.5\n",
      "final state: [1 3 2 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "Episode : 34\n",
      "state : [1 3 2 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "action : 1\n",
      "new state: [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "action : 3\n",
      "new state: [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "action : 2\n",
      "new state: [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "reward: 0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "action : 2\n",
      "new state: [2 3 1 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [2 3 1 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "action : 2\n",
      "new state: [2 3 1 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.25\n",
      "final state: [2 3 1 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "Episode : 35\n",
      "state : [2 3 1 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 3\n",
      "new state: [3 3 1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 3 1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "action : 1\n",
      "new state: [3 3 1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "reward: 0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 3 1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "action : 1\n",
      "new state: [3 3 1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "reward: 0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 3 1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "action : 3\n",
      "new state: [3 3 1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 3 1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "action : 3\n",
      "new state: [3 3 1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "final state: [3 3 1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "Episode : 36\n",
      "state : [3 3 1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "action : 3\n",
      "new state: [3 3 1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "reward: 0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 3 1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "action : 3\n",
      "new state: [3 3 1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 3 1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "action : 1\n",
      "new state: [3 3 1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 3 1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "action : 2\n",
      "new state: [3 3 1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 3 1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "action : 0\n",
      "new state: [3 3 1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "reward: 0.0\n",
      "epReward so far: 0.0\n",
      "final state: [3 3 1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "Episode : 37\n",
      "state : [3 3 1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "action : 2\n",
      "new state: [3 3 0 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [3 3 0 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 0\n",
      "new state: [3 3 0 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "reward: -0.75\n",
      "epReward so far: -0.5\n",
      "state : [3 3 0 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "action : 3\n",
      "new state: [3 3 0 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: 0.25\n",
      "epReward so far: -0.25\n",
      "state : [3 3 0 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 3\n",
      "new state: [3 3 0 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: -0.25\n",
      "state : [3 3 0 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 0\n",
      "new state: [3 3 0 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: 0.25\n",
      "epReward so far: 0.0\n",
      "final state: [3 3 0 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "Episode : 38\n",
      "state : [3 3 0 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 3\n",
      "new state: [3 3 0 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "reward: 0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 3 0 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "action : 3\n",
      "new state: [3 3 1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 3 1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "action : 0\n",
      "new state: [2 3 1 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [2 3 1 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 0\n",
      "new state: [2 3 1 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.25\n",
      "state : [2 3 1 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 3\n",
      "new state: [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "reward: 0.0\n",
      "epReward so far: 0.25\n",
      "final state: [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Episode : 39\n",
      "state : [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "action : 0\n",
      "new state: [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "reward: 0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "action : 2\n",
      "new state: [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "reward: 0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "action : 2\n",
      "new state: [2 3 1 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [2 3 1 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "action : 1\n",
      "new state: [2 2 1 3 0 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.5\n",
      "state : [2 2 1 3 0 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1]\n",
      "action : 1\n",
      "new state: [3 2 1 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: 0.0\n",
      "epReward so far: 0.5\n",
      "final state: [3 2 1 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "Episode : 40\n",
      "state : [3 2 1 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 2\n",
      "new state: [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "action : 0\n",
      "new state: [2 2 2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [2 2 2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 3\n",
      "new state: [2 2 2 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: 0.0\n",
      "epReward so far: 0.25\n",
      "state : [2 2 2 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 0\n",
      "new state: [1 2 2 4 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.5\n",
      "state : [1 2 2 4 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 1\n",
      "new state: [1 2 2 4 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: 0.0\n",
      "epReward so far: 0.5\n",
      "final state: [1 2 2 4 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "Episode : 41\n",
      "state : [1 2 2 4 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 3\n",
      "new state: [1 2 3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [1 2 3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 3\n",
      "new state: [1 2 3 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.25\n",
      "state : [1 2 3 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "action : 1\n",
      "new state: [1 2 4 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.25\n",
      "state : [1 2 4 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "action : 0\n",
      "new state: [1 2 4 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.25\n",
      "state : [1 2 4 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "action : 0\n",
      "new state: [1 2 4 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.25\n",
      "final state: [1 2 4 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "Episode : 42\n",
      "state : [1 2 4 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "action : 1\n",
      "new state: [1 1 4 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [1 1 4 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 0\n",
      "new state: [0 1 4 3 2 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.5\n",
      "state : [0 1 4 3 2 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 1]\n",
      "action : 1\n",
      "new state: [0 1 5 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.25\n",
      "state : [0 1 5 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "action : 1\n",
      "new state: [0 0 6 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.0\n",
      "state : [0 0 6 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "action : 2\n",
      "new state: [0 0 6 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "final state: [0 0 6 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "Episode : 43\n",
      "state : [0 0 6 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 2\n",
      "new state: [0 0 7 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "reward: 0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 0 7 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "action : 3\n",
      "new state: [0 0 7 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [0 0 7 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "action : 2\n",
      "new state: [0 0 6 2 1 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 3]\n",
      "reward: -0.5\n",
      "epReward so far: -0.25\n",
      "state : [0 0 6 2 1 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 3]\n",
      "action : 2\n",
      "new state: [0 1 5 2 3 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "reward: -0.5\n",
      "epReward so far: -0.75\n",
      "state : [0 1 5 2 3 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "action : 1\n",
      "new state: [0 2 5 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "reward: -0.0\n",
      "epReward so far: -0.75\n",
      "final state: [0 2 5 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "Episode : 44\n",
      "state : [0 2 5 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "action : 1\n",
      "new state: [0 1 5 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [0 1 5 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 3\n",
      "new state: [0 1 5 2 1 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "reward: 0.25\n",
      "epReward so far: -0.25\n",
      "state : [0 1 5 2 1 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "action : 2\n",
      "new state: [0 2 5 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "reward: -0.0\n",
      "epReward so far: -0.25\n",
      "state : [0 2 5 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "action : 1\n",
      "new state: [0 2 6 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "reward: -0.0\n",
      "epReward so far: -0.25\n",
      "state : [0 2 6 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "action : 3\n",
      "new state: [0 2 6 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.0\n",
      "final state: [0 2 6 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "Episode : 45\n",
      "state : [0 2 6 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 1\n",
      "new state: [0 2 6 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "reward: 0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 2 6 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "action : 2\n",
      "new state: [0 3 6 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 3 6 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "action : 1\n",
      "new state: [0 2 6 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [0 2 6 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 3\n",
      "new state: [0 2 6 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: -0.0\n",
      "epReward so far: -0.5\n",
      "state : [0 2 6 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 2\n",
      "new state: [0 3 5 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: 0.25\n",
      "epReward so far: -0.25\n",
      "final state: [0 3 5 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "Episode : 46\n",
      "state : [0 3 5 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 3\n",
      "new state: [0 3 5 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "reward: 0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 3 5 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "action : 3\n",
      "new state: [1 3 5 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [1 3 5 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 1\n",
      "new state: [1 3 5 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.25\n",
      "state : [1 3 5 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 2\n",
      "new state: [2 3 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.25\n",
      "state : [2 3 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "action : 0\n",
      "new state: [2 3 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "reward: 0.0\n",
      "epReward so far: 0.25\n",
      "final state: [2 3 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "Episode : 47\n",
      "state : [2 3 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "action : 0\n",
      "new state: [2 3 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 3 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "action : 1\n",
      "new state: [2 2 5 0 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [2 2 5 0 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 2\n",
      "new state: [2 2 5 0 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.25\n",
      "state : [2 2 5 0 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "action : 0\n",
      "new state: [1 2 5 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: 0.25\n",
      "epReward so far: 0.5\n",
      "state : [1 2 5 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 2\n",
      "new state: [1 2 4 1 1 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.75\n",
      "final state: [1 2 4 1 1 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0]\n",
      "Episode : 48\n",
      "state : [1 2 4 1 1 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0]\n",
      "action : 1\n",
      "new state: [1 2 4 1 0 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [1 2 4 1 0 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2]\n",
      "action : 2\n",
      "new state: [1 2 4 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: 0.0\n",
      "epReward so far: 0.25\n",
      "state : [1 2 4 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 3\n",
      "new state: [2 2 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "reward: 0.0\n",
      "epReward so far: 0.25\n",
      "state : [2 2 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "action : 3\n",
      "new state: [2 2 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "reward: 0.0\n",
      "epReward so far: 0.25\n",
      "state : [2 2 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "action : 1\n",
      "new state: [2 1 4 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.5\n",
      "final state: [2 1 4 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "Episode : 49\n",
      "state : [2 1 4 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "action : 3\n",
      "new state: [2 1 4 1 2 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [2 1 4 1 2 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "action : 0\n",
      "new state: [1 1 5 1 2 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.5\n",
      "state : [1 1 5 1 2 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0]\n",
      "action : 1\n",
      "new state: [2 0 5 1 2 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 3]\n",
      "reward: 0.25\n",
      "epReward so far: 0.75\n",
      "state : [2 0 5 1 2 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 3]\n",
      "action : 2\n",
      "new state: [2 0 6 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.75\n",
      "state : [2 0 6 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "action : 0\n",
      "new state: [3 0 6 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.75\n",
      "final state: [3 0 6 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "Episode : 50\n",
      "state : [3 0 6 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "action : 2\n",
      "new state: [3 0 6 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "reward: 0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 0 6 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "action : 0\n",
      "new state: [2 0 6 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [2 0 6 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "action : 2\n",
      "new state: [2 0 5 1 1 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.5\n",
      "state : [2 0 5 1 1 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "action : 0\n",
      "new state: [1 1 5 1 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 1]\n",
      "reward: -0.5\n",
      "epReward so far: 0.0\n",
      "state : [1 1 5 1 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 1]\n",
      "action : 2\n",
      "new state: [1 2 4 1 2 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 3]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "final state: [1 2 4 1 2 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 3]\n",
      "Episode : 51\n",
      "state : [1 2 4 1 2 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 3]\n",
      "action : 1\n",
      "new state: [1 1 5 1 3 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [1 1 5 1 3 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "action : 1\n",
      "new state: [1 1 5 1 3 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 3]\n",
      "reward: 0.25\n",
      "epReward so far: 0.5\n",
      "state : [1 1 5 1 3 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 3]\n",
      "action : 2\n",
      "new state: [1 1 4 2 3 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.75\n",
      "state : [1 1 4 2 3 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "action : 2\n",
      "new state: [1 1 4 2 3 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "reward: 0.25\n",
      "epReward so far: 1.0\n",
      "state : [1 1 4 2 3 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "action : 0\n",
      "new state: [0 1 4 3 2 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "reward: 0.25\n",
      "epReward so far: 1.25\n",
      "final state: [0 1 4 3 2 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "Episode : 52\n",
      "state : [0 1 4 3 2 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "action : 1\n",
      "new state: [2 0 4 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 0 4 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 0\n",
      "new state: [2 0 5 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "reward: -0.0\n",
      "epReward so far: -0.75\n",
      "state : [2 0 5 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "action : 0\n",
      "new state: [1 1 5 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [1 1 5 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "action : 2\n",
      "new state: [1 1 4 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: 0.25\n",
      "epReward so far: -1.25\n",
      "state : [1 1 4 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 3\n",
      "new state: [1 1 4 2 1 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "reward: 0.25\n",
      "epReward so far: -1.0\n",
      "final state: [1 1 4 2 1 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "Episode : 53\n",
      "state : [1 1 4 2 1 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "action : 0\n",
      "new state: [1 2 4 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 2 4 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 3\n",
      "new state: [1 2 5 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 2 5 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "action : 2\n",
      "new state: [1 2 5 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 2 5 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "action : 1\n",
      "new state: [1 2 5 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "reward: 0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 2 5 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "action : 2\n",
      "new state: [1 2 4 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "final state: [1 2 4 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "Episode : 54\n",
      "state : [1 2 4 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "action : 2\n",
      "new state: [1 2 3 2 1 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [1 2 3 2 1 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 0]\n",
      "action : 3\n",
      "new state: [1 3 3 1 0 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.5\n",
      "state : [1 3 3 1 0 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1]\n",
      "action : 1\n",
      "new state: [1 4 3 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "reward: 0.0\n",
      "epReward so far: 0.5\n",
      "state : [1 4 3 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "action : 1\n",
      "new state: [2 4 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.5\n",
      "state : [2 4 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "action : 0\n",
      "new state: [2 4 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.5\n",
      "final state: [2 4 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "Episode : 55\n",
      "state : [2 4 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "action : 0\n",
      "new state: [1 4 3 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [1 4 3 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 0\n",
      "new state: [1 4 3 1 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.25\n",
      "state : [1 4 3 1 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "action : 2\n",
      "new state: [1 4 3 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.5\n",
      "state : [1 4 3 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "action : 0\n",
      "new state: [0 4 3 1 1 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 3]\n",
      "reward: 0.25\n",
      "epReward so far: 0.75\n",
      "state : [0 4 3 1 1 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 3]\n",
      "action : 1\n",
      "new state: [0 4 3 1 3 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1]\n",
      "reward: -0.5\n",
      "epReward so far: 0.25\n",
      "final state: [0 4 3 1 3 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1]\n",
      "Episode : 56\n",
      "state : [0 4 3 1 3 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1]\n",
      "action : 1\n",
      "new state: [0 5 3 1 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: 0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 5 3 1 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 2\n",
      "new state: [0 5 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 5 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "action : 2\n",
      "new state: [0 5 2 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [0 5 2 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 1\n",
      "new state: [0 5 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: -0.75\n",
      "epReward so far: -0.5\n",
      "state : [0 5 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 2\n",
      "new state: [0 6 1 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: 0.25\n",
      "epReward so far: -0.25\n",
      "final state: [0 6 1 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "Episode : 57\n",
      "state : [0 6 1 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 2\n",
      "new state: [0 6 0 2 3 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 3]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [0 6 0 2 3 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 3]\n",
      "action : 3\n",
      "new state: [0 6 0 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.25\n",
      "state : [0 6 0 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 1\n",
      "new state: [1 5 0 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: -0.5\n",
      "epReward so far: -0.25\n",
      "state : [1 5 0 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 0\n",
      "new state: [0 5 0 3 2 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 3]\n",
      "reward: 0.25\n",
      "epReward so far: 0.0\n",
      "state : [0 5 0 3 2 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 3]\n",
      "action : 1\n",
      "new state: [0 4 1 3 3 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "final state: [0 4 1 3 3 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1]\n",
      "Episode : 58\n",
      "state : [0 4 1 3 3 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1]\n",
      "action : 1\n",
      "new state: [0 4 2 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 4 2 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 2\n",
      "new state: [0 4 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 4 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "action : 2\n",
      "new state: [0 4 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 4 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "action : 1\n",
      "new state: [0 4 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 4 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "action : 3\n",
      "new state: [0 4 2 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "final state: [0 4 2 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "Episode : 59\n",
      "state : [0 4 2 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 3\n",
      "new state: [0 4 2 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: 0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 4 2 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 3\n",
      "new state: [0 5 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 5 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "action : 1\n",
      "new state: [0 5 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [0 5 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "action : 1\n",
      "new state: [0 5 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [0 5 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "action : 2\n",
      "new state: [0 5 1 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: 0.25\n",
      "epReward so far: -1.25\n",
      "final state: [0 5 1 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "Episode : 60\n",
      "state : [0 5 1 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 2\n",
      "new state: [0 5 0 3 1 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [0 5 0 3 1 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "action : 3\n",
      "new state: [0 6 0 2 1 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 3]\n",
      "reward: 0.25\n",
      "epReward so far: 0.5\n",
      "state : [0 6 0 2 1 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 3]\n",
      "action : 1\n",
      "new state: [1 5 0 2 1 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 3]\n",
      "reward: 0.25\n",
      "epReward so far: 0.75\n",
      "state : [1 5 0 2 1 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 3]\n",
      "action : 0\n",
      "new state: [0 6 0 2 3 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2]\n",
      "reward: -0.5\n",
      "epReward so far: 0.25\n",
      "state : [0 6 0 2 3 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2]\n",
      "action : 1\n",
      "new state: [0 6 0 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.5\n",
      "final state: [0 6 0 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "Episode : 61\n",
      "state : [0 6 0 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 1\n",
      "new state: [0 6 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [0 6 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "action : 1\n",
      "new state: [0 6 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "reward: 0.0\n",
      "epReward so far: -0.75\n",
      "state : [0 6 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "action : 1\n",
      "new state: [0 6 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [0 6 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "action : 1\n",
      "new state: [0 5 0 4 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "reward: 0.25\n",
      "epReward so far: -1.25\n",
      "state : [0 5 0 4 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "action : 1\n",
      "new state: [0 5 0 4 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "final state: [0 5 0 4 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "Episode : 62\n",
      "state : [0 5 0 4 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "action : 1\n",
      "new state: [1 5 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [1 5 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "action : 0\n",
      "new state: [1 5 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [1 5 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "action : 0\n",
      "new state: [1 5 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "reward: -0.0\n",
      "epReward so far: -1.5\n",
      "state : [1 5 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "action : 3\n",
      "new state: [1 5 0 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: 0.25\n",
      "epReward so far: -1.25\n",
      "state : [1 5 0 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 0\n",
      "new state: [0 5 0 3 1 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 3]\n",
      "reward: 0.25\n",
      "epReward so far: -1.0\n",
      "final state: [0 5 0 3 1 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 3]\n",
      "Episode : 63\n",
      "state : [0 5 0 3 1 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 3]\n",
      "action : 1\n",
      "new state: [0 5 0 3 3 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [0 5 0 3 3 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2]\n",
      "action : 1\n",
      "new state: [0 4 2 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: -0.75\n",
      "epReward so far: -0.5\n",
      "state : [0 4 2 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 3\n",
      "new state: [0 4 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "reward: -0.0\n",
      "epReward so far: -0.5\n",
      "state : [0 4 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "action : 3\n",
      "new state: [0 4 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "reward: -0.0\n",
      "epReward so far: -0.5\n",
      "state : [0 4 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "action : 3\n",
      "new state: [0 4 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "reward: 0.0\n",
      "epReward so far: -0.5\n",
      "final state: [0 4 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "Episode : 64\n",
      "state : [0 4 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "action : 1\n",
      "new state: [0 4 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "reward: 0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 4 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "action : 1\n",
      "new state: [0 4 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [0 4 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "action : 1\n",
      "new state: [0 4 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [0 4 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "action : 1\n",
      "new state: [0 3 2 4 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: 0.25\n",
      "epReward so far: -1.25\n",
      "state : [0 3 2 4 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 3\n",
      "new state: [0 3 2 3 3 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "reward: 0.25\n",
      "epReward so far: -1.0\n",
      "final state: [0 3 2 3 3 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "Episode : 65\n",
      "state : [0 3 2 3 3 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "action : 2\n",
      "new state: [0 3 1 4 0 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [0 3 1 4 0 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "action : 2\n",
      "new state: [0 3 2 4 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.25\n",
      "state : [0 3 2 4 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 3\n",
      "new state: [1 3 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.25\n",
      "state : [1 3 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "action : 3\n",
      "new state: [1 3 2 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.5\n",
      "state : [1 3 2 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "action : 2\n",
      "new state: [1 3 2 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.5\n",
      "final state: [1 3 2 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "Episode : 66\n",
      "state : [1 3 2 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 3\n",
      "new state: [1 4 2 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [1 4 2 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 3\n",
      "new state: [1 4 2 1 1 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.5\n",
      "state : [1 4 2 1 1 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1]\n",
      "action : 1\n",
      "new state: [1 5 2 1 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "reward: 0.0\n",
      "epReward so far: 0.5\n",
      "state : [1 5 2 1 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "action : 1\n",
      "new state: [1 4 3 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: 0.25\n",
      "epReward so far: 0.75\n",
      "state : [1 4 3 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 0\n",
      "new state: [1 4 3 1 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.75\n",
      "final state: [1 4 3 1 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "Episode : 67\n",
      "state : [1 4 3 1 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "action : 0\n",
      "new state: [1 4 4 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 4 4 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "action : 2\n",
      "new state: [1 4 3 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [1 4 3 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 2\n",
      "new state: [1 4 2 1 3 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.5\n",
      "state : [1 4 2 1 3 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "action : 1\n",
      "new state: [1 3 2 2 2 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.75\n",
      "state : [1 3 2 2 2 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1]\n",
      "action : 1\n",
      "new state: [2 3 2 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.75\n",
      "final state: [2 3 2 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "Episode : 68\n",
      "state : [2 3 2 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 0\n",
      "new state: [1 3 3 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [1 3 3 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "action : 0\n",
      "new state: [0 3 3 2 3 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 3]\n",
      "reward: 0.25\n",
      "epReward so far: 0.5\n",
      "state : [0 3 3 2 3 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 3]\n",
      "action : 1\n",
      "new state: [0 3 3 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.25\n",
      "state : [0 3 3 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 3\n",
      "new state: [0 4 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: -0.25\n",
      "state : [0 4 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "action : 2\n",
      "new state: [0 4 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "reward: 0.0\n",
      "epReward so far: -0.25\n",
      "final state: [0 4 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "Episode : 69\n",
      "state : [0 4 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "action : 1\n",
      "new state: [0 4 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [0 4 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "action : 2\n",
      "new state: [0 4 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "reward: -0.0\n",
      "epReward so far: -0.75\n",
      "state : [0 4 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "action : 1\n",
      "new state: [0 4 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [0 4 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "action : 2\n",
      "new state: [0 4 2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "reward: 0.25\n",
      "epReward so far: -1.25\n",
      "state : [0 4 2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "action : 1\n",
      "new state: [0 3 2 3 3 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 2]\n",
      "reward: 0.25\n",
      "epReward so far: -1.0\n",
      "final state: [0 3 2 3 3 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 2]\n",
      "Episode : 70\n",
      "state : [0 3 2 3 3 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 2]\n",
      "action : 3\n",
      "new state: [0 3 2 3 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 3]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [0 3 2 3 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 3]\n",
      "action : 2\n",
      "new state: [0 3 1 4 2 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.5\n",
      "state : [0 3 1 4 2 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "action : 1\n",
      "new state: [0 2 2 4 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.75\n",
      "state : [0 2 2 4 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "action : 1\n",
      "new state: [0 1 2 5 2 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 1]\n",
      "reward: -0.5\n",
      "epReward so far: 0.25\n",
      "state : [0 1 2 5 2 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 1]\n",
      "action : 1\n",
      "new state: [0 1 3 5 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.5\n",
      "final state: [0 1 3 5 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "Episode : 71\n",
      "state : [0 1 3 5 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "action : 2\n",
      "new state: [0 1 3 5 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [0 1 3 5 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 2\n",
      "new state: [0 1 2 5 1 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.5\n",
      "state : [0 1 2 5 1 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1]\n",
      "action : 1\n",
      "new state: [0 2 2 5 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.5\n",
      "state : [0 2 2 5 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 1\n",
      "new state: [1 1 2 5 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "reward: -0.5\n",
      "epReward so far: 0.0\n",
      "state : [1 1 2 5 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "action : 1\n",
      "new state: [1 0 2 5 3 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "final state: [1 0 2 5 3 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "Episode : 72\n",
      "state : [1 0 2 5 3 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "action : 0\n",
      "new state: [0 0 2 6 2 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [0 0 2 6 2 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "action : 2\n",
      "new state: [0 0 2 6 2 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "reward: -0.5\n",
      "epReward so far: -0.25\n",
      "state : [0 0 2 6 2 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "action : 2\n",
      "new state: [0 0 2 6 0 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.0\n",
      "state : [0 0 2 6 0 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "action : 2\n",
      "new state: [0 0 2 6 0 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 0]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [0 0 2 6 0 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 0]\n",
      "action : 3\n",
      "new state: [1 0 2 5 0 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "reward: 0.25\n",
      "epReward so far: -0.25\n",
      "final state: [1 0 2 5 0 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "Episode : 73\n",
      "state : [1 0 2 5 0 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "action : 0\n",
      "new state: [1 0 3 5 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: 0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 0 3 5 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 0\n",
      "new state: [2 0 3 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 0 3 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "action : 3\n",
      "new state: [2 0 3 4 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [2 0 3 4 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "action : 3\n",
      "new state: [2 0 3 4 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.25\n",
      "state : [2 0 3 4 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "action : 0\n",
      "new state: [2 0 4 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.25\n",
      "final state: [2 0 4 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "Episode : 74\n",
      "state : [2 0 4 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "action : 3\n",
      "new state: [2 0 4 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 0 4 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "action : 2\n",
      "new state: [2 0 3 4 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [2 0 3 4 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "action : 0\n",
      "new state: [2 0 3 4 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: -0.75\n",
      "epReward so far: -0.5\n",
      "state : [2 0 3 4 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 0\n",
      "new state: [3 0 3 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "reward: -0.0\n",
      "epReward so far: -0.5\n",
      "state : [3 0 3 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "action : 0\n",
      "new state: [2 0 3 4 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: 0.25\n",
      "epReward so far: -0.25\n",
      "final state: [2 0 3 4 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "Episode : 75\n",
      "state : [2 0 3 4 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 2\n",
      "new state: [2 0 2 4 1 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 3]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [2 0 2 4 1 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 3]\n",
      "action : 3\n",
      "new state: [2 1 2 4 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.25\n",
      "state : [2 1 2 4 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 0\n",
      "new state: [1 1 2 5 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: 0.25\n",
      "epReward so far: 0.5\n",
      "state : [1 1 2 5 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 0\n",
      "new state: [0 1 2 5 2 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.75\n",
      "state : [0 1 2 5 2 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2]\n",
      "action : 2\n",
      "new state: [0 1 3 5 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: 0.0\n",
      "epReward so far: 0.75\n",
      "final state: [0 1 3 5 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "Episode : 76\n",
      "state : [0 1 3 5 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 2\n",
      "new state: [0 1 2 6 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [0 1 2 6 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 2\n",
      "new state: [0 1 1 6 3 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.5\n",
      "state : [0 1 1 6 3 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "action : 3\n",
      "new state: [0 1 1 6 1 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.75\n",
      "state : [0 1 1 6 1 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "action : 1\n",
      "new state: [0 0 1 7 1 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "reward: 0.25\n",
      "epReward so far: 1.0\n",
      "state : [0 0 1 7 1 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "action : 2\n",
      "new state: [0 1 0 7 2 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0]\n",
      "reward: -0.5\n",
      "epReward so far: 0.5\n",
      "final state: [0 1 0 7 2 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0]\n",
      "Episode : 77\n",
      "state : [0 1 0 7 2 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0]\n",
      "action : 1\n",
      "new state: [0 1 1 7 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 1 1 7 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 2\n",
      "new state: [0 1 2 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 1 2 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "action : 2\n",
      "new state: [0 1 2 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "reward: 0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 1 2 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "action : 1\n",
      "new state: [0 0 2 7 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [0 0 2 7 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "action : 2\n",
      "new state: [0 0 2 7 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.5\n",
      "final state: [0 0 2 7 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "Episode : 78\n",
      "state : [0 0 2 7 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 2\n",
      "new state: [1 0 2 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [1 0 2 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "action : 3\n",
      "new state: [1 0 2 6 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: 0.25\n",
      "epReward so far: -0.5\n",
      "state : [1 0 2 6 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 0\n",
      "new state: [0 1 2 6 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [0 1 2 6 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 3\n",
      "new state: [0 1 3 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "reward: -0.0\n",
      "epReward so far: -1.25\n",
      "state : [0 1 3 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "action : 1\n",
      "new state: [0 1 3 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "reward: 0.0\n",
      "epReward so far: -1.25\n",
      "final state: [0 1 3 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "Episode : 79\n",
      "state : [0 1 3 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "action : 3\n",
      "new state: [0 1 3 5 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [0 1 3 5 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "action : 1\n",
      "new state: [0 1 3 5 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.5\n",
      "state : [0 1 3 5 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 1\n",
      "new state: [0 1 4 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [0 1 4 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "action : 1\n",
      "new state: [0 0 4 5 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: 0.25\n",
      "epReward so far: -1.0\n",
      "state : [0 0 4 5 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 3\n",
      "new state: [0 0 4 4 3 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2]\n",
      "reward: 0.25\n",
      "epReward so far: -0.75\n",
      "final state: [0 0 4 4 3 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2]\n",
      "Episode : 80\n",
      "state : [0 0 4 4 3 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2]\n",
      "action : 2\n",
      "new state: [0 0 4 5 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 0 4 5 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 3\n",
      "new state: [0 0 5 4 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [0 0 5 4 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 2\n",
      "new state: [0 0 5 4 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: -0.75\n",
      "epReward so far: -0.5\n",
      "state : [0 0 5 4 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 3\n",
      "new state: [0 0 6 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "reward: 0.0\n",
      "epReward so far: -0.5\n",
      "state : [0 0 6 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "action : 2\n",
      "new state: [0 0 6 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "reward: 0.0\n",
      "epReward so far: -0.5\n",
      "final state: [0 0 6 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "Episode : 81\n",
      "state : [0 0 6 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "action : 2\n",
      "new state: [0 0 5 4 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [0 0 5 4 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "action : 2\n",
      "new state: [0 0 4 4 0 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2]\n",
      "reward: -0.5\n",
      "epReward so far: -0.25\n",
      "state : [0 0 4 4 0 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2]\n",
      "action : 2\n",
      "new state: [1 0 4 4 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "reward: -0.0\n",
      "epReward so far: -0.25\n",
      "state : [1 0 4 4 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "action : 3\n",
      "new state: [1 0 5 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: 0.25\n",
      "epReward so far: 0.0\n",
      "state : [1 0 5 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 3\n",
      "new state: [1 0 5 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "reward: 0.0\n",
      "epReward so far: 0.0\n",
      "final state: [1 0 5 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "Episode : 82\n",
      "state : [1 0 5 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "action : 0\n",
      "new state: [1 0 5 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [1 0 5 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 0\n",
      "new state: [0 1 5 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [0 1 5 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "action : 2\n",
      "new state: [1 1 4 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: 0.25\n",
      "epReward so far: -1.0\n",
      "state : [1 1 4 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 3\n",
      "new state: [1 1 4 2 1 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "reward: 0.25\n",
      "epReward so far: -0.75\n",
      "state : [1 1 4 2 1 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "action : 3\n",
      "new state: [1 2 4 1 1 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "reward: 0.25\n",
      "epReward so far: -0.5\n",
      "final state: [1 2 4 1 1 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "Episode : 83\n",
      "state : [1 2 4 1 1 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "action : 3\n",
      "new state: [1 3 4 0 1 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [1 3 4 0 1 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "action : 0\n",
      "new state: [1 4 4 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.5\n",
      "state : [1 4 4 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 0\n",
      "new state: [0 5 4 0 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "reward: 0.25\n",
      "epReward so far: -0.25\n",
      "state : [0 5 4 0 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "action : 1\n",
      "new state: [0 4 4 0 2 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.0\n",
      "state : [0 4 4 0 2 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "action : 1\n",
      "new state: [0 4 5 0 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "final state: [0 4 5 0 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "Episode : 84\n",
      "state : [0 4 5 0 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "action : 1\n",
      "new state: [0 3 6 0 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [0 3 6 0 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 1\n",
      "new state: [0 3 6 0 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.5\n",
      "state : [0 3 6 0 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 1\n",
      "new state: [0 2 7 0 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: -0.5\n",
      "epReward so far: -1.0\n",
      "state : [0 2 7 0 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 2\n",
      "new state: [0 2 6 0 2 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 3]\n",
      "reward: 0.25\n",
      "epReward so far: -0.75\n",
      "state : [0 2 6 0 2 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 3]\n",
      "action : 2\n",
      "new state: [0 2 6 0 3 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 2]\n",
      "reward: 0.25\n",
      "epReward so far: -0.5\n",
      "final state: [0 2 6 0 3 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 2]\n",
      "Episode : 85\n",
      "state : [0 2 6 0 3 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 2]\n",
      "action : 1\n",
      "new state: [0 1 6 1 3 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [0 1 6 1 3 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2]\n",
      "action : 2\n",
      "new state: [0 1 6 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: 0.0\n",
      "epReward so far: -0.5\n",
      "state : [0 1 6 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 1\n",
      "new state: [0 1 7 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [0 1 7 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "action : 1\n",
      "new state: [0 0 7 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: -0.5\n",
      "epReward so far: -1.75\n",
      "state : [0 0 7 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 2\n",
      "new state: [0 0 7 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "reward: 0.0\n",
      "epReward so far: -1.75\n",
      "final state: [0 0 7 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "Episode : 86\n",
      "state : [0 0 7 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "action : 3\n",
      "new state: [0 1 7 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [0 1 7 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 1\n",
      "new state: [0 1 7 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.5\n",
      "state : [0 1 7 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 2\n",
      "new state: [1 1 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: -0.5\n",
      "state : [1 1 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "action : 0\n",
      "new state: [0 1 7 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "reward: 0.25\n",
      "epReward so far: -0.25\n",
      "state : [0 1 7 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "action : 3\n",
      "new state: [0 1 7 0 2 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 3]\n",
      "reward: 0.25\n",
      "epReward so far: 0.0\n",
      "final state: [0 1 7 0 2 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 3]\n",
      "Episode : 87\n",
      "state : [0 1 7 0 2 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 3]\n",
      "action : 1\n",
      "new state: [0 1 8 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [0 1 8 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 2\n",
      "new state: [1 1 7 0 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: 0.25\n",
      "epReward so far: -0.5\n",
      "state : [1 1 7 0 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 0\n",
      "new state: [0 1 7 0 3 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 3]\n",
      "reward: -0.5\n",
      "epReward so far: -1.0\n",
      "state : [0 1 7 0 3 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 3]\n",
      "action : 1\n",
      "new state: [0 1 7 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: -0.75\n",
      "epReward so far: -1.75\n",
      "state : [0 1 7 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 1\n",
      "new state: [0 1 7 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "reward: -0.5\n",
      "epReward so far: -2.25\n",
      "final state: [0 1 7 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "Episode : 88\n",
      "state : [0 1 7 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "action : 1\n",
      "new state: [0 0 7 1 2 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 3]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [0 0 7 1 2 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 3]\n",
      "action : 2\n",
      "new state: [0 0 7 1 3 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 2]\n",
      "reward: -0.5\n",
      "epReward so far: -0.25\n",
      "state : [0 0 7 1 3 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 2]\n",
      "action : 3\n",
      "new state: [1 0 7 1 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "reward: -0.0\n",
      "epReward so far: -0.25\n",
      "state : [1 0 7 1 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "action : 0\n",
      "new state: [0 0 7 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: -0.5\n",
      "epReward so far: -0.75\n",
      "state : [0 0 7 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 2\n",
      "new state: [0 0 6 2 3 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 3]\n",
      "reward: -0.5\n",
      "epReward so far: -1.25\n",
      "final state: [0 0 6 2 3 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 3]\n",
      "Episode : 89\n",
      "state : [0 0 6 2 3 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 3]\n",
      "action : 2\n",
      "new state: [0 0 6 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [0 0 6 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 3\n",
      "new state: [0 0 6 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "reward: 0.25\n",
      "epReward so far: -0.5\n",
      "state : [0 0 6 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "action : 2\n",
      "new state: [0 0 5 3 2 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "reward: -0.5\n",
      "epReward so far: -1.0\n",
      "state : [0 0 5 3 2 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "action : 2\n",
      "new state: [0 0 5 3 2 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "reward: -0.5\n",
      "epReward so far: -1.5\n",
      "state : [0 0 5 3 2 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "action : 3\n",
      "new state: [0 0 6 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: -0.0\n",
      "epReward so far: -1.5\n",
      "final state: [0 0 6 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "Episode : 90\n",
      "state : [0 0 6 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 3\n",
      "new state: [0 0 7 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [0 0 7 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "action : 2\n",
      "new state: [0 0 7 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.5\n",
      "state : [0 0 7 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "action : 2\n",
      "new state: [0 0 7 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: 0.25\n",
      "epReward so far: -0.25\n",
      "state : [0 0 7 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 2\n",
      "new state: [0 0 7 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: -0.0\n",
      "epReward so far: -0.25\n",
      "state : [0 0 7 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 2\n",
      "new state: [0 1 6 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.0\n",
      "final state: [0 1 6 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "Episode : 91\n",
      "state : [0 1 6 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "action : 3\n",
      "new state: [0 1 6 1 3 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [0 1 6 1 3 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 0]\n",
      "action : 3\n",
      "new state: [0 1 6 1 0 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 3]\n",
      "reward: 0.25\n",
      "epReward so far: 0.5\n",
      "state : [0 1 6 1 0 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 3]\n",
      "action : 1\n",
      "new state: [1 0 6 1 0 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0]\n",
      "reward: -0.5\n",
      "epReward so far: 0.0\n",
      "state : [1 0 6 1 0 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0]\n",
      "action : 0\n",
      "new state: [1 0 6 1 0 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 2]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [1 0 6 1 0 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 2]\n",
      "action : 3\n",
      "new state: [1 0 6 1 0 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "reward: 0.25\n",
      "epReward so far: -0.25\n",
      "final state: [1 0 6 1 0 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "Episode : 92\n",
      "state : [1 0 6 1 0 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "action : 2\n",
      "new state: [2 0 5 1 0 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [2 0 5 1 0 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "action : 3\n",
      "new state: [2 0 6 0 0 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 3]\n",
      "reward: 0.25\n",
      "epReward so far: 0.5\n",
      "state : [2 0 6 0 0 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 3]\n",
      "action : 0\n",
      "new state: [3 0 6 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "reward: -0.75\n",
      "epReward so far: -0.25\n",
      "state : [3 0 6 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "action : 0\n",
      "new state: [2 1 6 0 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: -0.5\n",
      "epReward so far: -0.75\n",
      "state : [2 1 6 0 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 2\n",
      "new state: [2 1 5 0 3 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2]\n",
      "reward: 0.25\n",
      "epReward so far: -0.5\n",
      "final state: [2 1 5 0 3 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2]\n",
      "Episode : 93\n",
      "state : [2 1 5 0 3 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2]\n",
      "action : 2\n",
      "new state: [2 1 5 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: 0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 1 5 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 3\n",
      "new state: [3 1 5 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "reward: 0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 1 5 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "action : 3\n",
      "new state: [3 1 5 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "reward: 0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 1 5 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "action : 2\n",
      "new state: [3 1 4 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [3 1 4 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "action : 3\n",
      "new state: [3 1 4 0 3 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.5\n",
      "final state: [3 1 4 0 3 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "Episode : 94\n",
      "state : [3 1 4 0 3 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "action : 1\n",
      "new state: [3 0 4 1 2 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [3 0 4 1 2 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "action : 2\n",
      "new state: [4 0 3 1 2 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 3]\n",
      "reward: 0.25\n",
      "epReward so far: 0.5\n",
      "state : [4 0 3 1 2 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 3]\n",
      "action : 0\n",
      "new state: [3 0 4 1 3 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.75\n",
      "state : [3 0 4 1 3 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2]\n",
      "action : 2\n",
      "new state: [4 0 4 1 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "reward: 0.0\n",
      "epReward so far: 0.75\n",
      "state : [4 0 4 1 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "action : 0\n",
      "new state: [4 0 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "reward: -0.75\n",
      "epReward so far: 0.0\n",
      "final state: [4 0 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "Episode : 95\n",
      "state : [4 0 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "action : 2\n",
      "new state: [4 0 3 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [4 0 3 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 2\n",
      "new state: [4 0 2 2 0 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.5\n",
      "state : [4 0 2 2 0 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 1]\n",
      "action : 2\n",
      "new state: [5 0 1 2 1 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.75\n",
      "state : [5 0 1 2 1 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "action : 0\n",
      "new state: [5 0 1 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: 0.0\n",
      "epReward so far: 0.75\n",
      "state : [5 0 1 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 3\n",
      "new state: [5 1 1 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "reward: 0.25\n",
      "epReward so far: 1.0\n",
      "final state: [5 1 1 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "Episode : 96\n",
      "state : [5 1 1 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "action : 2\n",
      "new state: [5 1 1 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [5 1 1 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 3\n",
      "new state: [5 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "reward: 0.0\n",
      "epReward so far: 0.0\n",
      "state : [5 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "action : 2\n",
      "new state: [5 1 1 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [5 1 1 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 0\n",
      "new state: [4 1 1 2 0 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.5\n",
      "state : [4 1 1 2 0 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "action : 0\n",
      "new state: [5 1 1 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.5\n",
      "final state: [5 1 1 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "Episode : 97\n",
      "state : [5 1 1 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 2\n",
      "new state: [5 1 0 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [5 1 0 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 3\n",
      "new state: [5 1 0 2 0 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 3]\n",
      "reward: 0.25\n",
      "epReward so far: 0.5\n",
      "state : [5 1 0 2 0 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 3]\n",
      "action : 0\n",
      "new state: [5 1 0 2 3 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.75\n",
      "state : [5 1 0 2 3 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2]\n",
      "action : 0\n",
      "new state: [5 2 0 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "reward: -0.75\n",
      "epReward so far: 0.0\n",
      "state : [5 2 0 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "action : 0\n",
      "new state: [4 2 0 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "final state: [4 2 0 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "Episode : 98\n",
      "state : [4 2 0 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "action : 0\n",
      "new state: [4 2 0 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: 0.0\n",
      "epReward so far: 0.0\n",
      "state : [4 2 0 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 3\n",
      "new state: [4 3 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [4 3 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "action : 0\n",
      "new state: [3 3 0 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [3 3 0 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 3\n",
      "new state: [3 3 0 2 3 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 3]\n",
      "reward: 0.25\n",
      "epReward so far: 0.5\n",
      "state : [3 3 0 2 3 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 3]\n",
      "action : 1\n",
      "new state: [3 3 0 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.5\n",
      "final state: [3 3 0 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "Episode : 99\n",
      "state : [3 3 0 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 0\n",
      "new state: [3 4 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 4 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "action : 3\n",
      "new state: [3 4 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "reward: 0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 4 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "action : 0\n",
      "new state: [3 4 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "reward: 0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 4 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "action : 0\n",
      "new state: [3 4 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "reward: 0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 4 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "action : 0\n",
      "new state: [3 4 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "final state: [3 4 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "Episode : 0\n",
      "state : [3 4 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "action : 0\n",
      "new state: [2 4 0 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [2 4 0 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 0\n",
      "new state: [1 4 0 3 2 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 0]\n",
      "reward: -0.5\n",
      "epReward so far: -0.25\n",
      "state : [1 4 0 3 2 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 0]\n",
      "action : 3\n",
      "new state: [1 4 1 2 0 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 3]\n",
      "reward: 0.25\n",
      "epReward so far: 0.0\n",
      "state : [1 4 1 2 0 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 3]\n",
      "action : 3\n",
      "new state: [1 4 1 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: 0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 4 1 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 2\n",
      "new state: [2 4 0 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "final state: [2 4 0 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "Episode : 1\n",
      "state : [2 4 0 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "action : 3\n",
      "new state: [2 4 0 2 3 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [2 4 0 2 3 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0]\n",
      "action : 1\n",
      "new state: [2 4 0 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.25\n",
      "state : [2 4 0 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 0\n",
      "new state: [3 4 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.5\n",
      "state : [3 4 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "action : 0\n",
      "new state: [2 4 0 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: 0.25\n",
      "epReward so far: -0.25\n",
      "state : [2 4 0 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 0\n",
      "new state: [2 4 0 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.0\n",
      "final state: [2 4 0 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "Episode : 2\n",
      "state : [2 4 0 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 0\n",
      "new state: [1 4 1 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [1 4 1 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 0\n",
      "new state: [0 4 1 3 3 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 3]\n",
      "reward: 0.25\n",
      "epReward so far: 0.5\n",
      "state : [0 4 1 3 3 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 3]\n",
      "action : 1\n",
      "new state: [0 3 1 4 3 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 3]\n",
      "reward: 0.25\n",
      "epReward so far: 0.75\n",
      "state : [0 3 1 4 3 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 3]\n",
      "action : 1\n",
      "new state: [0 3 2 4 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.75\n",
      "state : [0 3 2 4 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "action : 2\n",
      "new state: [0 3 2 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.75\n",
      "final state: [0 3 2 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "Episode : 3\n",
      "state : [0 3 2 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "action : 2\n",
      "new state: [0 3 1 5 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [0 3 1 5 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 1\n",
      "new state: [0 3 1 5 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "reward: 0.0\n",
      "epReward so far: 0.25\n",
      "state : [0 3 1 5 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "action : 1\n",
      "new state: [0 4 1 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.25\n",
      "state : [0 4 1 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "action : 1\n",
      "new state: [0 3 1 5 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: 0.25\n",
      "epReward so far: 0.5\n",
      "state : [0 3 1 5 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 2\n",
      "new state: [0 3 1 5 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.5\n",
      "final state: [0 3 1 5 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "Episode : 4\n",
      "state : [0 3 1 5 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 3\n",
      "new state: [0 3 1 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "reward: 0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 3 1 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "action : 1\n",
      "new state: [0 3 1 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [0 3 1 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "action : 1\n",
      "new state: [0 2 1 6 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: -0.5\n",
      "epReward so far: -1.25\n",
      "state : [0 2 1 6 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 1\n",
      "new state: [0 1 1 6 3 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2]\n",
      "reward: -0.5\n",
      "epReward so far: -1.75\n",
      "state : [0 1 1 6 3 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2]\n",
      "action : 2\n",
      "new state: [0 1 1 7 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: -0.0\n",
      "epReward so far: -1.75\n",
      "final state: [0 1 1 7 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "Episode : 5\n",
      "state : [0 1 1 7 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 2\n",
      "new state: [0 1 1 7 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [0 1 1 7 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 1\n",
      "new state: [0 0 1 7 3 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 2]\n",
      "reward: -0.5\n",
      "epReward so far: -0.25\n",
      "state : [0 0 1 7 3 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 2]\n",
      "action : 3\n",
      "new state: [0 0 1 8 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: -0.0\n",
      "epReward so far: -0.25\n",
      "state : [0 0 1 8 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 3\n",
      "new state: [0 0 2 7 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.0\n",
      "state : [0 0 2 7 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 2\n",
      "new state: [0 0 2 7 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "final state: [0 0 2 7 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "Episode : 6\n",
      "state : [0 0 2 7 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "action : 2\n",
      "new state: [0 1 1 7 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [0 1 1 7 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "action : 1\n",
      "new state: [0 0 1 7 0 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 1]\n",
      "reward: 0.25\n",
      "epReward so far: -0.25\n",
      "state : [0 0 1 7 0 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 1]\n",
      "action : 2\n",
      "new state: [1 0 0 7 1 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "reward: -0.5\n",
      "epReward so far: -0.75\n",
      "state : [1 0 0 7 1 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "action : 0\n",
      "new state: [1 0 0 8 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [1 0 0 8 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "action : 0\n",
      "new state: [1 1 0 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "reward: -0.0\n",
      "epReward so far: -1.5\n",
      "final state: [1 1 0 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "Episode : 7\n",
      "state : [1 1 0 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "action : 1\n",
      "new state: [1 0 0 8 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [1 0 0 8 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 0\n",
      "new state: [0 0 0 8 0 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 3]\n",
      "reward: 0.25\n",
      "epReward so far: 0.5\n",
      "state : [0 0 0 8 0 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 3]\n",
      "action : 3\n",
      "new state: [1 0 0 7 3 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 3]\n",
      "reward: -0.5\n",
      "epReward so far: 0.0\n",
      "state : [1 0 0 7 3 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 3]\n",
      "action : 0\n",
      "new state: [0 0 1 7 3 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 3]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [0 0 1 7 3 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 3]\n",
      "action : 3\n",
      "new state: [0 0 1 8 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: 0.0\n",
      "epReward so far: -0.5\n",
      "final state: [0 0 1 8 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "Episode : 8\n",
      "state : [0 0 1 8 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 2\n",
      "new state: [0 0 0 9 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [0 0 0 9 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 3\n",
      "new state: [0 0 1 8 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.5\n",
      "state : [0 0 1 8 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "action : 2\n",
      "new state: [0 0 0 9 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "reward: 0.25\n",
      "epReward so far: -0.25\n",
      "state : [0 0 0 9 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "action : 3\n",
      "new state: [0 0 0 9 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.0\n",
      "state : [0 0 0 9 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 3\n",
      "new state: [0 1 0 9 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "reward: -0.75\n",
      "epReward so far: -1.75\n",
      "final state: [0 1 0 9 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "Episode : 9\n",
      "state : [0 1 0 9 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "action : 3\n",
      "new state: [0 1 0 8 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [0 1 0 8 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 1\n",
      "new state: [0 1 0 8 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: -0.75\n",
      "epReward so far: -0.5\n",
      "state : [0 1 0 8 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 1\n",
      "new state: [1 0 0 8 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: -0.5\n",
      "epReward so far: -1.0\n",
      "state : [1 0 0 8 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 0\n",
      "new state: [0 0 0 8 0 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "reward: 0.25\n",
      "epReward so far: -0.75\n",
      "state : [0 0 0 8 0 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "action : 3\n",
      "new state: [1 0 0 7 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "reward: -0.5\n",
      "epReward so far: -1.25\n",
      "final state: [1 0 0 7 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "Episode : 10\n",
      "state : [1 0 0 7 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "action : 0\n",
      "new state: [1 0 0 8 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "reward: 0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 0 0 8 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "action : 0\n",
      "new state: [0 0 1 8 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [0 0 1 8 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "action : 2\n",
      "new state: [1 0 0 8 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.5\n",
      "state : [1 0 0 8 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 0\n",
      "new state: [0 2 0 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [0 2 0 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "action : 1\n",
      "new state: [1 1 0 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "final state: [1 1 0 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "Episode : 11\n",
      "state : [1 1 0 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "action : 0\n",
      "new state: [1 1 0 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [1 1 0 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "action : 0\n",
      "new state: [0 1 1 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [0 1 1 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "action : 2\n",
      "new state: [0 1 0 8 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "reward: 0.25\n",
      "epReward so far: -1.25\n",
      "state : [0 1 0 8 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "action : 1\n",
      "new state: [0 0 0 8 1 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 1]\n",
      "reward: 0.25\n",
      "epReward so far: -1.0\n",
      "state : [0 0 0 8 1 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 1]\n",
      "action : 3\n",
      "new state: [0 1 0 7 1 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2]\n",
      "reward: -0.5\n",
      "epReward so far: -1.5\n",
      "final state: [0 1 0 7 1 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2]\n",
      "Episode : 12\n",
      "state : [0 1 0 7 1 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2]\n",
      "action : 1\n",
      "new state: [0 1 1 7 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [0 1 1 7 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "action : 3\n",
      "new state: [0 2 1 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "reward: -0.0\n",
      "epReward so far: -0.75\n",
      "state : [0 2 1 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "action : 1\n",
      "new state: [0 2 1 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [0 2 1 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "action : 3\n",
      "new state: [0 2 1 6 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "reward: 0.25\n",
      "epReward so far: -1.25\n",
      "state : [0 2 1 6 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "action : 1\n",
      "new state: [0 1 1 6 2 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0]\n",
      "reward: 0.25\n",
      "epReward so far: -1.0\n",
      "final state: [0 1 1 6 2 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0]\n",
      "Episode : 13\n",
      "state : [0 1 1 6 2 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0]\n",
      "action : 1\n",
      "new state: [0 0 2 6 0 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [0 0 2 6 0 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1]\n",
      "action : 2\n",
      "new state: [1 0 2 6 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: -0.75\n",
      "epReward so far: -0.5\n",
      "state : [1 0 2 6 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 0\n",
      "new state: [1 0 2 6 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: 0.25\n",
      "epReward so far: -0.25\n",
      "state : [1 0 2 6 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 0\n",
      "new state: [0 1 2 6 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "reward: -0.75\n",
      "epReward so far: -1.0\n",
      "state : [0 1 2 6 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "action : 1\n",
      "new state: [0 1 2 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "reward: -0.0\n",
      "epReward so far: -1.0\n",
      "final state: [0 1 2 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "Episode : 14\n",
      "state : [0 1 2 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "action : 1\n",
      "new state: [0 1 2 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [0 1 2 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "action : 3\n",
      "new state: [0 1 2 6 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: 0.25\n",
      "epReward so far: -0.5\n",
      "state : [0 1 2 6 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 2\n",
      "new state: [0 1 1 6 0 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2]\n",
      "reward: 0.25\n",
      "epReward so far: -0.25\n",
      "state : [0 1 1 6 0 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2]\n",
      "action : 2\n",
      "new state: [1 1 1 6 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "reward: 0.0\n",
      "epReward so far: -0.25\n",
      "state : [1 1 1 6 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "action : 0\n",
      "new state: [1 1 1 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "reward: 0.0\n",
      "epReward so far: -0.25\n",
      "final state: [1 1 1 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "Episode : 15\n",
      "state : [1 1 1 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "action : 1\n",
      "new state: [1 1 1 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 1 1 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "action : 2\n",
      "new state: [1 1 1 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 1 1 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "action : 3\n",
      "new state: [1 1 1 6 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [1 1 1 6 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 3\n",
      "new state: [1 1 1 6 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: 0.0\n",
      "epReward so far: 0.25\n",
      "state : [1 1 1 6 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 2\n",
      "new state: [2 1 1 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "reward: 0.0\n",
      "epReward so far: 0.25\n",
      "final state: [2 1 1 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "Episode : 16\n",
      "state : [2 1 1 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "action : 2\n",
      "new state: [2 1 0 6 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [2 1 0 6 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 1\n",
      "new state: [2 1 0 6 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: 0.0\n",
      "epReward so far: 0.25\n",
      "state : [2 1 0 6 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 3\n",
      "new state: [2 2 0 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "reward: 0.0\n",
      "epReward so far: 0.25\n",
      "state : [2 2 0 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "action : 3\n",
      "new state: [2 2 0 5 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: 0.25\n",
      "epReward so far: 0.5\n",
      "state : [2 2 0 5 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 3\n",
      "new state: [2 2 0 5 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: 0.0\n",
      "epReward so far: 0.5\n",
      "final state: [2 2 0 5 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "Episode : 17\n",
      "state : [2 2 0 5 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 0\n",
      "new state: [3 2 0 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [3 2 0 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "action : 1\n",
      "new state: [3 1 0 5 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: 0.25\n",
      "epReward so far: -0.5\n",
      "state : [3 1 0 5 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 1\n",
      "new state: [3 1 0 5 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "reward: 0.0\n",
      "epReward so far: -0.5\n",
      "state : [3 1 0 5 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "action : 1\n",
      "new state: [3 0 1 5 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: 0.25\n",
      "epReward so far: -0.25\n",
      "state : [3 0 1 5 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 3\n",
      "new state: [3 0 1 5 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: 0.0\n",
      "epReward so far: -0.25\n",
      "final state: [3 0 1 5 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "Episode : 18\n",
      "state : [3 0 1 5 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 3\n",
      "new state: [4 0 1 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "reward: 0.0\n",
      "epReward so far: 0.0\n",
      "state : [4 0 1 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "action : 0\n",
      "new state: [3 0 1 5 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [3 0 1 5 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "action : 3\n",
      "new state: [3 0 1 5 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.25\n",
      "state : [3 0 1 5 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 2\n",
      "new state: [3 1 1 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.25\n",
      "state : [3 1 1 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "action : 1\n",
      "new state: [3 0 1 5 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.5\n",
      "final state: [3 0 1 5 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "Episode : 19\n",
      "state : [3 0 1 5 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "action : 2\n",
      "new state: [3 0 1 5 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 0 1 5 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "action : 2\n",
      "new state: [3 0 2 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 0 2 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "action : 0\n",
      "new state: [2 0 2 5 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [2 0 2 5 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "action : 2\n",
      "new state: [2 0 1 5 3 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 3]\n",
      "reward: 0.25\n",
      "epReward so far: -0.25\n",
      "state : [2 0 1 5 3 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 3]\n",
      "action : 2\n",
      "new state: [2 0 0 6 3 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 3]\n",
      "reward: 0.25\n",
      "epReward so far: 0.0\n",
      "final state: [2 0 0 6 3 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 3]\n",
      "Episode : 20\n",
      "state : [2 0 0 6 3 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 3]\n",
      "action : 0\n",
      "new state: [1 1 0 6 3 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 1]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [1 1 0 6 3 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 1]\n",
      "action : 0\n",
      "new state: [0 1 0 7 1 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 1]\n",
      "reward: -0.5\n",
      "epReward so far: -1.0\n",
      "state : [0 1 0 7 1 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 1]\n",
      "action : 1\n",
      "new state: [0 1 0 8 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "reward: -0.75\n",
      "epReward so far: -1.75\n",
      "state : [0 1 0 8 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "action : 1\n",
      "new state: [0 1 0 8 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "reward: 0.25\n",
      "epReward so far: -1.5\n",
      "state : [0 1 0 8 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "action : 1\n",
      "new state: [0 1 0 8 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "reward: -0.75\n",
      "epReward so far: -2.25\n",
      "final state: [0 1 0 8 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "Episode : 21\n",
      "state : [0 1 0 8 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "action : 3\n",
      "new state: [0 1 1 7 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [0 1 1 7 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 1\n",
      "new state: [0 1 1 7 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: -0.75\n",
      "epReward so far: -0.5\n",
      "state : [0 1 1 7 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 1\n",
      "new state: [1 1 1 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [1 1 1 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "action : 1\n",
      "new state: [1 0 1 7 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "reward: 0.25\n",
      "epReward so far: -1.0\n",
      "state : [1 0 1 7 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "action : 3\n",
      "new state: [1 0 1 7 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: -0.0\n",
      "epReward so far: -1.0\n",
      "final state: [1 0 1 7 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "Episode : 22\n",
      "state : [1 0 1 7 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 3\n",
      "new state: [1 0 2 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "reward: 0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 0 2 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "action : 0\n",
      "new state: [0 0 2 7 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [0 0 2 7 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "action : 2\n",
      "new state: [0 0 2 7 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [0 0 2 7 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 3\n",
      "new state: [0 0 2 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "reward: 0.0\n",
      "epReward so far: -1.25\n",
      "state : [0 0 2 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "action : 2\n",
      "new state: [0 0 2 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "final state: [0 0 2 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "Episode : 23\n",
      "state : [0 0 2 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "action : 2\n",
      "new state: [0 0 1 8 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [0 0 1 8 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 2\n",
      "new state: [0 0 1 8 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.5\n",
      "state : [0 0 1 8 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 2\n",
      "new state: [0 0 1 9 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [0 0 1 9 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "action : 2\n",
      "new state: [0 0 0 9 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "reward: -0.5\n",
      "epReward so far: -1.75\n",
      "state : [0 0 0 9 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "action : 3\n",
      "new state: [0 0 0 9 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "reward: -0.75\n",
      "epReward so far: -2.5\n",
      "final state: [0 0 0 9 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "Episode : 24\n",
      "state : [0 0 0 9 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "action : 3\n",
      "new state: [ 0  0  0 10  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  3  0]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [ 0  0  0 10  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  3  0]\n",
      "action : 3\n",
      "new state: [0 0 0 9 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: 0.25\n",
      "epReward so far: -0.5\n",
      "state : [0 0 0 9 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 3\n",
      "new state: [0 0 0 9 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: -0.0\n",
      "epReward so far: -0.5\n",
      "state : [0 0 0 9 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 3\n",
      "new state: [1 0 0 9 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [1 0 0 9 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "action : 3\n",
      "new state: [1 0 0 8 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "reward: 0.25\n",
      "epReward so far: -1.0\n",
      "final state: [1 0 0 8 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "Episode : 25\n",
      "state : [1 0 0 8 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "action : 0\n",
      "new state: [1 0 0 8 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [1 0 0 8 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 3\n",
      "new state: [1 1 0 7 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: 0.25\n",
      "epReward so far: -0.5\n",
      "state : [1 1 0 7 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 3\n",
      "new state: [1 1 0 6 1 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1]\n",
      "reward: 0.25\n",
      "epReward so far: -0.25\n",
      "state : [1 1 0 6 1 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1]\n",
      "action : 1\n",
      "new state: [1 2 0 6 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "reward: 0.0\n",
      "epReward so far: -0.25\n",
      "state : [1 2 0 6 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "action : 1\n",
      "new state: [1 2 0 6 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.0\n",
      "final state: [1 2 0 6 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "Episode : 26\n",
      "state : [1 2 0 6 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 3\n",
      "new state: [1 2 0 5 2 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [1 2 0 5 2 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "action : 0\n",
      "new state: [1 2 1 5 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.25\n",
      "state : [1 2 1 5 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 0\n",
      "new state: [0 2 2 5 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.5\n",
      "state : [0 2 2 5 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 3\n",
      "new state: [0 2 2 4 2 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.75\n",
      "state : [0 2 2 4 2 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 2]\n",
      "action : 3\n",
      "new state: [0 2 3 3 2 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 1]\n",
      "reward: 0.25\n",
      "epReward so far: 1.0\n",
      "final state: [0 2 3 3 2 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 1]\n",
      "Episode : 27\n",
      "state : [0 2 3 3 2 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 1]\n",
      "action : 1\n",
      "new state: [0 2 4 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [0 2 4 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 1\n",
      "new state: [0 2 5 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [0 2 5 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "action : 1\n",
      "new state: [0 2 5 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "reward: 0.0\n",
      "epReward so far: -1.5\n",
      "state : [0 2 5 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "action : 3\n",
      "new state: [0 2 5 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "reward: 0.0\n",
      "epReward so far: -1.5\n",
      "state : [0 2 5 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "action : 1\n",
      "new state: [0 2 5 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "reward: -0.75\n",
      "epReward so far: -2.25\n",
      "final state: [0 2 5 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "Episode : 28\n",
      "state : [0 2 5 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "action : 2\n",
      "new state: [0 2 5 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 2 5 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "action : 1\n",
      "new state: [0 1 5 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [0 1 5 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 1\n",
      "new state: [0 0 5 3 2 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0]\n",
      "reward: -0.5\n",
      "epReward so far: -1.0\n",
      "state : [0 0 5 3 2 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0]\n",
      "action : 2\n",
      "new state: [0 0 6 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: -0.75\n",
      "epReward so far: -1.75\n",
      "state : [0 0 6 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 2\n",
      "new state: [0 0 7 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "reward: 0.0\n",
      "epReward so far: -1.75\n",
      "final state: [0 0 7 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "Episode : 29\n",
      "state : [0 0 7 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "action : 3\n",
      "new state: [0 0 7 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 0 7 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "action : 3\n",
      "new state: [0 0 7 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [0 0 7 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "action : 2\n",
      "new state: [0 0 6 2 0 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 3]\n",
      "reward: -0.5\n",
      "epReward so far: -0.25\n",
      "state : [0 0 6 2 0 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 3]\n",
      "action : 3\n",
      "new state: [1 0 6 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "reward: 0.0\n",
      "epReward so far: -0.25\n",
      "state : [1 0 6 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "action : 0\n",
      "new state: [0 0 7 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: -0.5\n",
      "epReward so far: -0.75\n",
      "final state: [0 0 7 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "Episode : 30\n",
      "state : [0 0 7 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 3\n",
      "new state: [0 0 7 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: 0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 0 7 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 2\n",
      "new state: [1 0 7 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "reward: 0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 0 7 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "action : 2\n",
      "new state: [1 0 6 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [1 0 6 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 2\n",
      "new state: [1 0 6 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.25\n",
      "state : [1 0 6 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 2\n",
      "new state: [1 1 6 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "reward: 0.0\n",
      "epReward so far: 0.25\n",
      "final state: [1 1 6 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "Episode : 31\n",
      "state : [1 1 6 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "action : 0\n",
      "new state: [0 1 6 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [0 1 6 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 2\n",
      "new state: [0 1 5 2 2 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.5\n",
      "state : [0 1 5 2 2 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 0]\n",
      "action : 3\n",
      "new state: [0 1 6 1 0 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.75\n",
      "state : [0 1 6 1 0 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "action : 3\n",
      "new state: [1 1 6 0 0 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 3]\n",
      "reward: 0.25\n",
      "epReward so far: 1.0\n",
      "state : [1 1 6 0 0 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 3]\n",
      "action : 2\n",
      "new state: [2 1 6 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: -0.0\n",
      "epReward so far: 1.0\n",
      "final state: [2 1 6 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "Episode : 32\n",
      "state : [2 1 6 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 2\n",
      "new state: [2 2 5 0 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [2 2 5 0 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "action : 1\n",
      "new state: [2 2 5 0 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.25\n",
      "state : [2 2 5 0 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 0\n",
      "new state: [1 2 5 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.5\n",
      "state : [1 2 5 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "action : 2\n",
      "new state: [1 2 5 1 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.5\n",
      "state : [1 2 5 1 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "action : 3\n",
      "new state: [1 2 5 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.75\n",
      "final state: [1 2 5 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "Episode : 33\n",
      "state : [1 2 5 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "action : 1\n",
      "new state: [1 1 5 1 0 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [1 1 5 1 0 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0]\n",
      "action : 1\n",
      "new state: [2 0 5 1 0 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.5\n",
      "state : [2 0 5 1 0 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 0]\n",
      "action : 3\n",
      "new state: [3 0 5 0 0 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.75\n",
      "state : [3 0 5 0 0 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "action : 0\n",
      "new state: [3 0 5 0 1 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 2]\n",
      "reward: -0.5\n",
      "epReward so far: 0.25\n",
      "state : [3 0 5 0 1 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 2]\n",
      "action : 0\n",
      "new state: [4 0 5 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.5\n",
      "final state: [4 0 5 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "Episode : 34\n",
      "state : [4 0 5 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "action : 0\n",
      "new state: [3 1 5 0 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [3 1 5 0 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 0\n",
      "new state: [2 1 5 0 1 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 3]\n",
      "reward: -0.5\n",
      "epReward so far: -0.25\n",
      "state : [2 1 5 0 1 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 3]\n",
      "action : 0\n",
      "new state: [1 2 5 0 3 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.0\n",
      "state : [1 2 5 0 3 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 1]\n",
      "action : 2\n",
      "new state: [1 2 5 0 3 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [1 2 5 0 3 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "action : 0\n",
      "new state: [1 2 5 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "reward: 0.0\n",
      "epReward so far: 0.25\n",
      "final state: [1 2 5 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "Episode : 35\n",
      "state : [1 2 5 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "action : 3\n",
      "new state: [1 3 5 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [1 3 5 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 0\n",
      "new state: [0 3 5 0 0 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.5\n",
      "state : [0 3 5 0 0 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 1]\n",
      "action : 1\n",
      "new state: [1 3 5 0 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.25\n",
      "state : [1 3 5 0 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 0\n",
      "new state: [0 3 6 0 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "reward: -0.5\n",
      "epReward so far: -0.75\n",
      "state : [0 3 6 0 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "action : 1\n",
      "new state: [1 2 6 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "final state: [1 2 6 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "Episode : 36\n",
      "state : [1 2 6 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "action : 1\n",
      "new state: [1 2 6 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [1 2 6 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "action : 1\n",
      "new state: [1 1 6 0 0 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.5\n",
      "state : [1 1 6 0 0 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 1]\n",
      "action : 0\n",
      "new state: [2 1 6 0 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.5\n",
      "state : [2 1 6 0 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 0\n",
      "new state: [1 1 6 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.75\n",
      "state : [1 1 6 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "action : 0\n",
      "new state: [1 1 6 1 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.75\n",
      "final state: [1 1 6 1 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "Episode : 37\n",
      "state : [1 1 6 1 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 2\n",
      "new state: [1 1 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 1 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "action : 0\n",
      "new state: [0 1 7 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [0 1 7 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 3\n",
      "new state: [0 1 7 0 1 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.5\n",
      "state : [0 1 7 0 1 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 1]\n",
      "action : 1\n",
      "new state: [0 2 7 0 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.25\n",
      "state : [0 2 7 0 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 1\n",
      "new state: [0 1 8 0 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: -0.5\n",
      "epReward so far: -0.75\n",
      "final state: [0 1 8 0 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "Episode : 38\n",
      "state : [0 1 8 0 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 2\n",
      "new state: [0 1 7 0 2 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [0 1 7 0 2 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0]\n",
      "action : 1\n",
      "new state: [0 0 8 0 0 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.5\n",
      "state : [0 0 8 0 0 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 1]\n",
      "action : 2\n",
      "new state: [1 0 7 0 0 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 2]\n",
      "reward: -0.5\n",
      "epReward so far: 0.0\n",
      "state : [1 0 7 0 0 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 2]\n",
      "action : 0\n",
      "new state: [2 0 7 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 0 7 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "action : 0\n",
      "new state: [2 1 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "final state: [2 1 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "Episode : 39\n",
      "state : [2 1 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "action : 1\n",
      "new state: [2 1 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 1 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "action : 1\n",
      "new state: [2 1 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 1 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "action : 0\n",
      "new state: [2 1 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 1 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "action : 2\n",
      "new state: [2 1 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "reward: -0.0\n",
      "epReward so far: -0.75\n",
      "state : [2 1 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "action : 0\n",
      "new state: [1 1 7 0 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: -0.5\n",
      "epReward so far: -1.25\n",
      "final state: [1 1 7 0 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "Episode : 40\n",
      "state : [1 1 7 0 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 2\n",
      "new state: [1 1 6 0 2 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [1 1 6 0 2 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 2]\n",
      "action : 0\n",
      "new state: [0 1 7 0 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "reward: -0.5\n",
      "epReward so far: -0.25\n",
      "state : [0 1 7 0 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "action : 2\n",
      "new state: [0 1 6 1 2 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.0\n",
      "state : [0 1 6 1 2 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2]\n",
      "action : 2\n",
      "new state: [0 1 7 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 1 7 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "action : 1\n",
      "new state: [1 0 7 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "final state: [1 0 7 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "Episode : 41\n",
      "state : [1 0 7 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "action : 0\n",
      "new state: [0 0 7 1 0 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 1]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [0 0 7 1 0 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 1]\n",
      "action : 2\n",
      "new state: [1 0 7 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: -0.0\n",
      "epReward so far: -0.5\n",
      "state : [1 0 7 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 0\n",
      "new state: [1 0 7 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: 0.25\n",
      "epReward so far: -0.25\n",
      "state : [1 0 7 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 3\n",
      "new state: [1 0 7 1 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: -0.25\n",
      "state : [1 0 7 1 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 2\n",
      "new state: [1 0 7 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "reward: 0.0\n",
      "epReward so far: -0.25\n",
      "final state: [1 0 7 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "Episode : 42\n",
      "state : [1 0 7 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "action : 2\n",
      "new state: [1 0 6 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [1 0 6 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 3\n",
      "new state: [1 0 6 1 3 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.5\n",
      "state : [1 0 6 1 3 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "action : 3\n",
      "new state: [1 0 6 1 1 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.75\n",
      "state : [1 0 6 1 1 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "action : 3\n",
      "new state: [1 0 7 0 1 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0]\n",
      "reward: 0.25\n",
      "epReward so far: 1.0\n",
      "state : [1 0 7 0 1 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0]\n",
      "action : 0\n",
      "new state: [0 1 7 0 0 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 3]\n",
      "reward: -0.5\n",
      "epReward so far: 0.5\n",
      "final state: [0 1 7 0 0 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 3]\n",
      "Episode : 43\n",
      "state : [0 1 7 0 0 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 3]\n",
      "action : 2\n",
      "new state: [0 2 6 0 0 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [0 2 6 0 0 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "action : 1\n",
      "new state: [2 1 6 0 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.5\n",
      "state : [2 1 6 0 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 1\n",
      "new state: [2 1 6 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "reward: -0.0\n",
      "epReward so far: -0.5\n",
      "state : [2 1 6 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "action : 3\n",
      "new state: [2 1 6 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "reward: 0.25\n",
      "epReward so far: -0.25\n",
      "state : [2 1 6 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "action : 0\n",
      "new state: [1 1 6 0 0 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.0\n",
      "final state: [1 1 6 0 0 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "Episode : 44\n",
      "state : [1 1 6 0 0 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "action : 0\n",
      "new state: [2 1 6 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "reward: 0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 1 6 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "action : 0\n",
      "new state: [1 2 6 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [1 2 6 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 0\n",
      "new state: [1 2 6 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [1 2 6 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 1\n",
      "new state: [2 2 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "reward: 0.0\n",
      "epReward so far: -1.25\n",
      "state : [2 2 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "action : 0\n",
      "new state: [2 2 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "final state: [2 2 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "Episode : 45\n",
      "state : [2 2 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "action : 0\n",
      "new state: [1 2 6 0 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [1 2 6 0 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 0\n",
      "new state: [1 2 6 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: -0.75\n",
      "epReward so far: -0.5\n",
      "state : [1 2 6 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 2\n",
      "new state: [1 3 5 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: 0.25\n",
      "epReward so far: -0.25\n",
      "state : [1 3 5 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 1\n",
      "new state: [1 3 5 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: -0.0\n",
      "epReward so far: -0.25\n",
      "state : [1 3 5 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 0\n",
      "new state: [1 3 5 0 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "reward: -0.5\n",
      "epReward so far: -0.75\n",
      "final state: [1 3 5 0 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "Episode : 46\n",
      "state : [1 3 5 0 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "action : 1\n",
      "new state: [1 2 5 0 1 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [1 2 5 0 1 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "action : 1\n",
      "new state: [1 3 5 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.25\n",
      "state : [1 3 5 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 1\n",
      "new state: [2 3 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "reward: 0.0\n",
      "epReward so far: 0.25\n",
      "state : [2 3 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "action : 0\n",
      "new state: [2 3 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "reward: -0.75\n",
      "epReward so far: -0.5\n",
      "state : [2 3 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "action : 2\n",
      "new state: [2 3 4 0 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "reward: 0.25\n",
      "epReward so far: -0.25\n",
      "final state: [2 3 4 0 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "Episode : 47\n",
      "state : [2 3 4 0 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "action : 2\n",
      "new state: [2 3 3 0 3 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 3]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [2 3 3 0 3 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 3]\n",
      "action : 2\n",
      "new state: [2 3 2 1 3 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.5\n",
      "state : [2 3 2 1 3 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 1]\n",
      "action : 0\n",
      "new state: [1 4 2 1 3 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 3]\n",
      "reward: 0.25\n",
      "epReward so far: 0.75\n",
      "state : [1 4 2 1 3 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 3]\n",
      "action : 0\n",
      "new state: [0 4 2 2 3 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0]\n",
      "reward: 0.25\n",
      "epReward so far: 1.0\n",
      "state : [0 4 2 2 3 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0]\n",
      "action : 1\n",
      "new state: [0 4 2 2 3 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1]\n",
      "reward: 0.25\n",
      "epReward so far: 1.25\n",
      "final state: [0 4 2 2 3 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1]\n",
      "Episode : 48\n",
      "state : [0 4 2 2 3 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1]\n",
      "action : 1\n",
      "new state: [0 4 2 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: 0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 4 2 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 2\n",
      "new state: [1 4 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 4 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "action : 2\n",
      "new state: [1 4 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 4 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "action : 2\n",
      "new state: [1 4 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 4 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "action : 0\n",
      "new state: [0 4 2 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "final state: [0 4 2 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "Episode : 49\n",
      "state : [0 4 2 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 1\n",
      "new state: [0 3 2 3 2 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [0 3 2 3 2 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "action : 2\n",
      "new state: [0 3 2 3 0 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "reward: 0.25\n",
      "epReward so far: -0.25\n",
      "state : [0 3 2 3 0 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "action : 1\n",
      "new state: [0 3 3 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "reward: -0.0\n",
      "epReward so far: -0.25\n",
      "state : [0 3 3 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "action : 2\n",
      "new state: [1 3 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "reward: -0.0\n",
      "epReward so far: -0.25\n",
      "state : [1 3 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "action : 3\n",
      "new state: [1 3 3 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.0\n",
      "final state: [1 3 3 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "Episode : 50\n",
      "state : [1 3 3 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "action : 0\n",
      "new state: [1 3 3 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: 0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 3 3 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 0\n",
      "new state: [1 3 3 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [1 3 3 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 2\n",
      "new state: [1 3 2 2 2 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.5\n",
      "state : [1 3 2 2 2 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "action : 0\n",
      "new state: [1 3 3 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "reward: 0.0\n",
      "epReward so far: 0.5\n",
      "state : [1 3 3 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "action : 0\n",
      "new state: [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "reward: 0.0\n",
      "epReward so far: 0.5\n",
      "final state: [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "Episode : 51\n",
      "state : [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "action : 1\n",
      "new state: [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "reward: 0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "action : 1\n",
      "new state: [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "action : 0\n",
      "new state: [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "action : 2\n",
      "new state: [2 3 2 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [2 3 2 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 3\n",
      "new state: [2 3 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.25\n",
      "final state: [2 3 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "Episode : 52\n",
      "state : [2 3 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "action : 1\n",
      "new state: [2 2 2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [2 2 2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "action : 0\n",
      "new state: [2 2 2 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.25\n",
      "state : [2 2 2 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "action : 0\n",
      "new state: [2 2 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "reward: 0.0\n",
      "epReward so far: 0.25\n",
      "state : [2 2 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "action : 1\n",
      "new state: [2 1 2 4 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.5\n",
      "state : [2 1 2 4 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "action : 2\n",
      "new state: [2 1 1 4 2 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.75\n",
      "final state: [2 1 1 4 2 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1]\n",
      "Episode : 53\n",
      "state : [2 1 1 4 2 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1]\n",
      "action : 1\n",
      "new state: [2 1 2 4 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: 0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 1 2 4 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "reward: 0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 2 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "action : 1\n",
      "new state: [2 1 2 4 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [2 1 2 4 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "action : 0\n",
      "new state: [1 1 2 4 0 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.5\n",
      "state : [1 1 2 4 0 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2]\n",
      "action : 2\n",
      "new state: [2 1 2 4 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "reward: 0.0\n",
      "epReward so far: 0.5\n",
      "final state: [2 1 2 4 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "Episode : 54\n",
      "state : [2 1 2 4 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "action : 2\n",
      "new state: [2 2 1 4 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [2 2 1 4 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 3\n",
      "new state: [2 2 1 3 1 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.5\n",
      "state : [2 2 1 3 1 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 0]\n",
      "action : 3\n",
      "new state: [2 3 1 2 0 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.75\n",
      "state : [2 3 1 2 0 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "action : 0\n",
      "new state: [2 4 1 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.75\n",
      "state : [2 4 1 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "action : 3\n",
      "new state: [3 4 1 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: 0.25\n",
      "epReward so far: 1.0\n",
      "final state: [3 4 1 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "Episode : 55\n",
      "state : [3 4 1 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 3\n",
      "new state: [3 4 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 4 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 2\n",
      "new state: [4 4 0 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [4 4 0 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 0\n",
      "new state: [3 4 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "reward: -0.75\n",
      "epReward so far: -0.5\n",
      "state : [3 4 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "action : 0\n",
      "new state: [4 4 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "reward: 0.0\n",
      "epReward so far: -0.5\n",
      "state : [4 4 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "action : 2\n",
      "new state: [4 4 0 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "reward: 0.25\n",
      "epReward so far: -0.25\n",
      "final state: [4 4 0 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "Episode : 56\n",
      "state : [4 4 0 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "action : 1\n",
      "new state: [4 3 0 1 3 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 3]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [4 3 0 1 3 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 3]\n",
      "action : 0\n",
      "new state: [3 3 0 2 3 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "reward: -0.5\n",
      "epReward so far: -0.25\n",
      "state : [3 3 0 2 3 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "action : 0\n",
      "new state: [3 3 0 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: 0.0\n",
      "epReward so far: -0.25\n",
      "state : [3 3 0 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 3\n",
      "new state: [3 3 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "reward: -0.0\n",
      "epReward so far: -0.25\n",
      "state : [3 3 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "action : 1\n",
      "new state: [3 2 0 4 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.0\n",
      "final state: [3 2 0 4 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "Episode : 57\n",
      "state : [3 2 0 4 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 0\n",
      "new state: [3 2 0 4 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 2 0 4 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "action : 0\n",
      "new state: [2 2 1 4 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [2 2 1 4 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "action : 3\n",
      "new state: [2 2 1 3 1 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.5\n",
      "state : [2 2 1 3 1 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "action : 3\n",
      "new state: [2 3 1 2 1 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.75\n",
      "state : [2 3 1 2 1 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "action : 1\n",
      "new state: [3 2 1 2 1 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "reward: 0.25\n",
      "epReward so far: 1.0\n",
      "final state: [3 2 1 2 1 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "Episode : 58\n",
      "state : [3 2 1 2 1 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "action : 0\n",
      "new state: [2 3 1 2 2 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [2 3 1 2 2 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1]\n",
      "action : 1\n",
      "new state: [2 3 2 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: 0.0\n",
      "epReward so far: 0.25\n",
      "state : [2 3 2 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 2\n",
      "new state: [2 3 2 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.5\n",
      "state : [2 3 2 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "action : 2\n",
      "new state: [2 3 2 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.5\n",
      "state : [2 3 2 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "action : 2\n",
      "new state: [3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.5\n",
      "final state: [3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "Episode : 59\n",
      "state : [3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "action : 1\n",
      "new state: [3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "reward: 0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "action : 1\n",
      "new state: [3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "reward: 0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "action : 2\n",
      "new state: [3 3 1 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [3 3 1 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "action : 1\n",
      "new state: [3 3 1 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.25\n",
      "state : [3 3 1 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "action : 1\n",
      "new state: [3 3 1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.25\n",
      "final state: [3 3 1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "Episode : 60\n",
      "state : [3 3 1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "action : 1\n",
      "new state: [3 3 1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "reward: 0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 3 1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "action : 3\n",
      "new state: [3 3 1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "reward: 0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 3 1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "action : 1\n",
      "new state: [3 2 1 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [3 2 1 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "action : 0\n",
      "new state: [3 2 1 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "reward: 0.0\n",
      "epReward so far: 0.25\n",
      "state : [3 2 1 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "action : 1\n",
      "new state: [3 1 2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: 0.25\n",
      "epReward so far: 0.5\n",
      "final state: [3 1 2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "Episode : 61\n",
      "state : [3 1 2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 2\n",
      "new state: [3 1 2 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 1 2 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "action : 1\n",
      "new state: [3 0 2 4 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [3 0 2 4 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 3\n",
      "new state: [3 0 2 3 2 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 3]\n",
      "reward: 0.25\n",
      "epReward so far: 0.5\n",
      "state : [3 0 2 3 2 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 3]\n",
      "action : 0\n",
      "new state: [2 0 3 3 3 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.75\n",
      "state : [2 0 3 3 3 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "action : 0\n",
      "new state: [1 0 4 3 3 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 3]\n",
      "reward: -0.5\n",
      "epReward so far: 0.25\n",
      "final state: [1 0 4 3 3 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 3]\n",
      "Episode : 62\n",
      "state : [1 0 4 3 3 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 3]\n",
      "action : 3\n",
      "new state: [1 0 4 4 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 0 4 4 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 0\n",
      "new state: [0 0 5 4 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [0 0 5 4 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "action : 2\n",
      "new state: [0 0 5 4 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.5\n",
      "state : [0 0 5 4 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "action : 2\n",
      "new state: [0 0 5 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [0 0 5 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "action : 3\n",
      "new state: [0 0 5 4 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: 0.25\n",
      "epReward so far: -1.0\n",
      "final state: [0 0 5 4 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "Episode : 63\n",
      "state : [0 0 5 4 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 3\n",
      "new state: [0 0 5 3 2 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [0 0 5 3 2 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 0]\n",
      "action : 3\n",
      "new state: [0 0 6 2 0 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.5\n",
      "state : [0 0 6 2 0 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 2]\n",
      "action : 3\n",
      "new state: [0 1 6 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.5\n",
      "state : [0 1 6 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 3\n",
      "new state: [1 1 6 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.75\n",
      "state : [1 1 6 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "action : 1\n",
      "new state: [1 0 6 1 1 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "reward: 0.25\n",
      "epReward so far: 1.0\n",
      "final state: [1 0 6 1 1 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "Episode : 64\n",
      "state : [1 0 6 1 1 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "action : 0\n",
      "new state: [0 1 6 1 2 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 3]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [0 1 6 1 2 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 3]\n",
      "action : 2\n",
      "new state: [0 1 6 1 2 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0]\n",
      "reward: 0.25\n",
      "epReward so far: -0.25\n",
      "state : [0 1 6 1 2 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0]\n",
      "action : 1\n",
      "new state: [0 0 7 1 0 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 3]\n",
      "reward: 0.25\n",
      "epReward so far: 0.0\n",
      "state : [0 0 7 1 0 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 3]\n",
      "action : 2\n",
      "new state: [0 0 7 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [0 0 7 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 2\n",
      "new state: [1 0 6 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: -0.5\n",
      "epReward so far: -1.25\n",
      "final state: [1 0 6 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "Episode : 65\n",
      "state : [1 0 6 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 2\n",
      "new state: [1 0 6 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: 0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 0 6 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 2\n",
      "new state: [1 0 6 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "reward: 0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 0 6 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "action : 0\n",
      "new state: [1 0 6 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 0 6 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "action : 2\n",
      "new state: [1 0 5 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [1 0 5 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 3\n",
      "new state: [1 0 5 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: 0.0\n",
      "epReward so far: 0.25\n",
      "final state: [1 0 5 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "Episode : 66\n",
      "state : [1 0 5 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 3\n",
      "new state: [1 1 5 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [1 1 5 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 1\n",
      "new state: [1 1 5 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "reward: 0.0\n",
      "epReward so far: 0.25\n",
      "state : [1 1 5 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "action : 0\n",
      "new state: [1 2 5 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.25\n",
      "state : [1 2 5 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "action : 0\n",
      "new state: [0 2 5 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.5\n",
      "state : [0 2 5 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 1\n",
      "new state: [0 1 5 2 3 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "reward: -0.5\n",
      "epReward so far: 0.0\n",
      "final state: [0 1 5 2 3 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "Episode : 67\n",
      "state : [0 1 5 2 3 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "action : 1\n",
      "new state: [0 1 5 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [0 1 5 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 1\n",
      "new state: [0 1 6 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "reward: 0.0\n",
      "epReward so far: -0.75\n",
      "state : [0 1 6 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "action : 3\n",
      "new state: [0 1 6 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: 0.25\n",
      "epReward so far: -0.5\n",
      "state : [0 1 6 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 2\n",
      "new state: [0 1 6 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: 0.0\n",
      "epReward so far: -0.5\n",
      "state : [0 1 6 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 2\n",
      "new state: [0 2 6 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "reward: -0.0\n",
      "epReward so far: -0.5\n",
      "final state: [0 2 6 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "Episode : 68\n",
      "state : [0 2 6 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "action : 1\n",
      "new state: [0 2 6 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [0 2 6 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "action : 1\n",
      "new state: [0 2 6 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [0 2 6 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "action : 3\n",
      "new state: [0 2 6 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "reward: 0.25\n",
      "epReward so far: -1.25\n",
      "state : [0 2 6 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "action : 1\n",
      "new state: [0 2 6 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "state : [0 2 6 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 1\n",
      "new state: [0 2 6 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "reward: -0.5\n",
      "epReward so far: -2.5\n",
      "final state: [0 2 6 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "Episode : 69\n",
      "state : [0 2 6 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "action : 3\n",
      "new state: [0 2 6 0 3 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [0 2 6 0 3 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "action : 1\n",
      "new state: [0 2 6 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "reward: -0.75\n",
      "epReward so far: -0.5\n",
      "state : [0 2 6 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "action : 1\n",
      "new state: [1 1 6 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: 0.25\n",
      "epReward so far: -0.25\n",
      "state : [1 1 6 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 3\n",
      "new state: [1 1 6 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: -0.0\n",
      "epReward so far: -0.25\n",
      "state : [1 1 6 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 3\n",
      "new state: [2 1 6 0 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.0\n",
      "final state: [2 1 6 0 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "Episode : 70\n",
      "state : [2 1 6 0 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 1\n",
      "new state: [2 1 6 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: 0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 1 6 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 0\n",
      "new state: [1 2 6 0 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [1 2 6 0 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "action : 2\n",
      "new state: [1 2 5 0 2 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.5\n",
      "state : [1 2 5 0 2 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "action : 1\n",
      "new state: [1 1 6 0 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.75\n",
      "state : [1 1 6 0 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "action : 0\n",
      "new state: [0 2 6 0 2 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 1]\n",
      "reward: 0.25\n",
      "epReward so far: 1.0\n",
      "final state: [0 2 6 0 2 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 1]\n",
      "Episode : 71\n",
      "state : [0 2 6 0 2 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 1]\n",
      "action : 2\n",
      "new state: [0 2 6 0 1 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [0 2 6 0 1 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 0]\n",
      "action : 1\n",
      "new state: [0 1 7 0 1 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "reward: -0.5\n",
      "epReward so far: -0.25\n",
      "state : [0 1 7 0 1 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "action : 1\n",
      "new state: [0 1 7 0 2 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.0\n",
      "state : [0 1 7 0 2 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 0]\n",
      "action : 1\n",
      "new state: [1 0 7 0 2 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [1 0 7 0 2 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "action : 0\n",
      "new state: [1 0 8 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: 0.0\n",
      "epReward so far: -0.5\n",
      "final state: [1 0 8 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "Episode : 72\n",
      "state : [1 0 8 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 0\n",
      "new state: [2 0 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 0 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "action : 0\n",
      "new state: [1 0 8 0 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [1 0 8 0 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 2\n",
      "new state: [1 0 8 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "reward: 0.0\n",
      "epReward so far: -0.5\n",
      "state : [1 0 8 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "action : 0\n",
      "new state: [1 1 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "reward: 0.0\n",
      "epReward so far: -0.5\n",
      "state : [1 1 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "action : 2\n",
      "new state: [1 1 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "reward: -0.0\n",
      "epReward so far: -0.5\n",
      "final state: [1 1 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "Episode : 73\n",
      "state : [1 1 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "action : 0\n",
      "new state: [1 1 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [1 1 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "action : 1\n",
      "new state: [1 1 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "reward: -0.0\n",
      "epReward so far: -0.75\n",
      "state : [1 1 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "action : 2\n",
      "new state: [1 1 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "reward: -0.0\n",
      "epReward so far: -0.75\n",
      "state : [1 1 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "action : 1\n",
      "new state: [1 1 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: -0.75\n",
      "state : [1 1 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "action : 2\n",
      "new state: [1 1 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "reward: 0.0\n",
      "epReward so far: -0.75\n",
      "final state: [1 1 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "Episode : 74\n",
      "state : [1 1 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "action : 0\n",
      "new state: [0 1 8 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [0 1 8 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "action : 2\n",
      "new state: [0 1 8 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "reward: -0.0\n",
      "epReward so far: -0.5\n",
      "state : [0 1 8 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "action : 1\n",
      "new state: [1 0 8 0 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "reward: -0.5\n",
      "epReward so far: -1.0\n",
      "state : [1 0 8 0 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "action : 0\n",
      "new state: [0 0 8 0 1 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0]\n",
      "reward: -0.5\n",
      "epReward so far: -1.5\n",
      "state : [0 0 8 0 1 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0]\n",
      "action : 2\n",
      "new state: [0 1 7 0 0 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "reward: -0.5\n",
      "epReward so far: -2.0\n",
      "final state: [0 1 7 0 0 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "Episode : 75\n",
      "state : [0 1 7 0 0 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "action : 1\n",
      "new state: [1 1 7 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [1 1 7 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 0\n",
      "new state: [2 1 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [2 1 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "action : 1\n",
      "new state: [2 1 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "reward: 0.0\n",
      "epReward so far: -1.5\n",
      "state : [2 1 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "action : 2\n",
      "new state: [2 1 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "reward: 0.0\n",
      "epReward so far: -1.5\n",
      "state : [2 1 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "action : 0\n",
      "new state: [1 1 7 0 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: 0.25\n",
      "epReward so far: -1.25\n",
      "final state: [1 1 7 0 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "Episode : 76\n",
      "state : [1 1 7 0 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 2\n",
      "new state: [1 1 7 0 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: 0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 1 7 0 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 0\n",
      "new state: [1 1 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [1 1 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "action : 1\n",
      "new state: [1 0 7 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "reward: 0.25\n",
      "epReward so far: -0.5\n",
      "state : [1 0 7 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "action : 0\n",
      "new state: [1 0 7 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: 0.0\n",
      "epReward so far: -0.5\n",
      "state : [1 0 7 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 3\n",
      "new state: [2 0 7 0 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: 0.25\n",
      "epReward so far: -0.25\n",
      "final state: [2 0 7 0 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "Episode : 77\n",
      "state : [2 0 7 0 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 0\n",
      "new state: [1 0 7 0 2 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [1 0 7 0 2 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "action : 0\n",
      "new state: [0 0 8 0 1 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 0]\n",
      "reward: -0.5\n",
      "epReward so far: -0.25\n",
      "state : [0 0 8 0 1 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 0]\n",
      "action : 2\n",
      "new state: [0 0 7 1 1 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "reward: -0.5\n",
      "epReward so far: -0.75\n",
      "state : [0 0 7 1 1 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "action : 3\n",
      "new state: [0 1 7 0 1 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 3]\n",
      "reward: 0.25\n",
      "epReward so far: -0.5\n",
      "state : [0 1 7 0 1 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 3]\n",
      "action : 1\n",
      "new state: [1 1 7 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "final state: [1 1 7 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "Episode : 78\n",
      "state : [1 1 7 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 0\n",
      "new state: [0 2 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [0 2 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "action : 3\n",
      "new state: [0 2 7 0 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "reward: 0.25\n",
      "epReward so far: -0.5\n",
      "state : [0 2 7 0 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "action : 1\n",
      "new state: [0 2 7 0 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "reward: -0.0\n",
      "epReward so far: -0.5\n",
      "state : [0 2 7 0 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "action : 1\n",
      "new state: [0 1 8 0 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "reward: -0.5\n",
      "epReward so far: -1.0\n",
      "state : [0 1 8 0 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "action : 1\n",
      "new state: [0 0 8 0 1 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "reward: 0.25\n",
      "epReward so far: -0.75\n",
      "final state: [0 0 8 0 1 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "Episode : 79\n",
      "state : [0 0 8 0 1 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "action : 2\n",
      "new state: [0 1 7 0 1 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [0 1 7 0 1 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0]\n",
      "action : 1\n",
      "new state: [0 1 8 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "reward: -0.0\n",
      "epReward so far: -0.5\n",
      "state : [0 1 8 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "action : 1\n",
      "new state: [0 1 8 0 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: -0.5\n",
      "epReward so far: -1.0\n",
      "state : [0 1 8 0 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 1\n",
      "new state: [0 0 8 0 1 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 1]\n",
      "reward: -0.5\n",
      "epReward so far: -1.5\n",
      "state : [0 0 8 0 1 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 1]\n",
      "action : 2\n",
      "new state: [0 1 7 0 1 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 3]\n",
      "reward: -0.5\n",
      "epReward so far: -2.0\n",
      "final state: [0 1 7 0 1 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 3]\n",
      "Episode : 80\n",
      "state : [0 1 7 0 1 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 3]\n",
      "action : 1\n",
      "new state: [0 1 7 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 1 7 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 3\n",
      "new state: [0 2 7 0 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [0 2 7 0 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 1\n",
      "new state: [0 1 7 0 2 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 3]\n",
      "reward: -0.5\n",
      "epReward so far: -0.25\n",
      "state : [0 1 7 0 2 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 3]\n",
      "action : 1\n",
      "new state: [0 1 8 0 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.0\n",
      "state : [0 1 8 0 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 1\n",
      "new state: [0 1 9 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.75\n",
      "final state: [0 1 9 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "Episode : 81\n",
      "state : [0 1 9 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "action : 2\n",
      "new state: [0 1 9 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 1 9 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "action : 1\n",
      "new state: [0 1 9 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [0 1 9 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "action : 2\n",
      "new state: [0 1 8 0 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: 0.25\n",
      "epReward so far: -0.5\n",
      "state : [0 1 8 0 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 2\n",
      "new state: [0 1 8 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "reward: 0.0\n",
      "epReward so far: -0.5\n",
      "state : [0 1 8 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "action : 1\n",
      "new state: [0 1 8 0 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: 0.25\n",
      "epReward so far: -0.25\n",
      "final state: [0 1 8 0 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "Episode : 82\n",
      "state : [0 1 8 0 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 1\n",
      "new state: [0 1 8 0 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [0 1 8 0 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 1\n",
      "new state: [0 0 8 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: -0.5\n",
      "epReward so far: -1.25\n",
      "state : [0 0 8 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 2\n",
      "new state: [0 0 7 1 1 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "reward: -0.5\n",
      "epReward so far: -1.75\n",
      "state : [0 0 7 1 1 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "action : 3\n",
      "new state: [0 1 7 0 1 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "reward: 0.25\n",
      "epReward so far: -1.5\n",
      "state : [0 1 7 0 1 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "action : 1\n",
      "new state: [0 1 8 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: -2.25\n",
      "final state: [0 1 8 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "Episode : 83\n",
      "state : [0 1 8 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 1\n",
      "new state: [0 2 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "reward: 0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 2 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "action : 1\n",
      "new state: [0 2 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 2 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "action : 1\n",
      "new state: [0 1 8 0 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [0 1 8 0 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 1\n",
      "new state: [0 0 8 0 1 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "reward: -0.5\n",
      "epReward so far: -1.0\n",
      "state : [0 0 8 0 1 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "action : 2\n",
      "new state: [0 1 7 0 1 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "reward: -0.5\n",
      "epReward so far: -1.5\n",
      "final state: [0 1 7 0 1 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "Episode : 84\n",
      "state : [0 1 7 0 1 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "action : 1\n",
      "new state: [1 1 7 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [1 1 7 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 0\n",
      "new state: [1 2 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "reward: -0.0\n",
      "epReward so far: -0.75\n",
      "state : [1 2 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "action : 0\n",
      "new state: [0 2 7 0 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "reward: -0.5\n",
      "epReward so far: -1.25\n",
      "state : [0 2 7 0 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "action : 1\n",
      "new state: [0 2 7 0 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "state : [0 2 7 0 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 1\n",
      "new state: [0 1 8 0 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: -0.5\n",
      "epReward so far: -2.5\n",
      "final state: [0 1 8 0 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "Episode : 85\n",
      "state : [0 1 8 0 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 2\n",
      "new state: [0 1 7 0 2 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [0 1 7 0 2 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 0]\n",
      "action : 1\n",
      "new state: [0 1 8 0 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.5\n",
      "state : [0 1 8 0 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 1\n",
      "new state: [0 1 8 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "reward: -0.0\n",
      "epReward so far: -0.5\n",
      "state : [0 1 8 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "action : 1\n",
      "new state: [0 0 8 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: 0.25\n",
      "epReward so far: -0.25\n",
      "state : [0 0 8 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 2\n",
      "new state: [0 0 8 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: -0.0\n",
      "epReward so far: -0.25\n",
      "final state: [0 0 8 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "Episode : 86\n",
      "state : [0 0 8 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 3\n",
      "new state: [1 0 8 0 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [1 0 8 0 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "action : 2\n",
      "new state: [1 0 7 0 1 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.5\n",
      "state : [1 0 7 0 1 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "action : 0\n",
      "new state: [1 1 7 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: 0.0\n",
      "epReward so far: 0.5\n",
      "state : [1 1 7 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 0\n",
      "new state: [0 2 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "reward: -0.75\n",
      "epReward so far: -0.25\n",
      "state : [0 2 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "action : 1\n",
      "new state: [0 2 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.0\n",
      "final state: [0 2 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "Episode : 87\n",
      "state : [0 2 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "action : 2\n",
      "new state: [0 2 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 2 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "action : 1\n",
      "new state: [0 2 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [0 2 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "action : 3\n",
      "new state: [0 2 7 0 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "reward: 0.25\n",
      "epReward so far: -0.5\n",
      "state : [0 2 7 0 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "action : 1\n",
      "new state: [0 1 7 0 2 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "reward: 0.25\n",
      "epReward so far: -0.25\n",
      "state : [0 1 7 0 2 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "action : 1\n",
      "new state: [0 0 8 0 2 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "reward: -0.5\n",
      "epReward so far: -0.75\n",
      "final state: [0 0 8 0 2 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "Episode : 88\n",
      "state : [0 0 8 0 2 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "action : 2\n",
      "new state: [0 0 8 0 2 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 2]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [0 0 8 0 2 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 2]\n",
      "action : 2\n",
      "new state: [0 0 9 0 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [0 0 9 0 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 2\n",
      "new state: [0 0 9 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: 0.25\n",
      "epReward so far: -1.0\n",
      "state : [0 0 9 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 2\n",
      "new state: [0 0 8 0 0 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 2]\n",
      "reward: 0.25\n",
      "epReward so far: -0.75\n",
      "state : [0 0 8 0 0 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 2]\n",
      "action : 2\n",
      "new state: [1 0 7 0 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "reward: -0.5\n",
      "epReward so far: -1.25\n",
      "final state: [1 0 7 0 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "Episode : 89\n",
      "state : [1 0 7 0 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "action : 0\n",
      "new state: [1 0 7 1 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 0 7 1 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "action : 0\n",
      "new state: [1 0 8 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [1 0 8 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "action : 2\n",
      "new state: [1 0 7 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: 0.25\n",
      "epReward so far: -0.5\n",
      "state : [1 0 7 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 0\n",
      "new state: [1 0 7 1 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [1 0 7 1 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 3\n",
      "new state: [1 0 7 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: 0.25\n",
      "epReward so far: -1.0\n",
      "final state: [1 0 7 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "Episode : 90\n",
      "state : [1 0 7 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 0\n",
      "new state: [0 0 7 1 1 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 3]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [0 0 7 1 1 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 3]\n",
      "action : 2\n",
      "new state: [0 1 7 1 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.25\n",
      "state : [0 1 7 1 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "action : 1\n",
      "new state: [0 0 8 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: 0.25\n",
      "epReward so far: 0.5\n",
      "state : [0 0 8 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 3\n",
      "new state: [0 0 8 1 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "reward: 0.0\n",
      "epReward so far: 0.5\n",
      "state : [0 0 8 1 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "action : 2\n",
      "new state: [0 0 9 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.25\n",
      "final state: [0 0 9 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "Episode : 91\n",
      "state : [0 0 9 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "action : 2\n",
      "new state: [0 0 9 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [0 0 9 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "action : 2\n",
      "new state: [0 0 8 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: 0.25\n",
      "epReward so far: -0.5\n",
      "state : [0 0 8 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 2\n",
      "new state: [0 0 8 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [0 0 8 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 3\n",
      "new state: [0 1 8 0 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: 0.25\n",
      "epReward so far: -1.0\n",
      "state : [0 1 8 0 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 1\n",
      "new state: [0 1 8 0 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.75\n",
      "final state: [0 1 8 0 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "Episode : 92\n",
      "state : [0 1 8 0 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 1\n",
      "new state: [0 0 9 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [0 0 9 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "action : 2\n",
      "new state: [0 0 8 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "reward: 0.25\n",
      "epReward so far: -0.5\n",
      "state : [0 0 8 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "action : 2\n",
      "new state: [0 0 7 1 1 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "reward: -0.5\n",
      "epReward so far: -1.0\n",
      "state : [0 0 7 1 1 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "action : 2\n",
      "new state: [0 1 6 1 2 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 1]\n",
      "reward: -0.5\n",
      "epReward so far: -1.5\n",
      "state : [0 1 6 1 2 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 1]\n",
      "action : 2\n",
      "new state: [0 1 7 1 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: -0.0\n",
      "epReward so far: -1.5\n",
      "final state: [0 1 7 1 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "Episode : 93\n",
      "state : [0 1 7 1 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 3\n",
      "new state: [0 1 8 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 1 8 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "action : 1\n",
      "new state: [0 0 8 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [0 0 8 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "action : 2\n",
      "new state: [1 0 7 1 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.5\n",
      "state : [1 0 7 1 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 3\n",
      "new state: [1 0 8 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "reward: -0.0\n",
      "epReward so far: -0.5\n",
      "state : [1 0 8 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "action : 0\n",
      "new state: [0 1 8 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "final state: [0 1 8 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "Episode : 94\n",
      "state : [0 1 8 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "action : 2\n",
      "new state: [0 1 8 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "reward: 0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 1 8 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "action : 1\n",
      "new state: [0 1 8 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [0 1 8 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "action : 1\n",
      "new state: [0 0 8 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "reward: -0.5\n",
      "epReward so far: -1.25\n",
      "state : [0 0 8 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "action : 2\n",
      "new state: [0 0 7 1 1 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "reward: 0.25\n",
      "epReward so far: -1.0\n",
      "state : [0 0 7 1 1 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "action : 2\n",
      "new state: [0 1 6 1 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "reward: -0.5\n",
      "epReward so far: -1.5\n",
      "final state: [0 1 6 1 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "Episode : 95\n",
      "state : [0 1 6 1 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "action : 2\n",
      "new state: [0 2 5 1 2 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [0 2 5 1 2 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 0]\n",
      "action : 3\n",
      "new state: [0 2 6 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.25\n",
      "state : [0 2 6 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "action : 1\n",
      "new state: [1 1 6 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.5\n",
      "state : [1 1 6 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "action : 0\n",
      "new state: [1 1 6 1 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "reward: 0.0\n",
      "epReward so far: 0.5\n",
      "state : [1 1 6 1 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "action : 1\n",
      "new state: [1 0 6 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: 0.25\n",
      "epReward so far: 0.75\n",
      "final state: [1 0 6 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "Episode : 96\n",
      "state : [1 0 6 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 2\n",
      "new state: [1 0 6 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 0 6 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 3\n",
      "new state: [1 0 6 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 0 6 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "action : 0\n",
      "new state: [0 0 6 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [0 0 6 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 3\n",
      "new state: [0 0 6 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "reward: 0.0\n",
      "epReward so far: 0.25\n",
      "state : [0 0 6 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "action : 2\n",
      "new state: [0 0 6 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "reward: -0.75\n",
      "epReward so far: -0.5\n",
      "final state: [0 0 6 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "Episode : 97\n",
      "state : [0 0 6 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "action : 2\n",
      "new state: [0 0 6 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [0 0 6 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "action : 2\n",
      "new state: [0 0 6 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [0 0 6 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "action : 2\n",
      "new state: [0 0 6 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "reward: -0.75\n",
      "epReward so far: -2.25\n",
      "state : [0 0 6 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "action : 3\n",
      "new state: [0 0 6 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "reward: 0.0\n",
      "epReward so far: -2.25\n",
      "state : [0 0 6 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "action : 3\n",
      "new state: [0 0 6 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "reward: -0.0\n",
      "epReward so far: -2.25\n",
      "final state: [0 0 6 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "Episode : 98\n",
      "state : [0 0 6 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "action : 3\n",
      "new state: [0 0 6 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [0 0 6 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "action : 2\n",
      "new state: [0 0 6 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.25\n",
      "state : [0 0 6 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "action : 2\n",
      "new state: [0 0 7 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.5\n",
      "state : [0 0 7 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "action : 3\n",
      "new state: [0 0 7 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "reward: -0.0\n",
      "epReward so far: -0.5\n",
      "state : [0 0 7 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "action : 2\n",
      "new state: [0 0 7 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "final state: [0 0 7 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "Episode : 99\n",
      "state : [0 0 7 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "action : 2\n",
      "new state: [0 0 7 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 0 7 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "action : 2\n",
      "new state: [0 0 6 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [0 0 6 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 2\n",
      "new state: [0 0 6 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: -0.5\n",
      "state : [0 0 6 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 2\n",
      "new state: [0 0 7 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [0 0 7 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "action : 2\n",
      "new state: [0 0 7 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "final state: [0 0 7 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "**************************************************\n",
      "Experiment complete\n",
      "**************************************************\n",
      "**************************************************\n",
      "Saving data\n",
      "**************************************************\n",
      "[[ 0.00000000e+00  0.00000000e+00  7.50000000e-01  1.74450000e+04\n",
      "  -4.52813151e+00]\n",
      " [ 1.00000000e+00  0.00000000e+00  0.00000000e+00  1.60950000e+04\n",
      "  -4.98072543e+00]\n",
      " [ 2.00000000e+00  0.00000000e+00  2.50000000e-01  1.44810000e+04\n",
      "  -5.24921372e+00]\n",
      " [ 3.00000000e+00  0.00000000e+00  7.50000000e-01  6.87900000e+03\n",
      "  -5.36000149e+00]\n",
      " [ 4.00000000e+00  0.00000000e+00  0.00000000e+00  1.55640000e+04\n",
      "  -5.18605798e+00]\n",
      " [ 5.00000000e+00  0.00000000e+00  7.50000000e-01  1.40840000e+04\n",
      "  -5.22455100e+00]\n",
      " [ 6.00000000e+00  0.00000000e+00  1.00000000e+00  6.63900000e+03\n",
      "  -5.03918526e+00]\n",
      " [ 7.00000000e+00  0.00000000e+00  5.00000000e-01  1.56300000e+04\n",
      "  -5.08446366e+00]\n",
      " [ 8.00000000e+00  0.00000000e+00  7.50000000e-01  8.20700000e+03\n",
      "  -5.04490536e+00]\n",
      " [ 9.00000000e+00  0.00000000e+00  0.00000000e+00  1.78050000e+04\n",
      "  -4.85557658e+00]\n",
      " [ 1.00000000e+01  0.00000000e+00 -2.00000000e+00  9.85200000e+03\n",
      "  -5.13484141e+00]\n",
      " [ 1.10000000e+01  0.00000000e+00 -2.50000000e-01  1.41870000e+04\n",
      "  -5.17858480e+00]\n",
      " [ 1.20000000e+01  0.00000000e+00  5.00000000e-01  1.83430000e+04\n",
      "  -4.81960278e+00]\n",
      " [ 1.30000000e+01  0.00000000e+00  2.50000000e-01  5.87650000e+04\n",
      "  -5.11551210e+00]\n",
      " [ 1.40000000e+01  0.00000000e+00  2.50000000e-01  1.38010000e+04\n",
      "  -5.46177875e+00]\n",
      " [ 1.50000000e+01  0.00000000e+00 -7.50000000e-01  1.31000000e+04\n",
      "  -5.48271627e+00]\n",
      " [ 1.60000000e+01  0.00000000e+00  2.50000000e-01  7.84100000e+03\n",
      "  -4.90042378e+00]\n",
      " [ 1.70000000e+01  0.00000000e+00  5.00000000e-01  1.00350000e+04\n",
      "  -5.15893896e+00]\n",
      " [ 1.80000000e+01  0.00000000e+00  5.00000000e-01  9.58700000e+03\n",
      "  -5.18478019e+00]\n",
      " [ 1.90000000e+01  0.00000000e+00 -5.00000000e-01  1.17610000e+04\n",
      "  -2.52391925e+00]\n",
      " [ 2.00000000e+01  0.00000000e+00 -5.00000000e-01  1.22570000e+04\n",
      "  -4.76487648e+00]\n",
      " [ 2.10000000e+01  0.00000000e+00 -1.00000000e+00  1.52460000e+04\n",
      "  -5.14387136e+00]\n",
      " [ 2.20000000e+01  0.00000000e+00  5.00000000e-01  8.73400000e+03\n",
      "  -5.14032287e+00]\n",
      " [ 2.30000000e+01  0.00000000e+00 -1.00000000e+00  1.29190000e+04\n",
      "  -4.71052371e+00]\n",
      " [ 2.40000000e+01  0.00000000e+00 -2.00000000e+00  1.56360000e+04\n",
      "  -5.10421700e+00]\n",
      " [ 2.50000000e+01  0.00000000e+00  1.00000000e+00  1.36860000e+04\n",
      "  -5.21599490e+00]\n",
      " [ 2.60000000e+01  0.00000000e+00  5.00000000e-01  7.86200000e+03\n",
      "  -4.96529547e+00]\n",
      " [ 2.70000000e+01  0.00000000e+00  0.00000000e+00  1.46910000e+04\n",
      "  -4.53153671e+00]\n",
      " [ 2.80000000e+01  0.00000000e+00  0.00000000e+00  1.00280000e+04\n",
      "  -5.00999272e+00]\n",
      " [ 2.90000000e+01  0.00000000e+00  0.00000000e+00  1.91620000e+04\n",
      "  -5.23896093e+00]\n",
      " [ 3.00000000e+01  0.00000000e+00 -2.25000000e+00  1.02050000e+04\n",
      "  -5.14011932e+00]\n",
      " [ 3.10000000e+01  0.00000000e+00  2.50000000e-01  1.76940000e+04\n",
      "  -5.08307851e+00]\n",
      " [ 3.20000000e+01  0.00000000e+00  0.00000000e+00  4.67380000e+04\n",
      "  -4.82928075e+00]\n",
      " [ 3.30000000e+01  0.00000000e+00  5.00000000e-01  1.45270000e+04\n",
      "  -5.40496978e+00]\n",
      " [ 3.40000000e+01  0.00000000e+00  2.50000000e-01  1.47020000e+04\n",
      "  -5.56129789e+00]\n",
      " [ 3.50000000e+01  0.00000000e+00  0.00000000e+00  1.30910000e+04\n",
      "  -5.41124940e+00]\n",
      " [ 3.60000000e+01  0.00000000e+00  0.00000000e+00  1.30910000e+04\n",
      "  -5.53352683e+00]\n",
      " [ 3.70000000e+01  0.00000000e+00  0.00000000e+00  8.50700000e+03\n",
      "  -5.08046734e+00]\n",
      " [ 3.80000000e+01  0.00000000e+00  2.50000000e-01  1.19100000e+04\n",
      "  -4.81083724e+00]\n",
      " [ 3.90000000e+01  0.00000000e+00  5.00000000e-01  1.23150000e+04\n",
      "  -4.88447225e+00]\n",
      " [ 4.00000000e+01  0.00000000e+00  5.00000000e-01  1.44210000e+04\n",
      "  -5.03192557e+00]\n",
      " [ 4.10000000e+01  0.00000000e+00  2.50000000e-01  1.45860000e+04\n",
      "  -4.89057494e+00]\n",
      " [ 4.20000000e+01  0.00000000e+00 -7.50000000e-01  1.23560000e+04\n",
      "  -4.95997795e+00]\n",
      " [ 4.30000000e+01  0.00000000e+00 -7.50000000e-01  8.05100000e+03\n",
      "  -4.98671327e+00]\n",
      " [ 4.40000000e+01  0.00000000e+00  0.00000000e+00  1.66830000e+04\n",
      "  -4.89298833e+00]\n",
      " [ 4.50000000e+01  0.00000000e+00 -2.50000000e-01  1.42230000e+04\n",
      "  -4.89614084e+00]\n",
      " [ 4.60000000e+01  0.00000000e+00  2.50000000e-01  1.33980000e+04\n",
      "  -5.13703039e+00]\n",
      " [ 4.70000000e+01  0.00000000e+00  7.50000000e-01  1.50400000e+04\n",
      "  -5.19590901e+00]\n",
      " [ 4.80000000e+01  0.00000000e+00  5.00000000e-01  6.15700000e+03\n",
      "  -5.29496225e+00]\n",
      " [ 4.90000000e+01  0.00000000e+00  7.50000000e-01  2.44320000e+04\n",
      "  -5.09588704e+00]\n",
      " [ 5.00000000e+01  0.00000000e+00  2.50000000e-01  8.22100000e+03\n",
      "  -4.74042403e+00]\n",
      " [ 5.10000000e+01  0.00000000e+00  1.25000000e+00  1.78720000e+04\n",
      "  -5.04386963e+00]\n",
      " [ 5.20000000e+01  0.00000000e+00 -1.00000000e+00  1.03900000e+04\n",
      "  -4.62486648e+00]\n",
      " [ 5.30000000e+01  0.00000000e+00  2.50000000e-01  2.06530000e+04\n",
      "  -4.89247976e+00]\n",
      " [ 5.40000000e+01  0.00000000e+00  5.00000000e-01  7.62000000e+03\n",
      "  -4.69906611e+00]\n",
      " [ 5.50000000e+01  0.00000000e+00  2.50000000e-01  8.91300000e+03\n",
      "  -4.98479465e+00]\n",
      " [ 5.60000000e+01  0.00000000e+00 -2.50000000e-01  1.04950000e+04\n",
      "  -3.02176594e+00]\n",
      " [ 5.70000000e+01  0.00000000e+00 -5.00000000e-01  1.02820000e+04\n",
      "  -4.53722309e+00]\n",
      " [ 5.80000000e+01  0.00000000e+00  2.50000000e-01  1.06050000e+04\n",
      "  -4.52385828e+00]\n",
      " [ 5.90000000e+01  0.00000000e+00 -1.25000000e+00  1.11310000e+04\n",
      "  -4.60624460e+00]\n",
      " [ 6.00000000e+01  0.00000000e+00 -5.00000000e-01  1.11540000e+04\n",
      "  -4.95753321e+00]\n",
      " [ 6.10000000e+01  0.00000000e+00 -2.00000000e+00  1.46780000e+04\n",
      "  -5.08254036e+00]\n",
      " [ 6.20000000e+01  0.00000000e+00 -1.00000000e+00  1.07820000e+04\n",
      "  -5.14677662e+00]\n",
      " [ 6.30000000e+01  0.00000000e+00 -5.00000000e-01  1.22660000e+04\n",
      "  -4.81280133e+00]\n",
      " [ 6.40000000e+01  0.00000000e+00 -1.00000000e+00  1.27800000e+04\n",
      "  -5.07259840e+00]\n",
      " [ 6.50000000e+01  0.00000000e+00  5.00000000e-01  1.33520000e+04\n",
      "  -4.64151494e+00]\n",
      " [ 6.60000000e+01  0.00000000e+00  7.50000000e-01  8.39900000e+03\n",
      "  -5.17482691e+00]\n",
      " [ 6.70000000e+01  0.00000000e+00  7.50000000e-01  1.38390000e+04\n",
      "  -5.07100162e+00]\n",
      " [ 6.80000000e+01  0.00000000e+00 -2.50000000e-01  1.53240000e+04\n",
      "  -5.06241528e+00]\n",
      " [ 6.90000000e+01  0.00000000e+00 -1.00000000e+00  8.58200000e+03\n",
      "  -4.95844928e+00]\n",
      " [ 7.00000000e+01  0.00000000e+00 -5.00000000e-01  1.86340000e+04\n",
      "  -4.75485747e+00]\n",
      " [ 7.10000000e+01  0.00000000e+00  2.50000000e-01  9.26300000e+03\n",
      "  -5.06090988e+00]\n",
      " [ 7.20000000e+01  0.00000000e+00 -2.50000000e-01  1.63230000e+04\n",
      "  -4.89901542e+00]\n",
      " [ 7.30000000e+01  0.00000000e+00  2.50000000e-01  8.39900000e+03\n",
      "  -4.90920663e+00]\n",
      " [ 7.40000000e+01  0.00000000e+00 -2.50000000e-01  1.75380000e+04\n",
      "  -5.15166500e+00]\n",
      " [ 7.50000000e+01  0.00000000e+00  7.50000000e-01  6.97500000e+03\n",
      "  -5.17204925e+00]\n",
      " [ 7.60000000e+01  0.00000000e+00  5.00000000e-01  9.42200000e+03\n",
      "  -5.00849278e+00]\n",
      " [ 7.70000000e+01  0.00000000e+00 -5.00000000e-01  7.12600000e+03\n",
      "  -5.39038174e+00]\n",
      " [ 7.80000000e+01  0.00000000e+00 -1.25000000e+00  6.83100000e+03\n",
      "  -5.25719024e+00]\n",
      " [ 7.90000000e+01  0.00000000e+00 -7.50000000e-01  9.32500000e+03\n",
      "  -4.88873705e+00]\n",
      " [ 8.00000000e+01  0.00000000e+00 -5.00000000e-01  1.09270000e+04\n",
      "  -4.95644857e+00]\n",
      " [ 8.10000000e+01  0.00000000e+00  0.00000000e+00  1.67930000e+04\n",
      "  -4.87555931e+00]\n",
      " [ 8.20000000e+01  0.00000000e+00 -5.00000000e-01  1.31010000e+04\n",
      "  -4.87200380e+00]\n",
      " [ 8.30000000e+01  0.00000000e+00 -7.50000000e-01  1.31010000e+04\n",
      "  -4.81871686e+00]\n",
      " [ 8.40000000e+01  0.00000000e+00 -5.00000000e-01  8.97900000e+03\n",
      "  -4.35146215e+00]\n",
      " [ 8.50000000e+01  0.00000000e+00 -1.75000000e+00  1.31000000e+04\n",
      "  -4.82919126e+00]\n",
      " [ 8.60000000e+01  0.00000000e+00  0.00000000e+00  1.30990000e+04\n",
      "  -4.77054254e+00]\n",
      " [ 8.70000000e+01  0.00000000e+00 -2.25000000e+00  1.31030000e+04\n",
      "  -4.67881889e+00]\n",
      " [ 8.80000000e+01  0.00000000e+00 -1.25000000e+00  1.31020000e+04\n",
      "  -4.72002592e+00]\n",
      " [ 8.90000000e+01  0.00000000e+00 -1.50000000e+00  1.31000000e+04\n",
      "  -4.73723563e+00]\n",
      " [ 9.00000000e+01  0.00000000e+00  0.00000000e+00  1.31000000e+04\n",
      "  -4.70100598e+00]\n",
      " [ 9.10000000e+01  0.00000000e+00 -2.50000000e-01  1.30970000e+04\n",
      "  -4.57155335e+00]\n",
      " [ 9.20000000e+01  0.00000000e+00 -5.00000000e-01  1.73080000e+04\n",
      "  -4.58391708e+00]\n",
      " [ 9.30000000e+01  0.00000000e+00  5.00000000e-01  8.87300000e+03\n",
      "  -1.60180766e+00]\n",
      " [ 9.40000000e+01  0.00000000e+00  0.00000000e+00  9.89600000e+03\n",
      "  -4.82656980e+00]\n",
      " [ 9.50000000e+01  0.00000000e+00  1.00000000e+00  1.02790000e+04\n",
      "  -4.63284963e+00]\n",
      " [ 9.60000000e+01  0.00000000e+00  5.00000000e-01  1.49800000e+04\n",
      "  -4.99611520e+00]\n",
      " [ 9.70000000e+01  0.00000000e+00  2.50000000e-01  1.12570000e+04\n",
      "  -4.94610220e+00]\n",
      " [ 9.80000000e+01  0.00000000e+00  5.00000000e-01  1.15730000e+04\n",
      "  -4.79924882e+00]\n",
      " [ 9.90000000e+01  0.00000000e+00 -7.50000000e-01  1.19330000e+04\n",
      "  -5.11119226e+00]\n",
      " [ 0.00000000e+00  1.00000000e+00  2.50000000e-01  1.34400000e+04\n",
      "  -4.59731045e+00]\n",
      " [ 1.00000000e+00  1.00000000e+00 -1.00000000e+00  1.74280000e+04\n",
      "  -5.17786591e+00]\n",
      " [ 2.00000000e+00  1.00000000e+00  7.50000000e-01  1.41770000e+04\n",
      "  -4.98249716e+00]\n",
      " [ 3.00000000e+00  1.00000000e+00  5.00000000e-01  1.94460000e+04\n",
      "  -4.97363529e+00]\n",
      " [ 4.00000000e+00  1.00000000e+00 -1.75000000e+00  1.45410000e+04\n",
      "  -4.95936620e+00]\n",
      " [ 5.00000000e+00  1.00000000e+00 -7.50000000e-01  1.25880000e+04\n",
      "  -5.18533370e+00]\n",
      " [ 6.00000000e+00  1.00000000e+00 -1.50000000e+00  1.56200000e+04\n",
      "  -4.74053322e+00]\n",
      " [ 7.00000000e+00  1.00000000e+00 -5.00000000e-01  1.05030000e+04\n",
      "  -5.15736396e+00]\n",
      " [ 8.00000000e+00  1.00000000e+00 -1.75000000e+00  1.91270000e+04\n",
      "  -4.71477124e+00]\n",
      " [ 9.00000000e+00  1.00000000e+00 -1.25000000e+00  1.87000000e+04\n",
      "  -4.97088199e+00]\n",
      " [ 1.00000000e+01  1.00000000e+00 -2.00000000e+00  1.19550000e+04\n",
      "  -5.17150304e+00]\n",
      " [ 1.10000000e+01  1.00000000e+00 -1.50000000e+00  9.68600000e+03\n",
      "  -5.00237350e+00]\n",
      " [ 1.20000000e+01  1.00000000e+00 -1.00000000e+00  1.01500000e+04\n",
      "  -4.90183413e+00]\n",
      " [ 1.30000000e+01  1.00000000e+00 -1.00000000e+00  9.49300000e+03\n",
      "  -5.02755127e+00]\n",
      " [ 1.40000000e+01  1.00000000e+00 -2.50000000e-01  1.99560000e+04\n",
      "  -5.09029458e+00]\n",
      " [ 1.50000000e+01  1.00000000e+00  2.50000000e-01  8.52600000e+03\n",
      "  -5.37307023e+00]\n",
      " [ 1.60000000e+01  1.00000000e+00  5.00000000e-01  1.23000000e+04\n",
      "  -4.98263625e+00]\n",
      " [ 1.70000000e+01  1.00000000e+00 -2.50000000e-01  2.15870000e+04\n",
      "  -4.93951894e+00]\n",
      " [ 1.80000000e+01  1.00000000e+00  5.00000000e-01  7.85500000e+03\n",
      "  -5.24251717e+00]\n",
      " [ 1.90000000e+01  1.00000000e+00  0.00000000e+00  4.54130000e+04\n",
      "  -5.07020419e+00]\n",
      " [ 2.00000000e+01  1.00000000e+00 -2.25000000e+00  1.44530000e+04\n",
      "  -5.31808662e+00]\n",
      " [ 2.10000000e+01  1.00000000e+00 -1.00000000e+00  7.54800000e+03\n",
      "  -5.11730097e+00]\n",
      " [ 2.20000000e+01  1.00000000e+00 -2.00000000e+00  1.15620000e+04\n",
      "  -4.91943748e+00]\n",
      " [ 2.30000000e+01  1.00000000e+00 -2.50000000e+00  7.21400000e+03\n",
      "  -4.93908603e+00]\n",
      " [ 2.40000000e+01  1.00000000e+00 -1.00000000e+00  8.43700000e+03\n",
      "  -4.91565774e+00]\n",
      " [ 2.50000000e+01  1.00000000e+00  0.00000000e+00  9.64300000e+03\n",
      "  -5.18375913e+00]\n",
      " [ 2.60000000e+01  1.00000000e+00  1.00000000e+00  9.63200000e+03\n",
      "  -4.89035294e+00]\n",
      " [ 2.70000000e+01  1.00000000e+00 -2.25000000e+00  1.41240000e+04\n",
      "  -4.79491593e+00]\n",
      " [ 2.80000000e+01  1.00000000e+00 -1.75000000e+00  1.45860000e+04\n",
      "  -4.25853618e+00]\n",
      " [ 2.90000000e+01  1.00000000e+00 -7.50000000e-01  1.10110000e+04\n",
      "  -4.89658746e+00]\n",
      " [ 3.00000000e+01  1.00000000e+00  2.50000000e-01  1.87320000e+04\n",
      "  -2.94556686e+00]\n",
      " [ 3.10000000e+01  1.00000000e+00  1.00000000e+00  1.79580000e+04\n",
      "  -4.92067878e+00]\n",
      " [ 3.20000000e+01  1.00000000e+00  7.50000000e-01  1.39440000e+04\n",
      "  -5.03382706e+00]\n",
      " [ 3.30000000e+01  1.00000000e+00 -5.00000000e-01  2.19770000e+04\n",
      "  -5.04165381e+00]\n",
      " [ 3.40000000e+01  1.00000000e+00  2.50000000e-01  1.10720000e+04\n",
      "  -4.80581270e+00]\n",
      " [ 3.50000000e+01  1.00000000e+00 -1.50000000e+00  1.97090000e+04\n",
      "  -5.10080624e+00]\n",
      " [ 3.60000000e+01  1.00000000e+00  7.50000000e-01  1.49830000e+04\n",
      "  -4.98528268e+00]\n",
      " [ 3.70000000e+01  1.00000000e+00 -7.50000000e-01  2.07540000e+04\n",
      "  -4.92980707e+00]\n",
      " [ 3.80000000e+01  1.00000000e+00 -1.50000000e+00  1.19020000e+04\n",
      "  -4.90343921e+00]\n",
      " [ 3.90000000e+01  1.00000000e+00 -1.25000000e+00  1.39550000e+04\n",
      "  -4.79042956e+00]\n",
      " [ 4.00000000e+01  1.00000000e+00  2.50000000e-01  5.93810000e+04\n",
      "  -4.85337394e+00]\n",
      " [ 4.10000000e+01  1.00000000e+00 -2.50000000e-01  1.40520000e+04\n",
      "  -5.44706358e+00]\n",
      " [ 4.20000000e+01  1.00000000e+00  5.00000000e-01  1.30950000e+04\n",
      "  -5.44198603e+00]\n",
      " [ 4.30000000e+01  1.00000000e+00  0.00000000e+00  1.30990000e+04\n",
      "  -5.44011605e+00]\n",
      " [ 4.40000000e+01  1.00000000e+00 -2.00000000e+00  1.31860000e+04\n",
      "  -4.92549523e+00]\n",
      " [ 4.50000000e+01  1.00000000e+00 -7.50000000e-01  1.31020000e+04\n",
      "  -5.21814908e+00]\n",
      " [ 4.60000000e+01  1.00000000e+00 -2.50000000e-01  1.52510000e+04\n",
      "  -5.27296475e+00]\n",
      " [ 4.70000000e+01  1.00000000e+00  1.25000000e+00  1.30960000e+04\n",
      "  -5.14231990e+00]\n",
      " [ 4.80000000e+01  1.00000000e+00  2.50000000e-01  1.69800000e+04\n",
      "  -4.62343286e+00]\n",
      " [ 4.90000000e+01  1.00000000e+00  0.00000000e+00  1.40200000e+04\n",
      "  -4.35885212e+00]\n",
      " [ 5.00000000e+01  1.00000000e+00  5.00000000e-01  1.45870000e+04\n",
      "  -4.48081614e+00]\n",
      " [ 5.10000000e+01  1.00000000e+00  2.50000000e-01  2.79080000e+04\n",
      "  -4.52544209e+00]\n",
      " [ 5.20000000e+01  1.00000000e+00  7.50000000e-01  1.32640000e+04\n",
      "  -5.36595313e+00]\n",
      " [ 5.30000000e+01  1.00000000e+00  5.00000000e-01  1.39300000e+04\n",
      "  -4.44609733e+00]\n",
      " [ 5.40000000e+01  1.00000000e+00  1.00000000e+00  1.13650000e+04\n",
      "  -3.62600624e+00]\n",
      " [ 5.50000000e+01  1.00000000e+00 -2.50000000e-01  1.45700000e+04\n",
      "  -3.55068808e+00]\n",
      " [ 5.60000000e+01  1.00000000e+00  0.00000000e+00  1.26350000e+04\n",
      "  -3.79869868e+00]\n",
      " [ 5.70000000e+01  1.00000000e+00  1.00000000e+00  1.40300000e+04\n",
      "  -4.28600024e+00]\n",
      " [ 5.80000000e+01  1.00000000e+00  5.00000000e-01  1.57260000e+04\n",
      "  -4.21217326e+00]\n",
      " [ 5.90000000e+01  1.00000000e+00  2.50000000e-01  1.57570000e+04\n",
      "  -4.32074879e+00]\n",
      " [ 6.00000000e+01  1.00000000e+00  5.00000000e-01  1.45320000e+04\n",
      "  -4.93258173e+00]\n",
      " [ 6.10000000e+01  1.00000000e+00  2.50000000e-01  1.67510000e+04\n",
      "  -5.26509252e+00]\n",
      " [ 6.20000000e+01  1.00000000e+00 -1.00000000e+00  1.66280000e+04\n",
      "  -4.73186292e+00]\n",
      " [ 6.30000000e+01  1.00000000e+00  1.00000000e+00  1.24870000e+04\n",
      "  -4.84810123e+00]\n",
      " [ 6.40000000e+01  1.00000000e+00 -1.25000000e+00  1.65010000e+04\n",
      "  -4.56141518e+00]\n",
      " [ 6.50000000e+01  1.00000000e+00  2.50000000e-01  1.08910000e+04\n",
      "  -5.15765390e+00]\n",
      " [ 6.60000000e+01  1.00000000e+00  0.00000000e+00  1.94790000e+04\n",
      "  -4.76874387e+00]\n",
      " [ 6.70000000e+01  1.00000000e+00 -5.00000000e-01  3.65560000e+04\n",
      "  -5.05349134e+00]\n",
      " [ 6.80000000e+01  1.00000000e+00 -2.50000000e+00  7.00600000e+03\n",
      "  -5.20846981e+00]\n",
      " [ 6.90000000e+01  1.00000000e+00  0.00000000e+00  7.48300000e+03\n",
      "  -5.26818771e+00]\n",
      " [ 7.00000000e+01  1.00000000e+00  1.00000000e+00  1.58530000e+04\n",
      "  -4.91247561e+00]\n",
      " [ 7.10000000e+01  1.00000000e+00 -5.00000000e-01  9.50400000e+03\n",
      "  -5.05021179e+00]\n",
      " [ 7.20000000e+01  1.00000000e+00 -5.00000000e-01  7.31800000e+03\n",
      "  -5.24644822e+00]\n",
      " [ 7.30000000e+01  1.00000000e+00 -7.50000000e-01  7.87000000e+03\n",
      "  -4.91156829e+00]\n",
      " [ 7.40000000e+01  1.00000000e+00 -2.00000000e+00  7.82400000e+03\n",
      "  -4.75915891e+00]\n",
      " [ 7.50000000e+01  1.00000000e+00 -1.25000000e+00  1.15630000e+04\n",
      "  -4.82805842e+00]\n",
      " [ 7.60000000e+01  1.00000000e+00 -2.50000000e-01  1.01050000e+04\n",
      "  -4.71620886e+00]\n",
      " [ 7.70000000e+01  1.00000000e+00 -1.25000000e+00  1.12530000e+04\n",
      "  -4.55447884e+00]\n",
      " [ 7.80000000e+01  1.00000000e+00 -7.50000000e-01  1.43640000e+04\n",
      "  -4.70774604e+00]\n",
      " [ 7.90000000e+01  1.00000000e+00 -2.00000000e+00  1.56330000e+04\n",
      "  -4.84573299e+00]\n",
      " [ 8.00000000e+01  1.00000000e+00 -1.75000000e+00  1.13810000e+04\n",
      "  -4.65646067e+00]\n",
      " [ 8.10000000e+01  1.00000000e+00 -2.50000000e-01  2.67300000e+04\n",
      "  -5.00017687e+00]\n",
      " [ 8.20000000e+01  1.00000000e+00 -2.25000000e+00  1.00390000e+04\n",
      "  -5.09211646e+00]\n",
      " [ 8.30000000e+01  1.00000000e+00 -1.50000000e+00  2.27510000e+04\n",
      "  -4.58067800e+00]\n",
      " [ 8.40000000e+01  1.00000000e+00 -2.50000000e+00  1.30220000e+04\n",
      "  -4.81107156e+00]\n",
      " [ 8.50000000e+01  1.00000000e+00 -2.50000000e-01  1.94840000e+04\n",
      "  -4.68334599e+00]\n",
      " [ 8.60000000e+01  1.00000000e+00 -1.00000000e+00  1.15530000e+04\n",
      "  -4.64235592e+00]\n",
      " [ 8.70000000e+01  1.00000000e+00 -7.50000000e-01  1.28450000e+04\n",
      "  -5.09654933e+00]\n",
      " [ 8.80000000e+01  1.00000000e+00 -1.25000000e+00  2.31570000e+04\n",
      "  -5.07996892e+00]\n",
      " [ 8.90000000e+01  1.00000000e+00 -1.00000000e+00  1.48370000e+04\n",
      "  -4.75286566e+00]\n",
      " [ 9.00000000e+01  1.00000000e+00 -2.50000000e-01  1.74880000e+04\n",
      "  -5.36722945e+00]\n",
      " [ 9.10000000e+01  1.00000000e+00 -1.75000000e+00  1.14080000e+04\n",
      "  -5.12753901e+00]\n",
      " [ 9.20000000e+01  1.00000000e+00 -1.50000000e+00  6.47320000e+04\n",
      "  -5.04379569e+00]\n",
      " [ 9.30000000e+01  1.00000000e+00 -1.25000000e+00  1.39560000e+04\n",
      "  -5.41660216e+00]\n",
      " [ 9.40000000e+01  1.00000000e+00 -1.50000000e+00  1.10110000e+04\n",
      "  -4.37929903e+00]\n",
      " [ 9.50000000e+01  1.00000000e+00  7.50000000e-01  1.87660000e+04\n",
      "  -5.07134357e+00]\n",
      " [ 9.60000000e+01  1.00000000e+00 -5.00000000e-01  8.76700000e+03\n",
      "  -5.36896787e+00]\n",
      " [ 9.70000000e+01  1.00000000e+00 -2.25000000e+00  7.72700000e+03\n",
      "  -4.88302320e+00]\n",
      " [ 9.80000000e+01  1.00000000e+00 -1.25000000e+00  9.05200000e+03\n",
      "  -5.18819127e+00]\n",
      " [ 9.90000000e+01  1.00000000e+00 -2.00000000e+00  1.12920000e+04\n",
      "  -4.79901726e+00]]\n",
      "Writing to file data.csv\n",
      "**************************************************\n",
      "Data save complete\n",
      "**************************************************\n",
      "randomcar\n",
      "**************************************************\n",
      "Running experiment\n",
      "**************************************************\n",
      "Episode : 0\n",
      "state : [0 0 7 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "action : 2\n",
      "new state: [0 0 7 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "reward: 0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 0 7 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "action : 2\n",
      "new state: [0 0 7 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [0 0 7 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "action : 2\n",
      "new state: [0 0 7 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [0 0 7 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "action : 2\n",
      "new state: [0 0 6 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: 0.25\n",
      "epReward so far: -1.25\n",
      "state : [0 0 6 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 2\n",
      "new state: [0 0 6 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "reward: -0.0\n",
      "epReward so far: -1.25\n",
      "final state: [0 0 6 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "Episode : 1\n",
      "state : [0 0 6 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "action : 3\n",
      "new state: [0 1 6 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [0 1 6 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "action : 2\n",
      "new state: [0 1 6 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [0 1 6 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "action : 3\n",
      "new state: [0 1 6 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "reward: -0.75\n",
      "epReward so far: -2.25\n",
      "state : [0 1 6 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "action : 3\n",
      "new state: [0 1 6 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "reward: -0.5\n",
      "epReward so far: -2.75\n",
      "state : [0 1 6 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "action : 1\n",
      "new state: [0 1 6 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "reward: -0.75\n",
      "epReward so far: -3.5\n",
      "final state: [0 1 6 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "Episode : 2\n",
      "state : [0 1 6 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "action : 1\n",
      "new state: [0 1 6 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [0 1 6 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 2\n",
      "new state: [0 2 5 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "reward: -0.75\n",
      "epReward so far: -0.5\n",
      "state : [0 2 5 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "action : 2\n",
      "new state: [1 2 4 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "reward: -0.5\n",
      "epReward so far: -1.0\n",
      "state : [1 2 4 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "action : 2\n",
      "new state: [1 2 3 2 3 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 1]\n",
      "reward: -0.5\n",
      "epReward so far: -1.5\n",
      "state : [1 2 3 2 3 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 1]\n",
      "action : 1\n",
      "new state: [1 1 3 3 1 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 1]\n",
      "reward: -0.5\n",
      "epReward so far: -2.0\n",
      "final state: [1 1 3 3 1 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 1]\n",
      "Episode : 3\n",
      "state : [1 1 3 3 1 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 1]\n",
      "action : 2\n",
      "new state: [1 1 3 4 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 1 3 4 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 2\n",
      "new state: [1 2 3 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [1 2 3 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "action : 3\n",
      "new state: [1 2 3 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [1 2 3 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "action : 2\n",
      "new state: [1 2 3 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "reward: 0.0\n",
      "epReward so far: -1.5\n",
      "state : [1 2 3 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "action : 1\n",
      "new state: [1 1 3 4 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: 0.25\n",
      "epReward so far: -1.25\n",
      "final state: [1 1 3 4 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "Episode : 4\n",
      "state : [1 1 3 4 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 0\n",
      "new state: [1 1 3 4 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [1 1 3 4 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 2\n",
      "new state: [1 1 3 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [1 1 3 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "action : 3\n",
      "new state: [1 1 3 4 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: -0.5\n",
      "epReward so far: -2.0\n",
      "state : [1 1 3 4 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 3\n",
      "new state: [1 1 3 3 2 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 3]\n",
      "reward: -0.5\n",
      "epReward so far: -2.5\n",
      "state : [1 1 3 3 2 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 3]\n",
      "action : 1\n",
      "new state: [1 0 4 3 3 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 3]\n",
      "reward: -0.5\n",
      "epReward so far: -3.0\n",
      "final state: [1 0 4 3 3 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 3]\n",
      "Episode : 5\n",
      "state : [1 0 4 3 3 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 3]\n",
      "action : 2\n",
      "new state: [1 0 3 4 3 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [1 0 3 4 3 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "action : 3\n",
      "new state: [2 0 3 4 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [2 0 3 4 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "action : 3\n",
      "new state: [2 0 3 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "reward: -0.0\n",
      "epReward so far: -1.25\n",
      "state : [2 0 3 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "action : 2\n",
      "new state: [2 0 3 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "reward: -0.0\n",
      "epReward so far: -1.25\n",
      "state : [2 0 3 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "action : 3\n",
      "new state: [2 0 3 4 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "reward: -0.5\n",
      "epReward so far: -1.75\n",
      "final state: [2 0 3 4 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "Episode : 6\n",
      "state : [2 0 3 4 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "action : 0\n",
      "new state: [1 0 3 4 0 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [1 0 3 4 0 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2]\n",
      "action : 3\n",
      "new state: [2 0 3 4 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [2 0 3 4 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 3\n",
      "new state: [2 0 5 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "state : [2 0 5 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "action : 2\n",
      "new state: [2 0 5 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "reward: -0.75\n",
      "epReward so far: -2.75\n",
      "state : [2 0 5 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "action : 3\n",
      "new state: [2 0 5 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: -3.5\n",
      "final state: [2 0 5 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "Episode : 7\n",
      "state : [2 0 5 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "action : 2\n",
      "new state: [2 0 5 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 0 5 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "action : 2\n",
      "new state: [2 0 4 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "reward: -0.5\n",
      "epReward so far: -1.25\n",
      "state : [2 0 4 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "action : 0\n",
      "new state: [1 0 4 3 0 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "reward: -0.5\n",
      "epReward so far: -1.75\n",
      "state : [1 0 4 3 0 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "action : 0\n",
      "new state: [2 0 4 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: -0.0\n",
      "epReward so far: -1.75\n",
      "state : [2 0 4 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 3\n",
      "new state: [2 0 4 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "reward: 0.0\n",
      "epReward so far: -1.75\n",
      "final state: [2 0 4 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "Episode : 8\n",
      "state : [2 0 4 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "action : 0\n",
      "new state: [2 0 4 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 0 4 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "action : 2\n",
      "new state: [2 0 3 4 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: -0.5\n",
      "epReward so far: -1.25\n",
      "state : [2 0 3 4 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 2\n",
      "new state: [2 0 2 4 0 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "reward: -0.5\n",
      "epReward so far: -1.75\n",
      "state : [2 0 2 4 0 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "action : 2\n",
      "new state: [4 0 1 4 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: -0.75\n",
      "epReward so far: -2.5\n",
      "state : [4 0 1 4 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 3\n",
      "new state: [4 1 1 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "reward: -0.75\n",
      "epReward so far: -3.25\n",
      "final state: [4 1 1 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "Episode : 9\n",
      "state : [4 1 1 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "action : 0\n",
      "new state: [3 1 1 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [3 1 1 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "action : 3\n",
      "new state: [3 1 1 4 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: -0.5\n",
      "epReward so far: -1.25\n",
      "state : [3 1 1 4 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 0\n",
      "new state: [2 1 1 4 3 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "reward: 0.25\n",
      "epReward so far: -1.0\n",
      "state : [2 1 1 4 3 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "action : 0\n",
      "new state: [2 1 1 5 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.75\n",
      "state : [2 1 1 5 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "action : 3\n",
      "new state: [2 1 1 5 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: -0.5\n",
      "epReward so far: -2.25\n",
      "final state: [2 1 1 5 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "Episode : 10\n",
      "state : [2 1 1 5 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 0\n",
      "new state: [2 1 1 5 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 1 1 5 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 0\n",
      "new state: [1 2 1 5 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [1 2 1 5 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 1\n",
      "new state: [1 1 1 6 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.5\n",
      "state : [1 1 1 6 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 3\n",
      "new state: [1 1 1 6 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: 0.25\n",
      "epReward so far: -0.25\n",
      "state : [1 1 1 6 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 3\n",
      "new state: [1 1 1 5 1 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 3]\n",
      "reward: -0.5\n",
      "epReward so far: -0.75\n",
      "final state: [1 1 1 5 1 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 3]\n",
      "Episode : 11\n",
      "state : [1 1 1 5 1 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 3]\n",
      "action : 0\n",
      "new state: [0 2 1 5 3 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [0 2 1 5 3 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "action : 3\n",
      "new state: [1 2 1 4 3 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.5\n",
      "state : [1 2 1 4 3 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "action : 3\n",
      "new state: [1 2 1 4 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 1]\n",
      "reward: -0.5\n",
      "epReward so far: 0.0\n",
      "state : [1 2 1 4 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 1]\n",
      "action : 3\n",
      "new state: [1 3 1 3 2 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 1]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [1 3 1 3 2 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 1]\n",
      "action : 1\n",
      "new state: [1 3 2 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "final state: [1 3 2 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "Episode : 12\n",
      "state : [1 3 2 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 2\n",
      "new state: [1 4 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "reward: 0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 4 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "action : 0\n",
      "new state: [1 4 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [1 4 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "action : 1\n",
      "new state: [1 3 2 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: -0.5\n",
      "epReward so far: -1.25\n",
      "state : [1 3 2 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 3\n",
      "new state: [1 3 2 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "state : [1 3 2 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 3\n",
      "new state: [1 4 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "reward: -0.0\n",
      "epReward so far: -2.0\n",
      "final state: [1 4 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "Episode : 13\n",
      "state : [1 4 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "action : 3\n",
      "new state: [1 4 2 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [1 4 2 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "action : 2\n",
      "new state: [2 4 1 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.5\n",
      "state : [2 4 1 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 1\n",
      "new state: [2 4 1 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "reward: -0.5\n",
      "epReward so far: -1.0\n",
      "state : [2 4 1 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "action : 1\n",
      "new state: [2 4 1 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "reward: -0.75\n",
      "epReward so far: -1.75\n",
      "state : [2 4 1 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "action : 1\n",
      "new state: [2 4 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "reward: -0.0\n",
      "epReward so far: -1.75\n",
      "final state: [2 4 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Episode : 14\n",
      "state : [2 4 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "action : 1\n",
      "new state: [3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "action : 0\n",
      "new state: [3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "action : 0\n",
      "new state: [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "reward: -0.75\n",
      "epReward so far: -2.25\n",
      "state : [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "action : 0\n",
      "new state: [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "reward: -0.75\n",
      "epReward so far: -3.0\n",
      "state : [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "action : 0\n",
      "new state: [1 3 3 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: -0.5\n",
      "epReward so far: -3.5\n",
      "final state: [1 3 3 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "Episode : 15\n",
      "state : [1 3 3 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 3\n",
      "new state: [1 3 3 1 0 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 3]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [1 3 3 1 0 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 3]\n",
      "action : 2\n",
      "new state: [2 3 3 1 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: -0.0\n",
      "epReward so far: -0.5\n",
      "state : [2 3 3 1 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 1\n",
      "new state: [2 2 3 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "reward: -0.5\n",
      "epReward so far: -1.0\n",
      "state : [2 2 3 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "action : 2\n",
      "new state: [2 2 2 2 2 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 2]\n",
      "reward: -0.5\n",
      "epReward so far: -1.5\n",
      "state : [2 2 2 2 2 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 2]\n",
      "action : 2\n",
      "new state: [2 2 3 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: -0.75\n",
      "epReward so far: -2.25\n",
      "final state: [2 2 3 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "Episode : 16\n",
      "state : [2 2 3 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 1\n",
      "new state: [3 1 3 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [3 1 3 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "action : 0\n",
      "new state: [3 1 3 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: -0.5\n",
      "state : [3 1 3 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 0\n",
      "new state: [3 1 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [3 1 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "action : 3\n",
      "new state: [3 1 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "state : [3 1 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "action : 1\n",
      "new state: [3 0 3 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "reward: -0.5\n",
      "epReward so far: -2.5\n",
      "final state: [3 0 3 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "Episode : 17\n",
      "state : [3 0 3 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "action : 3\n",
      "new state: [3 0 3 2 1 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [3 0 3 2 1 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "action : 0\n",
      "new state: [2 1 3 2 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "reward: 0.25\n",
      "epReward so far: -0.25\n",
      "state : [2 1 3 2 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "action : 3\n",
      "new state: [3 2 3 1 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.0\n",
      "state : [3 2 3 1 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 2\n",
      "new state: [3 2 4 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.75\n",
      "state : [3 2 4 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "action : 1\n",
      "new state: [3 2 4 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "reward: -0.75\n",
      "epReward so far: -2.5\n",
      "final state: [3 2 4 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "Episode : 18\n",
      "state : [3 2 4 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "action : 1\n",
      "new state: [3 2 4 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [3 2 4 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "action : 1\n",
      "new state: [3 2 4 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "reward: -0.0\n",
      "epReward so far: -0.75\n",
      "state : [3 2 4 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "action : 0\n",
      "new state: [3 2 4 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [3 2 4 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "action : 2\n",
      "new state: [3 2 4 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "reward: -0.0\n",
      "epReward so far: -1.5\n",
      "state : [3 2 4 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "action : 2\n",
      "new state: [3 2 4 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "reward: -0.75\n",
      "epReward so far: -2.25\n",
      "final state: [3 2 4 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "Episode : 19\n",
      "state : [3 2 4 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "action : 2\n",
      "new state: [3 2 3 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [3 2 3 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 2\n",
      "new state: [3 2 3 1 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: -0.5\n",
      "state : [3 2 3 1 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 2\n",
      "new state: [3 2 3 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: -0.5\n",
      "epReward so far: -1.0\n",
      "state : [3 2 3 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 1\n",
      "new state: [3 2 3 1 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.75\n",
      "state : [3 2 3 1 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 0\n",
      "new state: [2 2 4 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: -0.5\n",
      "epReward so far: -2.25\n",
      "final state: [2 2 4 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "Episode : 20\n",
      "state : [2 2 4 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 2\n",
      "new state: [2 2 4 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 2 4 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 2\n",
      "new state: [2 3 3 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [2 3 3 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "action : 1\n",
      "new state: [2 3 3 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.5\n",
      "state : [2 3 3 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 1\n",
      "new state: [3 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "reward: -0.0\n",
      "epReward so far: -0.5\n",
      "state : [3 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "action : 0\n",
      "new state: [2 3 3 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: 0.25\n",
      "epReward so far: -0.25\n",
      "final state: [2 3 3 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "Episode : 21\n",
      "state : [2 3 3 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 1\n",
      "new state: [2 3 3 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "reward: 0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 3 3 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "action : 1\n",
      "new state: [2 3 3 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [2 3 3 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 2\n",
      "new state: [2 3 3 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [2 3 3 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 1\n",
      "new state: [3 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "state : [3 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "action : 2\n",
      "new state: [3 3 2 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: -0.5\n",
      "epReward so far: -2.5\n",
      "final state: [3 3 2 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "Episode : 22\n",
      "state : [3 3 2 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 0\n",
      "new state: [2 3 3 1 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 3 3 1 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "action : 1\n",
      "new state: [3 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [3 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "action : 1\n",
      "new state: [3 1 3 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: -0.5\n",
      "epReward so far: -2.0\n",
      "state : [3 1 3 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 2\n",
      "new state: [3 1 2 2 0 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 2]\n",
      "reward: -0.5\n",
      "epReward so far: -2.5\n",
      "state : [3 1 2 2 0 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 2]\n",
      "action : 3\n",
      "new state: [4 1 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: -2.5\n",
      "final state: [4 1 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "Episode : 23\n",
      "state : [4 1 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 2\n",
      "new state: [4 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "reward: 0.0\n",
      "epReward so far: 0.0\n",
      "state : [4 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "action : 2\n",
      "new state: [4 2 1 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [4 2 1 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "action : 2\n",
      "new state: [4 2 1 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.5\n",
      "state : [4 2 1 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 1\n",
      "new state: [4 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "reward: 0.0\n",
      "epReward so far: -0.5\n",
      "state : [4 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "action : 1\n",
      "new state: [4 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "reward: -0.0\n",
      "epReward so far: -0.5\n",
      "final state: [4 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "Episode : 24\n",
      "state : [4 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "action : 0\n",
      "new state: [4 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [4 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "action : 1\n",
      "new state: [5 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [5 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "action : 3\n",
      "new state: [5 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "reward: -0.75\n",
      "epReward so far: -2.25\n",
      "state : [5 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "action : 0\n",
      "new state: [5 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "reward: -0.75\n",
      "epReward so far: -3.0\n",
      "state : [5 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "action : 0\n",
      "new state: [4 2 2 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "reward: -0.5\n",
      "epReward so far: -3.5\n",
      "final state: [4 2 2 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "Episode : 25\n",
      "state : [4 2 2 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "action : 0\n",
      "new state: [3 2 2 1 0 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [3 2 2 1 0 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "action : 2\n",
      "new state: [4 2 2 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [4 2 2 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "action : 1\n",
      "new state: [4 2 2 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: -0.5\n",
      "epReward so far: -1.75\n",
      "state : [4 2 2 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 0\n",
      "new state: [3 2 2 1 0 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 1]\n",
      "reward: 0.25\n",
      "epReward so far: -1.5\n",
      "state : [3 2 2 1 0 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 1]\n",
      "action : 1\n",
      "new state: [4 2 2 1 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "reward: -0.75\n",
      "epReward so far: -2.25\n",
      "final state: [4 2 2 1 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "Episode : 26\n",
      "state : [4 2 2 1 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "action : 0\n",
      "new state: [4 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [4 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "action : 0\n",
      "new state: [3 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [3 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "action : 1\n",
      "new state: [3 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "reward: -0.0\n",
      "epReward so far: -0.75\n",
      "state : [3 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "action : 1\n",
      "new state: [3 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [3 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "action : 0\n",
      "new state: [2 2 3 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: 0.25\n",
      "epReward so far: -1.25\n",
      "final state: [2 2 3 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "Episode : 27\n",
      "state : [2 2 3 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 3\n",
      "new state: [2 2 3 1 3 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [2 2 3 1 3 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1]\n",
      "action : 0\n",
      "new state: [2 2 3 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.5\n",
      "state : [2 2 3 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "action : 2\n",
      "new state: [2 2 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "reward: -0.0\n",
      "epReward so far: -0.5\n",
      "state : [2 2 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "action : 2\n",
      "new state: [2 2 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [2 2 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "action : 2\n",
      "new state: [2 2 3 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "reward: -0.5\n",
      "epReward so far: -1.75\n",
      "final state: [2 2 3 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "Episode : 28\n",
      "state : [2 2 3 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "action : 0\n",
      "new state: [2 2 3 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 2 3 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 0\n",
      "new state: [1 2 3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "reward: 0.25\n",
      "epReward so far: -0.5\n",
      "state : [1 2 3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "action : 0\n",
      "new state: [0 2 3 3 2 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 0]\n",
      "reward: -0.5\n",
      "epReward so far: -1.0\n",
      "state : [0 2 3 3 2 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 0]\n",
      "action : 3\n",
      "new state: [0 2 4 2 0 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "reward: 0.25\n",
      "epReward so far: -0.75\n",
      "state : [0 2 4 2 0 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "action : 2\n",
      "new state: [0 2 5 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "final state: [0 2 5 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "Episode : 29\n",
      "state : [0 2 5 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "action : 2\n",
      "new state: [1 2 4 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [1 2 4 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "action : 2\n",
      "new state: [1 2 4 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [1 2 4 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "action : 0\n",
      "new state: [0 3 4 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: -0.5\n",
      "epReward so far: -1.75\n",
      "state : [0 3 4 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 3\n",
      "new state: [0 3 4 1 2 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 0]\n",
      "reward: -0.5\n",
      "epReward so far: -2.25\n",
      "state : [0 3 4 1 2 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 0]\n",
      "action : 3\n",
      "new state: [0 3 5 0 0 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1]\n",
      "reward: 0.25\n",
      "epReward so far: -2.0\n",
      "final state: [0 3 5 0 0 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1]\n",
      "Episode : 30\n",
      "state : [0 3 5 0 0 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1]\n",
      "action : 1\n",
      "new state: [1 3 5 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: 0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 3 5 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 2\n",
      "new state: [2 3 4 0 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [2 3 4 0 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 2\n",
      "new state: [2 3 4 0 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [2 3 4 0 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 0\n",
      "new state: [2 3 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "state : [2 3 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "action : 0\n",
      "new state: [2 3 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "reward: -0.75\n",
      "epReward so far: -2.75\n",
      "final state: [2 3 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "Episode : 31\n",
      "state : [2 3 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "action : 1\n",
      "new state: [2 2 5 0 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [2 2 5 0 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "action : 2\n",
      "new state: [2 2 5 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [2 2 5 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "action : 1\n",
      "new state: [2 3 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "state : [2 3 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "action : 2\n",
      "new state: [2 3 4 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "reward: 0.25\n",
      "epReward so far: -1.75\n",
      "state : [2 3 4 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "action : 2\n",
      "new state: [2 3 4 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "reward: -0.75\n",
      "epReward so far: -2.5\n",
      "final state: [2 3 4 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "Episode : 32\n",
      "state : [2 3 4 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "action : 1\n",
      "new state: [3 3 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 3 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "action : 2\n",
      "new state: [3 3 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [3 3 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "action : 0\n",
      "new state: [3 3 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [3 3 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "action : 1\n",
      "new state: [3 2 4 0 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: -0.5\n",
      "epReward so far: -2.0\n",
      "state : [3 2 4 0 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 1\n",
      "new state: [3 2 4 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "reward: -0.75\n",
      "epReward so far: -2.75\n",
      "final state: [3 2 4 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "Episode : 33\n",
      "state : [3 2 4 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "action : 2\n",
      "new state: [3 3 3 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [3 3 3 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "action : 2\n",
      "new state: [3 3 3 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [3 3 3 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 0\n",
      "new state: [3 3 3 0 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "reward: 0.25\n",
      "epReward so far: -1.0\n",
      "state : [3 3 3 0 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "action : 1\n",
      "new state: [3 3 3 0 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.75\n",
      "state : [3 3 3 0 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "action : 0\n",
      "new state: [2 3 4 0 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "reward: -0.5\n",
      "epReward so far: -2.25\n",
      "final state: [2 3 4 0 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "Episode : 34\n",
      "state : [2 3 4 0 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "action : 2\n",
      "new state: [2 3 4 0 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 3 4 0 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 1\n",
      "new state: [2 2 4 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: -0.5\n",
      "epReward so far: -1.25\n",
      "state : [2 2 4 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 2\n",
      "new state: [2 2 3 1 2 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "reward: 0.25\n",
      "epReward so far: -1.0\n",
      "state : [2 2 3 1 2 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "action : 3\n",
      "new state: [2 2 4 0 0 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "reward: -0.5\n",
      "epReward so far: -1.5\n",
      "state : [2 2 4 0 0 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "action : 2\n",
      "new state: [3 2 3 0 0 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 3]\n",
      "reward: -0.5\n",
      "epReward so far: -2.0\n",
      "final state: [3 2 3 0 0 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 3]\n",
      "Episode : 35\n",
      "state : [3 2 3 0 0 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 3]\n",
      "action : 0\n",
      "new state: [3 2 3 0 3 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [3 2 3 0 3 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 2]\n",
      "action : 1\n",
      "new state: [3 2 4 0 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "reward: -0.75\n",
      "epReward so far: -0.5\n",
      "state : [3 2 4 0 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "action : 0\n",
      "new state: [2 2 4 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: -0.5\n",
      "epReward so far: -1.0\n",
      "state : [2 2 4 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 2\n",
      "new state: [2 2 4 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "reward: 0.0\n",
      "epReward so far: -1.0\n",
      "state : [2 2 4 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "action : 2\n",
      "new state: [3 2 4 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.75\n",
      "final state: [3 2 4 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "Episode : 36\n",
      "state : [3 2 4 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "action : 1\n",
      "new state: [3 1 4 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [3 1 4 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "action : 2\n",
      "new state: [3 1 4 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [3 1 4 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "action : 2\n",
      "new state: [3 2 4 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "state : [3 2 4 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "action : 0\n",
      "new state: [3 2 4 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "reward: 0.0\n",
      "epReward so far: -2.0\n",
      "state : [3 2 4 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "action : 2\n",
      "new state: [3 2 4 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "reward: -0.75\n",
      "epReward so far: -2.75\n",
      "final state: [3 2 4 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "Episode : 37\n",
      "state : [3 2 4 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "action : 1\n",
      "new state: [3 1 4 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [3 1 4 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 2\n",
      "new state: [3 1 4 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [3 1 4 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 2\n",
      "new state: [3 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "state : [3 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "action : 2\n",
      "new state: [3 3 2 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: 0.25\n",
      "epReward so far: -1.75\n",
      "state : [3 3 2 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 3\n",
      "new state: [3 3 2 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: 0.0\n",
      "epReward so far: -1.75\n",
      "final state: [3 3 2 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "Episode : 38\n",
      "state : [3 3 2 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 1\n",
      "new state: [3 3 2 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [3 3 2 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "action : 1\n",
      "new state: [3 2 2 1 0 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "reward: -0.5\n",
      "epReward so far: -1.0\n",
      "state : [3 2 2 1 0 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "action : 0\n",
      "new state: [3 2 2 1 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1]\n",
      "reward: 0.25\n",
      "epReward so far: -0.75\n",
      "state : [3 2 2 1 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1]\n",
      "action : 0\n",
      "new state: [2 4 2 1 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [2 4 2 1 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 2\n",
      "new state: [2 4 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "reward: 0.0\n",
      "epReward so far: -1.5\n",
      "final state: [2 4 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "Episode : 39\n",
      "state : [2 4 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "action : 1\n",
      "new state: [2 3 3 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [2 3 3 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 0\n",
      "new state: [1 3 3 1 1 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 3]\n",
      "reward: -0.5\n",
      "epReward so far: -1.0\n",
      "state : [1 3 3 1 1 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 3]\n",
      "action : 2\n",
      "new state: [1 4 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: -0.75\n",
      "epReward so far: -1.75\n",
      "state : [1 4 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 1\n",
      "new state: [1 5 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "reward: -0.75\n",
      "epReward so far: -2.5\n",
      "state : [1 5 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "action : 1\n",
      "new state: [1 4 2 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: 0.25\n",
      "epReward so far: -2.25\n",
      "final state: [1 4 2 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "Episode : 40\n",
      "state : [1 4 2 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 2\n",
      "new state: [1 4 2 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 4 2 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 1\n",
      "new state: [2 4 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 4 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "action : 2\n",
      "new state: [2 4 1 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "reward: 0.25\n",
      "epReward so far: -0.5\n",
      "state : [2 4 1 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "action : 3\n",
      "new state: [2 4 1 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [2 4 1 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 2\n",
      "new state: [2 4 1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "final state: [2 4 1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "Episode : 41\n",
      "state : [2 4 1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "action : 3\n",
      "new state: [2 4 1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 4 1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "action : 1\n",
      "new state: [2 4 1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [2 4 1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "action : 3\n",
      "new state: [2 4 1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "reward: -0.75\n",
      "epReward so far: -2.25\n",
      "state : [2 4 1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "action : 3\n",
      "new state: [2 4 1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "reward: -0.75\n",
      "epReward so far: -3.0\n",
      "state : [2 4 1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "action : 3\n",
      "new state: [2 4 1 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "reward: -0.5\n",
      "epReward so far: -3.5\n",
      "final state: [2 4 1 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "Episode : 42\n",
      "state : [2 4 1 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "action : 1\n",
      "new state: [2 4 1 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 4 1 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 1\n",
      "new state: [3 4 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 4 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "action : 3\n",
      "new state: [3 4 1 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [3 4 1 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 0\n",
      "new state: [3 4 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.5\n",
      "state : [3 4 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "action : 1\n",
      "new state: [3 4 1 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "reward: -0.5\n",
      "epReward so far: -1.0\n",
      "final state: [3 4 1 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "Episode : 43\n",
      "state : [3 4 1 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "action : 0\n",
      "new state: [2 4 1 1 1 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [2 4 1 1 1 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 2]\n",
      "action : 1\n",
      "new state: [2 5 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: -0.75\n",
      "epReward so far: -0.5\n",
      "state : [2 5 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 0\n",
      "new state: [1 6 1 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: -0.5\n",
      "epReward so far: -1.0\n",
      "state : [1 6 1 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 3\n",
      "new state: [1 6 1 0 3 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 1]\n",
      "reward: 0.25\n",
      "epReward so far: -0.75\n",
      "state : [1 6 1 0 3 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 1]\n",
      "action : 0\n",
      "new state: [1 6 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: -0.0\n",
      "epReward so far: -0.75\n",
      "final state: [1 6 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "Episode : 44\n",
      "state : [1 6 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 1\n",
      "new state: [1 6 1 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [1 6 1 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "action : 1\n",
      "new state: [1 6 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [1 6 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "action : 1\n",
      "new state: [1 6 1 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "reward: 0.25\n",
      "epReward so far: -1.0\n",
      "state : [1 6 1 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "action : 0\n",
      "new state: [0 6 1 1 2 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 3]\n",
      "reward: -0.5\n",
      "epReward so far: -1.5\n",
      "state : [0 6 1 1 2 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 3]\n",
      "action : 1\n",
      "new state: [0 5 2 1 3 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 0]\n",
      "reward: -0.5\n",
      "epReward so far: -2.0\n",
      "final state: [0 5 2 1 3 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 0]\n",
      "Episode : 45\n",
      "state : [0 5 2 1 3 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 0]\n",
      "action : 3\n",
      "new state: [0 5 2 1 3 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [0 5 2 1 3 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1]\n",
      "action : 3\n",
      "new state: [0 6 2 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.5\n",
      "state : [0 6 2 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 1\n",
      "new state: [1 5 2 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: -0.5\n",
      "epReward so far: -1.0\n",
      "state : [1 5 2 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 1\n",
      "new state: [1 4 2 1 1 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 3]\n",
      "reward: -0.5\n",
      "epReward so far: -1.5\n",
      "state : [1 4 2 1 1 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 3]\n",
      "action : 1\n",
      "new state: [1 4 2 1 3 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 3]\n",
      "reward: 0.25\n",
      "epReward so far: -1.25\n",
      "final state: [1 4 2 1 3 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 3]\n",
      "Episode : 46\n",
      "state : [1 4 2 1 3 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 3]\n",
      "action : 0\n",
      "new state: [1 4 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 4 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 1\n",
      "new state: [1 4 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [1 4 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "action : 1\n",
      "new state: [1 3 2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: -0.5\n",
      "epReward so far: -1.25\n",
      "state : [1 3 2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 1\n",
      "new state: [1 3 2 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "state : [1 3 2 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 2\n",
      "new state: [1 3 1 4 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "reward: 0.25\n",
      "epReward so far: -1.75\n",
      "final state: [1 3 1 4 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "Episode : 47\n",
      "state : [1 3 1 4 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "action : 1\n",
      "new state: [1 2 1 4 3 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 1]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [1 2 1 4 3 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 1]\n",
      "action : 1\n",
      "new state: [1 1 1 5 1 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "reward: -0.5\n",
      "epReward so far: -1.0\n",
      "state : [1 1 1 5 1 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "action : 3\n",
      "new state: [1 2 1 4 1 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1]\n",
      "reward: 0.25\n",
      "epReward so far: -0.75\n",
      "state : [1 2 1 4 1 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1]\n",
      "action : 3\n",
      "new state: [1 3 1 4 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [1 3 1 4 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 2\n",
      "new state: [1 4 0 4 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: -0.5\n",
      "epReward so far: -2.0\n",
      "final state: [1 4 0 4 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "Episode : 48\n",
      "state : [1 4 0 4 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 1\n",
      "new state: [1 3 0 4 2 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [1 3 0 4 2 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "action : 3\n",
      "new state: [1 3 1 3 0 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "reward: -0.5\n",
      "epReward so far: -1.0\n",
      "state : [1 3 1 3 0 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "action : 3\n",
      "new state: [1 3 2 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.75\n",
      "state : [1 3 2 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 3\n",
      "new state: [2 3 2 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: -0.5\n",
      "epReward so far: -2.25\n",
      "state : [2 3 2 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 3\n",
      "new state: [2 3 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "reward: -0.75\n",
      "epReward so far: -3.0\n",
      "final state: [2 3 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "Episode : 49\n",
      "state : [2 3 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "action : 1\n",
      "new state: [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "action : 3\n",
      "new state: [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "action : 0\n",
      "new state: [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "action : 0\n",
      "new state: [1 3 2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: -0.5\n",
      "epReward so far: -2.0\n",
      "state : [1 3 2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 1\n",
      "new state: [1 3 2 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: -0.75\n",
      "epReward so far: -2.75\n",
      "final state: [1 3 2 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "Episode : 50\n",
      "state : [1 3 2 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 2\n",
      "new state: [1 3 1 4 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [1 3 1 4 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "action : 3\n",
      "new state: [1 3 1 4 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: -0.75\n",
      "epReward so far: -0.5\n",
      "state : [1 3 1 4 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 1\n",
      "new state: [1 2 1 5 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "reward: -0.5\n",
      "epReward so far: -1.0\n",
      "state : [1 2 1 5 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "action : 3\n",
      "new state: [1 2 1 5 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.75\n",
      "state : [1 2 1 5 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 1\n",
      "new state: [1 2 1 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "reward: -0.75\n",
      "epReward so far: -2.5\n",
      "final state: [1 2 1 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "Episode : 51\n",
      "state : [1 2 1 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "action : 0\n",
      "new state: [0 2 1 6 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [0 2 1 6 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 3\n",
      "new state: [0 2 1 5 2 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 0]\n",
      "reward: -0.5\n",
      "epReward so far: -0.25\n",
      "state : [0 2 1 5 2 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 0]\n",
      "action : 3\n",
      "new state: [0 2 2 5 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: -0.25\n",
      "state : [0 2 2 5 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 3\n",
      "new state: [0 2 3 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "reward: -0.75\n",
      "epReward so far: -1.0\n",
      "state : [0 2 3 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "action : 1\n",
      "new state: [0 2 3 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.75\n",
      "final state: [0 2 3 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "Episode : 52\n",
      "state : [0 2 3 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "action : 2\n",
      "new state: [0 3 2 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [0 3 2 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "action : 3\n",
      "new state: [0 3 2 4 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: 0.25\n",
      "epReward so far: -0.5\n",
      "state : [0 3 2 4 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 3\n",
      "new state: [0 3 2 4 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: 0.0\n",
      "epReward so far: -0.5\n",
      "state : [0 3 2 4 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 3\n",
      "new state: [1 3 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [1 3 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "action : 1\n",
      "new state: [2 2 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "final state: [2 2 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "Episode : 53\n",
      "state : [2 2 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "action : 3\n",
      "new state: [2 2 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 2 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "action : 2\n",
      "new state: [2 2 1 4 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "reward: -0.5\n",
      "epReward so far: -1.25\n",
      "state : [2 2 1 4 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "action : 0\n",
      "new state: [1 2 1 4 2 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 3]\n",
      "reward: -0.5\n",
      "epReward so far: -1.75\n",
      "state : [1 2 1 4 2 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 3]\n",
      "action : 2\n",
      "new state: [1 2 1 4 3 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1]\n",
      "reward: -0.5\n",
      "epReward so far: -2.25\n",
      "state : [1 2 1 4 3 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1]\n",
      "action : 3\n",
      "new state: [1 3 2 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: -0.75\n",
      "epReward so far: -3.0\n",
      "final state: [1 3 2 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "Episode : 54\n",
      "state : [1 3 2 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 0\n",
      "new state: [1 3 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [1 3 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "action : 1\n",
      "new state: [1 2 2 4 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "reward: -0.5\n",
      "epReward so far: -1.25\n",
      "state : [1 2 2 4 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "action : 2\n",
      "new state: [1 2 2 4 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "state : [1 2 2 4 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 3\n",
      "new state: [1 3 2 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: 0.25\n",
      "epReward so far: -1.75\n",
      "state : [1 3 2 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 3\n",
      "new state: [1 3 2 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: -0.75\n",
      "epReward so far: -2.5\n",
      "final state: [1 3 2 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "Episode : 55\n",
      "state : [1 3 2 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 1\n",
      "new state: [1 3 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [1 3 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "action : 2\n",
      "new state: [1 3 2 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: -0.5\n",
      "epReward so far: -1.25\n",
      "state : [1 3 2 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 2\n",
      "new state: [1 3 2 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "state : [1 3 2 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 2\n",
      "new state: [1 4 1 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "reward: -0.5\n",
      "epReward so far: -2.5\n",
      "state : [1 4 1 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "action : 1\n",
      "new state: [1 3 1 3 2 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "reward: 0.25\n",
      "epReward so far: -2.25\n",
      "final state: [1 3 1 3 2 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "Episode : 56\n",
      "state : [1 3 1 3 2 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "action : 1\n",
      "new state: [1 2 2 3 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [1 2 2 3 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "action : 3\n",
      "new state: [1 2 2 4 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.25\n",
      "state : [1 2 2 4 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 3\n",
      "new state: [1 2 3 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.5\n",
      "state : [1 2 3 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "action : 1\n",
      "new state: [1 1 3 4 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: -0.5\n",
      "epReward so far: -1.0\n",
      "state : [1 1 3 4 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 3\n",
      "new state: [1 1 3 4 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.75\n",
      "final state: [1 1 3 4 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "Episode : 57\n",
      "state : [1 1 3 4 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 3\n",
      "new state: [1 2 3 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 2 3 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "action : 1\n",
      "new state: [1 1 4 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [1 1 4 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "action : 2\n",
      "new state: [1 1 3 4 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: 0.25\n",
      "epReward so far: -0.5\n",
      "state : [1 1 3 4 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 3\n",
      "new state: [1 1 4 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [1 1 4 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "action : 3\n",
      "new state: [2 1 4 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "final state: [2 1 4 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "Episode : 58\n",
      "state : [2 1 4 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "action : 0\n",
      "new state: [1 1 4 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [1 1 4 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "action : 0\n",
      "new state: [0 1 4 3 3 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 3]\n",
      "reward: -0.5\n",
      "epReward so far: -0.25\n",
      "state : [0 1 4 3 3 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 3]\n",
      "action : 2\n",
      "new state: [0 1 4 4 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: -0.75\n",
      "epReward so far: -1.0\n",
      "state : [0 1 4 4 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 3\n",
      "new state: [1 1 4 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.75\n",
      "state : [1 1 4 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "action : 3\n",
      "new state: [1 1 4 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "reward: -0.75\n",
      "epReward so far: -2.5\n",
      "final state: [1 1 4 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "Episode : 59\n",
      "state : [1 1 4 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "action : 2\n",
      "new state: [1 1 3 4 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [1 1 3 4 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 3\n",
      "new state: [1 1 3 4 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [1 1 3 4 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 2\n",
      "new state: [2 1 3 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "state : [2 1 3 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "action : 3\n",
      "new state: [2 1 3 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "reward: -0.75\n",
      "epReward so far: -2.75\n",
      "state : [2 1 3 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "action : 2\n",
      "new state: [2 1 2 4 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: -0.5\n",
      "epReward so far: -3.25\n",
      "final state: [2 1 2 4 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "Episode : 60\n",
      "state : [2 1 2 4 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 1\n",
      "new state: [2 0 2 4 2 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 3]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [2 0 2 4 2 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 3]\n",
      "action : 3\n",
      "new state: [2 0 3 3 3 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 3]\n",
      "reward: -0.5\n",
      "epReward so far: -1.0\n",
      "state : [2 0 3 3 3 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 3]\n",
      "action : 3\n",
      "new state: [2 0 4 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.75\n",
      "state : [2 0 4 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 0\n",
      "new state: [1 0 4 4 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "reward: -0.5\n",
      "epReward so far: -2.25\n",
      "state : [1 0 4 4 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "action : 3\n",
      "new state: [1 0 4 4 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: -0.0\n",
      "epReward so far: -2.25\n",
      "final state: [1 0 4 4 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "Episode : 61\n",
      "state : [1 0 4 4 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 2\n",
      "new state: [1 1 3 4 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [1 1 3 4 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 2\n",
      "new state: [1 1 2 4 1 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "reward: -0.5\n",
      "epReward so far: -1.0\n",
      "state : [1 1 2 4 1 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "action : 2\n",
      "new state: [1 2 2 4 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.75\n",
      "state : [1 2 2 4 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "action : 2\n",
      "new state: [1 2 3 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "reward: -0.75\n",
      "epReward so far: -2.5\n",
      "state : [1 2 3 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "action : 2\n",
      "new state: [1 2 3 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "reward: -0.75\n",
      "epReward so far: -3.25\n",
      "final state: [1 2 3 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "Episode : 62\n",
      "state : [1 2 3 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "action : 0\n",
      "new state: [1 2 3 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [1 2 3 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "action : 1\n",
      "new state: [1 1 3 4 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: -0.5\n",
      "epReward so far: -1.25\n",
      "state : [1 1 3 4 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 0\n",
      "new state: [0 1 3 4 3 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 3]\n",
      "reward: -0.5\n",
      "epReward so far: -1.75\n",
      "state : [0 1 3 4 3 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 3]\n",
      "action : 3\n",
      "new state: [0 1 3 5 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: -0.75\n",
      "epReward so far: -2.5\n",
      "state : [0 1 3 5 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 2\n",
      "new state: [0 1 3 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "reward: -0.0\n",
      "epReward so far: -2.5\n",
      "final state: [0 1 3 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "Episode : 63\n",
      "state : [0 1 3 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "action : 3\n",
      "new state: [0 1 3 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [0 1 3 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "action : 1\n",
      "new state: [0 0 3 6 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "reward: 0.25\n",
      "epReward so far: -0.5\n",
      "state : [0 0 3 6 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "action : 3\n",
      "new state: [0 0 3 6 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [0 0 3 6 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "action : 2\n",
      "new state: [1 0 2 6 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: 0.25\n",
      "epReward so far: -1.0\n",
      "state : [1 0 2 6 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 3\n",
      "new state: [1 0 2 6 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.75\n",
      "final state: [1 0 2 6 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "Episode : 64\n",
      "state : [1 0 2 6 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "action : 3\n",
      "new state: [1 1 2 5 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [1 1 2 5 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 3\n",
      "new state: [1 1 2 4 1 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 3]\n",
      "reward: 0.25\n",
      "epReward so far: -0.25\n",
      "state : [1 1 2 4 1 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 3]\n",
      "action : 2\n",
      "new state: [1 2 2 4 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "reward: -0.75\n",
      "epReward so far: -1.0\n",
      "state : [1 2 2 4 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "action : 2\n",
      "new state: [2 3 1 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.75\n",
      "state : [2 3 1 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "action : 1\n",
      "new state: [2 2 1 4 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: -0.5\n",
      "epReward so far: -2.25\n",
      "final state: [2 2 1 4 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "Episode : 65\n",
      "state : [2 2 1 4 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 1\n",
      "new state: [2 2 1 4 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 2 1 4 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "action : 0\n",
      "new state: [1 3 1 4 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: 0.25\n",
      "epReward so far: -0.5\n",
      "state : [1 3 1 4 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 0\n",
      "new state: [1 3 1 4 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: -0.0\n",
      "epReward so far: -0.5\n",
      "state : [1 3 1 4 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 1\n",
      "new state: [1 3 1 4 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "reward: -0.5\n",
      "epReward so far: -1.0\n",
      "state : [1 3 1 4 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "action : 0\n",
      "new state: [0 3 1 4 3 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "reward: -0.5\n",
      "epReward so far: -1.5\n",
      "final state: [0 3 1 4 3 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "Episode : 66\n",
      "state : [0 3 1 4 3 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "action : 3\n",
      "new state: [0 3 1 5 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [0 3 1 5 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 2\n",
      "new state: [1 3 0 5 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "reward: 0.25\n",
      "epReward so far: -0.5\n",
      "state : [1 3 0 5 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "action : 1\n",
      "new state: [2 2 0 5 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [2 2 0 5 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "action : 3\n",
      "new state: [3 2 0 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "state : [3 2 0 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "action : 3\n",
      "new state: [3 2 0 4 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: -0.5\n",
      "epReward so far: -2.5\n",
      "final state: [3 2 0 4 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "Episode : 67\n",
      "state : [3 2 0 4 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 0\n",
      "new state: [3 2 0 4 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [3 2 0 4 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 0\n",
      "new state: [2 2 0 5 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: -0.5\n",
      "epReward so far: -1.25\n",
      "state : [2 2 0 5 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 1\n",
      "new state: [2 1 0 5 3 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "reward: -0.5\n",
      "epReward so far: -1.75\n",
      "state : [2 1 0 5 3 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "action : 0\n",
      "new state: [2 1 0 6 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "reward: -0.75\n",
      "epReward so far: -2.5\n",
      "state : [2 1 0 6 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "action : 3\n",
      "new state: [2 2 0 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "reward: -0.75\n",
      "epReward so far: -3.25\n",
      "final state: [2 2 0 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "Episode : 68\n",
      "state : [2 2 0 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "action : 3\n",
      "new state: [2 2 0 5 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [2 2 0 5 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 3\n",
      "new state: [2 2 0 5 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [2 2 0 5 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "action : 3\n",
      "new state: [2 2 0 5 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "reward: -0.5\n",
      "epReward so far: -1.75\n",
      "state : [2 2 0 5 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "action : 3\n",
      "new state: [2 2 0 5 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: -2.5\n",
      "state : [2 2 0 5 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 0\n",
      "new state: [2 2 0 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "reward: -0.75\n",
      "epReward so far: -3.25\n",
      "final state: [2 2 0 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "Episode : 69\n",
      "state : [2 2 0 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "action : 0\n",
      "new state: [2 2 0 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 2 0 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "action : 3\n",
      "new state: [2 2 0 5 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "reward: -0.5\n",
      "epReward so far: -1.25\n",
      "state : [2 2 0 5 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "action : 3\n",
      "new state: [2 2 0 5 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "state : [2 2 0 5 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 3\n",
      "new state: [2 2 0 5 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "reward: -0.5\n",
      "epReward so far: -2.5\n",
      "state : [2 2 0 5 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "action : 1\n",
      "new state: [3 1 0 5 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: -0.75\n",
      "epReward so far: -3.25\n",
      "final state: [3 1 0 5 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "Episode : 70\n",
      "state : [3 1 0 5 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 0\n",
      "new state: [2 1 0 6 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [2 1 0 6 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "action : 0\n",
      "new state: [1 1 0 6 3 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "reward: -0.5\n",
      "epReward so far: -1.0\n",
      "state : [1 1 0 6 3 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "action : 3\n",
      "new state: [1 1 0 7 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "reward: -0.0\n",
      "epReward so far: -1.0\n",
      "state : [1 1 0 7 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "action : 3\n",
      "new state: [1 2 0 6 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "reward: -0.5\n",
      "epReward so far: -1.5\n",
      "state : [1 2 0 6 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "action : 1\n",
      "new state: [1 2 0 6 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: -1.5\n",
      "final state: [1 2 0 6 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "Episode : 71\n",
      "state : [1 2 0 6 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 3\n",
      "new state: [1 3 0 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [1 3 0 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "action : 3\n",
      "new state: [1 3 0 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [1 3 0 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "action : 3\n",
      "new state: [1 3 0 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "reward: -0.75\n",
      "epReward so far: -2.25\n",
      "state : [1 3 0 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "action : 3\n",
      "new state: [1 3 0 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "reward: -0.75\n",
      "epReward so far: -3.0\n",
      "state : [1 3 0 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "action : 3\n",
      "new state: [1 3 0 5 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: -0.5\n",
      "epReward so far: -3.5\n",
      "final state: [1 3 0 5 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "Episode : 72\n",
      "state : [1 3 0 5 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 1\n",
      "new state: [1 2 0 5 3 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [1 2 0 5 3 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "action : 0\n",
      "new state: [1 2 0 6 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [1 2 0 6 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "action : 1\n",
      "new state: [1 1 0 7 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: 0.25\n",
      "epReward so far: -1.0\n",
      "state : [1 1 0 7 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 3\n",
      "new state: [1 1 0 6 0 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "reward: -0.5\n",
      "epReward so far: -1.5\n",
      "state : [1 1 0 6 0 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "action : 3\n",
      "new state: [2 1 0 5 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 3]\n",
      "reward: -0.5\n",
      "epReward so far: -2.0\n",
      "final state: [2 1 0 5 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 3]\n",
      "Episode : 73\n",
      "state : [2 1 0 5 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 3]\n",
      "action : 3\n",
      "new state: [2 1 0 6 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "reward: 0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 1 0 6 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "action : 3\n",
      "new state: [2 1 1 5 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [2 1 1 5 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 3\n",
      "new state: [2 1 1 5 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [2 1 1 5 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "action : 3\n",
      "new state: [3 1 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "state : [3 1 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "action : 3\n",
      "new state: [4 1 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "reward: -0.75\n",
      "epReward so far: -2.75\n",
      "final state: [4 1 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "Episode : 74\n",
      "state : [4 1 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "action : 0\n",
      "new state: [4 1 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [4 1 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "action : 1\n",
      "new state: [4 1 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [4 1 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "action : 0\n",
      "new state: [4 1 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "reward: -0.75\n",
      "epReward so far: -2.25\n",
      "state : [4 1 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "action : 0\n",
      "new state: [4 1 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "reward: -0.75\n",
      "epReward so far: -3.0\n",
      "state : [4 1 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "action : 3\n",
      "new state: [4 1 2 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: 0.25\n",
      "epReward so far: -2.75\n",
      "final state: [4 1 2 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "Episode : 75\n",
      "state : [4 1 2 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 3\n",
      "new state: [4 1 2 1 0 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [4 1 2 1 0 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0]\n",
      "action : 1\n",
      "new state: [5 0 2 1 0 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 3]\n",
      "reward: 0.25\n",
      "epReward so far: -0.25\n",
      "state : [5 0 2 1 0 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 3]\n",
      "action : 0\n",
      "new state: [4 0 2 2 0 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 3]\n",
      "reward: 0.25\n",
      "epReward so far: 0.0\n",
      "state : [4 0 2 2 0 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 3]\n",
      "action : 2\n",
      "new state: [5 0 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [5 0 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 2\n",
      "new state: [5 0 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "reward: -0.0\n",
      "epReward so far: -0.75\n",
      "final state: [5 0 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "Episode : 76\n",
      "state : [5 0 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "action : 0\n",
      "new state: [4 0 2 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [4 0 2 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 3\n",
      "new state: [4 0 2 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.5\n",
      "state : [4 0 2 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 3\n",
      "new state: [4 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [4 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "action : 0\n",
      "new state: [4 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "reward: -0.0\n",
      "epReward so far: -1.25\n",
      "state : [4 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "action : 3\n",
      "new state: [4 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "final state: [4 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "Episode : 77\n",
      "state : [4 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "action : 0\n",
      "new state: [4 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [4 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "action : 2\n",
      "new state: [4 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "reward: -0.0\n",
      "epReward so far: -0.75\n",
      "state : [4 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "action : 0\n",
      "new state: [4 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [4 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "action : 0\n",
      "new state: [3 1 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: -2.25\n",
      "state : [3 1 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "action : 2\n",
      "new state: [3 1 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "reward: -0.75\n",
      "epReward so far: -3.0\n",
      "final state: [3 1 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "Episode : 78\n",
      "state : [3 1 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "action : 0\n",
      "new state: [3 1 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [3 1 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "action : 0\n",
      "new state: [2 1 3 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "reward: -0.5\n",
      "epReward so far: -1.25\n",
      "state : [2 1 3 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "action : 1\n",
      "new state: [2 1 3 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: -0.0\n",
      "epReward so far: -1.25\n",
      "state : [2 1 3 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 3\n",
      "new state: [2 1 3 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "reward: -0.5\n",
      "epReward so far: -1.75\n",
      "state : [2 1 3 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "action : 0\n",
      "new state: [1 1 3 3 0 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2]\n",
      "reward: 0.25\n",
      "epReward so far: -1.5\n",
      "final state: [1 1 3 3 0 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2]\n",
      "Episode : 79\n",
      "state : [1 1 3 3 0 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2]\n",
      "action : 2\n",
      "new state: [2 1 3 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 1 3 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "action : 1\n",
      "new state: [2 1 3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [2 1 3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "action : 0\n",
      "new state: [1 1 3 3 2 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 3]\n",
      "reward: -0.5\n",
      "epReward so far: -0.25\n",
      "state : [1 1 3 3 2 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 3]\n",
      "action : 3\n",
      "new state: [1 1 4 2 3 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 3]\n",
      "reward: -0.5\n",
      "epReward so far: -0.75\n",
      "state : [1 1 4 2 3 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 3]\n",
      "action : 1\n",
      "new state: [2 0 4 2 3 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 3]\n",
      "reward: -0.5\n",
      "epReward so far: -1.25\n",
      "final state: [2 0 4 2 3 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 3]\n",
      "Episode : 80\n",
      "state : [2 0 4 2 3 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 3]\n",
      "action : 3\n",
      "new state: [2 0 4 2 3 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 3]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [2 0 4 2 3 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 3]\n",
      "action : 2\n",
      "new state: [2 0 3 3 3 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "reward: 0.25\n",
      "epReward so far: -0.25\n",
      "state : [2 0 3 3 3 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "action : 2\n",
      "new state: [2 0 2 4 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "reward: -0.5\n",
      "epReward so far: -0.75\n",
      "state : [2 0 2 4 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "action : 3\n",
      "new state: [2 0 2 4 2 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "reward: 0.25\n",
      "epReward so far: -0.5\n",
      "state : [2 0 2 4 2 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "action : 0\n",
      "new state: [1 0 3 4 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 3]\n",
      "reward: 0.25\n",
      "epReward so far: -0.25\n",
      "final state: [1 0 3 4 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 3]\n",
      "Episode : 81\n",
      "state : [1 0 3 4 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 3]\n",
      "action : 3\n",
      "new state: [1 1 3 4 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [1 1 3 4 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "action : 3\n",
      "new state: [1 1 4 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: 0.25\n",
      "epReward so far: -0.5\n",
      "state : [1 1 4 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 2\n",
      "new state: [1 1 3 3 0 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "reward: -0.5\n",
      "epReward so far: -1.0\n",
      "state : [1 1 3 3 0 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "action : 2\n",
      "new state: [2 1 3 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: -0.75\n",
      "epReward so far: -1.75\n",
      "state : [2 1 3 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 3\n",
      "new state: [2 1 3 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "reward: -0.75\n",
      "epReward so far: -2.5\n",
      "final state: [2 1 3 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "Episode : 82\n",
      "state : [2 1 3 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "action : 0\n",
      "new state: [1 1 3 4 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [1 1 3 4 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "action : 3\n",
      "new state: [1 1 3 4 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [1 1 3 4 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 3\n",
      "new state: [1 1 3 4 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "reward: -0.5\n",
      "epReward so far: -1.75\n",
      "state : [1 1 3 4 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "action : 0\n",
      "new state: [1 1 3 4 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "reward: -0.75\n",
      "epReward so far: -2.5\n",
      "state : [1 1 3 4 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "action : 1\n",
      "new state: [1 0 3 5 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "reward: -0.5\n",
      "epReward so far: -3.0\n",
      "final state: [1 0 3 5 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "Episode : 83\n",
      "state : [1 0 3 5 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "action : 3\n",
      "new state: [1 0 3 5 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [1 0 3 5 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 3\n",
      "new state: [2 0 3 4 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "reward: 0.25\n",
      "epReward so far: -0.5\n",
      "state : [2 0 3 4 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "action : 3\n",
      "new state: [2 0 3 4 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [2 0 3 4 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 2\n",
      "new state: [2 1 2 4 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "reward: -0.5\n",
      "epReward so far: -1.75\n",
      "state : [2 1 2 4 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "action : 1\n",
      "new state: [2 0 2 4 1 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 2]\n",
      "reward: 0.25\n",
      "epReward so far: -1.5\n",
      "final state: [2 0 2 4 1 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 2]\n",
      "Episode : 84\n",
      "state : [2 0 2 4 1 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 2]\n",
      "action : 0\n",
      "new state: [1 1 2 4 2 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [1 1 2 4 2 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "action : 3\n",
      "new state: [2 1 2 3 2 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 2]\n",
      "reward: 0.25\n",
      "epReward so far: -0.25\n",
      "state : [2 1 2 3 2 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 2]\n",
      "action : 3\n",
      "new state: [2 1 3 2 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.0\n",
      "state : [2 1 3 2 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1]\n",
      "action : 3\n",
      "new state: [2 2 3 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 2 3 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 1\n",
      "new state: [2 1 4 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: -0.5\n",
      "epReward so far: -1.25\n",
      "final state: [2 1 4 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "Episode : 85\n",
      "state : [2 1 4 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 2\n",
      "new state: [2 1 3 2 3 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [2 1 3 2 3 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 1]\n",
      "action : 0\n",
      "new state: [1 1 3 3 1 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.5\n",
      "state : [1 1 3 3 1 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 0]\n",
      "action : 2\n",
      "new state: [2 1 2 3 1 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "reward: -0.5\n",
      "epReward so far: 0.0\n",
      "state : [2 1 2 3 1 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "action : 3\n",
      "new state: [2 2 2 2 2 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [2 2 2 2 2 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "action : 0\n",
      "new state: [3 2 2 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: 0.0\n",
      "epReward so far: -0.5\n",
      "final state: [3 2 2 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "Episode : 86\n",
      "state : [3 2 2 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 3\n",
      "new state: [3 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "action : 2\n",
      "new state: [3 2 2 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [3 2 2 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 1\n",
      "new state: [3 1 2 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "reward: -0.75\n",
      "epReward so far: -0.5\n",
      "state : [3 1 2 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "action : 0\n",
      "new state: [2 2 2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: -0.5\n",
      "epReward so far: -1.0\n",
      "state : [2 2 2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 2\n",
      "new state: [2 2 2 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "reward: -0.0\n",
      "epReward so far: -1.0\n",
      "final state: [2 2 2 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "Episode : 87\n",
      "state : [2 2 2 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "action : 1\n",
      "new state: [2 1 2 4 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [2 1 2 4 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 2\n",
      "new state: [2 1 2 4 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "reward: 0.0\n",
      "epReward so far: -0.5\n",
      "state : [2 1 2 4 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "action : 0\n",
      "new state: [3 1 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [3 1 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "action : 0\n",
      "new state: [2 1 2 4 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "reward: -0.5\n",
      "epReward so far: -1.75\n",
      "state : [2 1 2 4 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "action : 3\n",
      "new state: [2 1 2 3 0 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 3]\n",
      "reward: -0.5\n",
      "epReward so far: -2.25\n",
      "final state: [2 1 2 3 0 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 3]\n",
      "Episode : 88\n",
      "state : [2 1 2 3 0 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 3]\n",
      "action : 3\n",
      "new state: [3 1 2 2 3 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [3 1 2 2 3 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "action : 0\n",
      "new state: [4 1 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [4 1 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "action : 3\n",
      "new state: [4 1 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "state : [4 1 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "action : 0\n",
      "new state: [4 1 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "reward: -0.75\n",
      "epReward so far: -2.75\n",
      "state : [4 1 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "action : 0\n",
      "new state: [4 1 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "reward: -0.75\n",
      "epReward so far: -3.5\n",
      "final state: [4 1 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "Episode : 89\n",
      "state : [4 1 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "action : 0\n",
      "new state: [3 1 2 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [3 1 2 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 1\n",
      "new state: [3 0 2 3 1 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1]\n",
      "reward: -0.5\n",
      "epReward so far: -1.0\n",
      "state : [3 0 2 3 1 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1]\n",
      "action : 3\n",
      "new state: [3 2 2 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.75\n",
      "state : [3 2 2 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 2\n",
      "new state: [3 2 2 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "reward: -0.5\n",
      "epReward so far: -2.25\n",
      "state : [3 2 2 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "action : 3\n",
      "new state: [3 2 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "reward: -0.75\n",
      "epReward so far: -3.0\n",
      "final state: [3 2 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "Episode : 90\n",
      "state : [3 2 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "action : 0\n",
      "new state: [2 2 2 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [2 2 2 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 0\n",
      "new state: [1 2 2 3 0 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "reward: 0.25\n",
      "epReward so far: -0.25\n",
      "state : [1 2 2 3 0 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "action : 3\n",
      "new state: [2 2 2 2 1 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.0\n",
      "state : [2 2 2 2 1 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "action : 3\n",
      "new state: [2 2 3 1 1 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 3]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [2 2 3 1 1 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 3]\n",
      "action : 0\n",
      "new state: [2 3 3 1 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "final state: [2 3 3 1 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "Episode : 91\n",
      "state : [2 3 3 1 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 2\n",
      "new state: [2 4 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 4 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "action : 2\n",
      "new state: [2 4 2 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "reward: -0.5\n",
      "epReward so far: -1.25\n",
      "state : [2 4 2 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "action : 0\n",
      "new state: [2 4 2 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "state : [2 4 2 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 0\n",
      "new state: [1 5 2 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: -0.5\n",
      "epReward so far: -2.5\n",
      "state : [1 5 2 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 2\n",
      "new state: [1 5 1 1 2 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "reward: -0.5\n",
      "epReward so far: -3.0\n",
      "final state: [1 5 1 1 2 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "Episode : 92\n",
      "state : [1 5 1 1 2 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "action : 1\n",
      "new state: [1 5 2 1 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [1 5 2 1 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 3\n",
      "new state: [1 5 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "reward: 0.0\n",
      "epReward so far: -0.75\n",
      "state : [1 5 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "action : 1\n",
      "new state: [1 5 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [1 5 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "action : 3\n",
      "new state: [1 5 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "reward: -0.0\n",
      "epReward so far: -1.5\n",
      "state : [1 5 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "action : 2\n",
      "new state: [1 5 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "reward: -0.75\n",
      "epReward so far: -2.25\n",
      "final state: [1 5 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "Episode : 93\n",
      "state : [1 5 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "action : 3\n",
      "new state: [1 5 2 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [1 5 2 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 3\n",
      "new state: [1 5 2 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "reward: -0.0\n",
      "epReward so far: -0.5\n",
      "state : [1 5 2 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "action : 1\n",
      "new state: [1 5 2 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: -0.5\n",
      "epReward so far: -1.0\n",
      "state : [1 5 2 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 1\n",
      "new state: [1 4 2 1 1 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 3]\n",
      "reward: -0.5\n",
      "epReward so far: -1.5\n",
      "state : [1 4 2 1 1 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 3]\n",
      "action : 1\n",
      "new state: [1 5 2 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "reward: -0.75\n",
      "epReward so far: -2.25\n",
      "final state: [1 5 2 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "Episode : 94\n",
      "state : [1 5 2 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "action : 1\n",
      "new state: [2 4 2 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [2 4 2 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 3\n",
      "new state: [2 4 2 0 0 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.5\n",
      "state : [2 4 2 0 0 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "action : 2\n",
      "new state: [3 4 1 0 1 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1]\n",
      "reward: -0.5\n",
      "epReward so far: 0.0\n",
      "state : [3 4 1 0 1 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1]\n",
      "action : 0\n",
      "new state: [3 4 2 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [3 4 2 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 0\n",
      "new state: [3 5 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "final state: [3 5 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "Episode : 95\n",
      "state : [3 5 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "action : 2\n",
      "new state: [3 5 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [3 5 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "action : 1\n",
      "new state: [3 4 2 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: 0.25\n",
      "epReward so far: -0.5\n",
      "state : [3 4 2 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 1\n",
      "new state: [3 3 2 0 0 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1]\n",
      "reward: -0.5\n",
      "epReward so far: -1.0\n",
      "state : [3 3 2 0 0 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1]\n",
      "action : 2\n",
      "new state: [4 3 2 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: -0.75\n",
      "epReward so far: -1.75\n",
      "state : [4 3 2 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 0\n",
      "new state: [5 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "reward: -0.75\n",
      "epReward so far: -2.5\n",
      "final state: [5 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Episode : 96\n",
      "state : [5 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "action : 0\n",
      "new state: [5 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [5 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "action : 2\n",
      "new state: [5 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [5 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "action : 1\n",
      "new state: [5 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "reward: 0.0\n",
      "epReward so far: -0.75\n",
      "state : [5 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "action : 0\n",
      "new state: [5 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [5 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "action : 0\n",
      "new state: [5 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "reward: -0.75\n",
      "epReward so far: -2.25\n",
      "final state: [5 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "Episode : 97\n",
      "state : [5 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "action : 1\n",
      "new state: [5 2 2 0 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [5 2 2 0 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 1\n",
      "new state: [5 2 2 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [5 2 2 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "action : 2\n",
      "new state: [6 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "state : [6 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "action : 0\n",
      "new state: [6 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "reward: -0.75\n",
      "epReward so far: -2.75\n",
      "state : [6 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "action : 0\n",
      "new state: [6 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "reward: -0.75\n",
      "epReward so far: -3.5\n",
      "final state: [6 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "Episode : 98\n",
      "state : [6 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "action : 0\n",
      "new state: [5 3 1 0 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [5 3 1 0 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 0\n",
      "new state: [4 4 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [4 4 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 1\n",
      "new state: [4 5 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "state : [4 5 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "action : 0\n",
      "new state: [4 5 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "reward: -0.75\n",
      "epReward so far: -2.75\n",
      "state : [4 5 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "action : 2\n",
      "new state: [4 5 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: -2.75\n",
      "final state: [4 5 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "Episode : 99\n",
      "state : [4 5 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "action : 0\n",
      "new state: [4 5 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [4 5 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "action : 0\n",
      "new state: [3 5 1 0 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [3 5 1 0 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 0\n",
      "new state: [3 5 1 0 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.5\n",
      "state : [3 5 1 0 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "action : 2\n",
      "new state: [3 5 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [3 5 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "action : 1\n",
      "new state: [3 5 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "final state: [3 5 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Episode : 0\n",
      "state : [3 5 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "action : 0\n",
      "new state: [3 5 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "reward: 0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 5 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "action : 1\n",
      "new state: [3 5 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [3 5 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "action : 1\n",
      "new state: [4 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [4 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "action : 0\n",
      "new state: [3 4 2 0 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "reward: 0.25\n",
      "epReward so far: -1.25\n",
      "state : [3 4 2 0 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "action : 2\n",
      "new state: [3 4 2 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "final state: [3 4 2 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "Episode : 1\n",
      "state : [3 4 2 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 0\n",
      "new state: [3 5 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [3 5 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "action : 2\n",
      "new state: [3 5 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [3 5 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "action : 3\n",
      "new state: [3 5 1 0 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "reward: -0.5\n",
      "epReward so far: -2.0\n",
      "state : [3 5 1 0 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "action : 1\n",
      "new state: [4 4 1 0 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: -0.75\n",
      "epReward so far: -2.75\n",
      "state : [4 4 1 0 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 0\n",
      "new state: [4 4 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: -3.5\n",
      "final state: [4 4 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "Episode : 2\n",
      "state : [4 4 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "action : 1\n",
      "new state: [4 4 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "reward: 0.0\n",
      "epReward so far: 0.0\n",
      "state : [4 4 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "action : 1\n",
      "new state: [4 4 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [4 4 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "action : 0\n",
      "new state: [3 4 1 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "reward: 0.25\n",
      "epReward so far: -0.5\n",
      "state : [3 4 1 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "action : 2\n",
      "new state: [3 4 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [3 4 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 0\n",
      "new state: [3 5 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "final state: [3 5 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "Episode : 3\n",
      "state : [3 5 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "action : 3\n",
      "new state: [3 5 1 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [3 5 1 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "action : 0\n",
      "new state: [2 5 1 0 0 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "reward: -0.5\n",
      "epReward so far: -1.0\n",
      "state : [2 5 1 0 0 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "action : 1\n",
      "new state: [3 4 1 0 0 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2]\n",
      "reward: -0.5\n",
      "epReward so far: -1.5\n",
      "state : [3 4 1 0 0 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2]\n",
      "action : 2\n",
      "new state: [4 4 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "reward: 0.0\n",
      "epReward so far: -1.5\n",
      "state : [4 4 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "action : 1\n",
      "new state: [6 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "reward: -0.75\n",
      "epReward so far: -2.25\n",
      "final state: [6 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "Episode : 4\n",
      "state : [6 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "action : 0\n",
      "new state: [5 3 1 0 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [5 3 1 0 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 1\n",
      "new state: [5 3 1 0 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [5 3 1 0 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 2\n",
      "new state: [5 3 0 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: -0.5\n",
      "epReward so far: -1.75\n",
      "state : [5 3 0 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 1\n",
      "new state: [5 3 0 1 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "reward: -0.0\n",
      "epReward so far: -1.75\n",
      "state : [5 3 0 1 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "action : 0\n",
      "new state: [4 3 1 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: 0.25\n",
      "epReward so far: -1.5\n",
      "final state: [4 3 1 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "Episode : 5\n",
      "state : [4 3 1 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 1\n",
      "new state: [4 2 1 1 1 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [4 2 1 1 1 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "action : 0\n",
      "new state: [4 3 1 1 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [4 3 1 1 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "action : 0\n",
      "new state: [3 3 1 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: 0.25\n",
      "epReward so far: -1.0\n",
      "state : [3 3 1 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 2\n",
      "new state: [3 3 0 2 1 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "reward: 0.25\n",
      "epReward so far: -0.75\n",
      "state : [3 3 0 2 1 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "action : 3\n",
      "new state: [3 4 0 1 0 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1]\n",
      "reward: -0.5\n",
      "epReward so far: -1.25\n",
      "final state: [3 4 0 1 0 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1]\n",
      "Episode : 6\n",
      "state : [3 4 0 1 0 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1]\n",
      "action : 1\n",
      "new state: [4 4 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [4 4 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 0\n",
      "new state: [4 4 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [4 4 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "action : 3\n",
      "new state: [4 4 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [4 4 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "action : 3\n",
      "new state: [4 4 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "reward: -0.75\n",
      "epReward so far: -2.25\n",
      "state : [4 4 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "action : 1\n",
      "new state: [4 3 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "reward: -0.75\n",
      "epReward so far: -3.0\n",
      "final state: [4 3 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "Episode : 7\n",
      "state : [4 3 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "action : 3\n",
      "new state: [4 3 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [4 3 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "action : 0\n",
      "new state: [4 3 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [4 3 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "action : 1\n",
      "new state: [4 3 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "reward: -0.75\n",
      "epReward so far: -2.25\n",
      "state : [4 3 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "action : 1\n",
      "new state: [4 3 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "reward: -0.75\n",
      "epReward so far: -3.0\n",
      "state : [4 3 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "action : 0\n",
      "new state: [3 3 0 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: -0.5\n",
      "epReward so far: -3.5\n",
      "final state: [3 3 0 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "Episode : 8\n",
      "state : [3 3 0 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 3\n",
      "new state: [3 3 0 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 3 0 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "action : 1\n",
      "new state: [3 3 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [3 3 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "action : 0\n",
      "new state: [3 3 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "reward: -0.0\n",
      "epReward so far: -0.75\n",
      "state : [3 3 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "action : 1\n",
      "new state: [3 2 0 4 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "reward: -0.5\n",
      "epReward so far: -1.25\n",
      "state : [3 2 0 4 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "action : 0\n",
      "new state: [2 2 0 4 3 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 0]\n",
      "reward: -0.5\n",
      "epReward so far: -1.75\n",
      "final state: [2 2 0 4 3 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 0]\n",
      "Episode : 9\n",
      "state : [2 2 0 4 3 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 0]\n",
      "action : 3\n",
      "new state: [2 2 0 4 0 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [2 2 0 4 0 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "action : 1\n",
      "new state: [3 2 0 4 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.25\n",
      "state : [3 2 0 4 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "action : 0\n",
      "new state: [3 2 0 4 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.5\n",
      "state : [3 2 0 4 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "action : 3\n",
      "new state: [3 2 0 3 1 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "reward: -0.5\n",
      "epReward so far: 0.0\n",
      "state : [3 2 0 3 1 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "action : 1\n",
      "new state: [3 2 0 3 0 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "final state: [3 2 0 3 0 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2]\n",
      "Episode : 10\n",
      "state : [3 2 0 3 0 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2]\n",
      "action : 0\n",
      "new state: [4 2 0 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [4 2 0 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "action : 3\n",
      "new state: [5 2 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "reward: -0.0\n",
      "epReward so far: -0.75\n",
      "state : [5 2 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "action : 3\n",
      "new state: [5 2 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [5 2 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "action : 0\n",
      "new state: [4 2 0 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: 0.25\n",
      "epReward so far: -1.25\n",
      "state : [4 2 0 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 0\n",
      "new state: [3 3 0 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "final state: [3 3 0 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "Episode : 11\n",
      "state : [3 3 0 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "action : 1\n",
      "new state: [3 3 0 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [3 3 0 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 3\n",
      "new state: [3 3 0 2 3 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 3]\n",
      "reward: -0.5\n",
      "epReward so far: -0.25\n",
      "state : [3 3 0 2 3 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 3]\n",
      "action : 3\n",
      "new state: [3 3 0 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.0\n",
      "state : [3 3 0 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 3\n",
      "new state: [3 3 1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "reward: -0.0\n",
      "epReward so far: -1.0\n",
      "state : [3 3 1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "action : 3\n",
      "new state: [3 3 1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "reward: -0.0\n",
      "epReward so far: -1.0\n",
      "final state: [3 3 1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "Episode : 12\n",
      "state : [3 3 1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "action : 0\n",
      "new state: [2 3 1 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [2 3 1 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 0\n",
      "new state: [2 3 1 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.5\n",
      "state : [2 3 1 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "action : 0\n",
      "new state: [1 3 1 4 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: 0.25\n",
      "epReward so far: -0.25\n",
      "state : [1 3 1 4 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 3\n",
      "new state: [1 3 1 3 1 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "reward: -0.5\n",
      "epReward so far: -0.75\n",
      "state : [1 3 1 3 1 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "action : 0\n",
      "new state: [1 4 1 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: -0.0\n",
      "epReward so far: -0.75\n",
      "final state: [1 4 1 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "Episode : 13\n",
      "state : [1 4 1 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 3\n",
      "new state: [1 4 1 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [1 4 1 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 1\n",
      "new state: [1 3 1 3 0 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "reward: -0.5\n",
      "epReward so far: -1.0\n",
      "state : [1 3 1 3 0 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "action : 3\n",
      "new state: [2 3 1 2 1 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 1]\n",
      "reward: 0.25\n",
      "epReward so far: -0.75\n",
      "state : [2 3 1 2 1 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 1]\n",
      "action : 0\n",
      "new state: [1 4 1 2 1 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 0]\n",
      "reward: -0.5\n",
      "epReward so far: -1.25\n",
      "state : [1 4 1 2 1 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 0]\n",
      "action : 1\n",
      "new state: [1 5 1 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "final state: [1 5 1 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "Episode : 14\n",
      "state : [1 5 1 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "action : 1\n",
      "new state: [1 6 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [1 6 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "action : 1\n",
      "new state: [1 6 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [1 6 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "action : 1\n",
      "new state: [1 5 1 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: -0.5\n",
      "epReward so far: -2.0\n",
      "state : [1 5 1 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 1\n",
      "new state: [1 4 1 2 0 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "reward: -0.5\n",
      "epReward so far: -2.5\n",
      "state : [1 4 1 2 0 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "action : 3\n",
      "new state: [2 4 1 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "reward: -0.0\n",
      "epReward so far: -2.5\n",
      "final state: [2 4 1 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "Episode : 15\n",
      "state : [2 4 1 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "action : 0\n",
      "new state: [1 4 2 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [1 4 2 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 3\n",
      "new state: [1 4 2 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [1 4 2 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 3\n",
      "new state: [2 4 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "reward: -0.0\n",
      "epReward so far: -1.25\n",
      "state : [2 4 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "action : 3\n",
      "new state: [2 4 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "state : [2 4 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "action : 1\n",
      "new state: [2 4 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "reward: -0.75\n",
      "epReward so far: -2.75\n",
      "final state: [2 4 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "Episode : 16\n",
      "state : [2 4 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "action : 3\n",
      "new state: [2 4 2 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [2 4 2 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 1\n",
      "new state: [2 3 3 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "reward: -0.75\n",
      "epReward so far: -0.5\n",
      "state : [2 3 3 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "action : 3\n",
      "new state: [3 3 3 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: -0.5\n",
      "epReward so far: -1.0\n",
      "state : [3 3 3 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 2\n",
      "new state: [3 3 3 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.75\n",
      "state : [3 3 3 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 1\n",
      "new state: [4 2 3 0 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "reward: -0.5\n",
      "epReward so far: -2.25\n",
      "final state: [4 2 3 0 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "Episode : 17\n",
      "state : [4 2 3 0 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "action : 2\n",
      "new state: [4 2 3 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [4 2 3 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 0\n",
      "new state: [3 3 3 0 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "reward: 0.25\n",
      "epReward so far: -0.5\n",
      "state : [3 3 3 0 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "action : 2\n",
      "new state: [3 3 3 0 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [3 3 3 0 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "action : 1\n",
      "new state: [3 2 3 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "reward: 0.25\n",
      "epReward so far: -1.0\n",
      "state : [3 2 3 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "action : 0\n",
      "new state: [3 2 3 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.75\n",
      "final state: [3 2 3 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "Episode : 18\n",
      "state : [3 2 3 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 2\n",
      "new state: [4 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [4 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "action : 0\n",
      "new state: [3 2 3 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: -0.5\n",
      "epReward so far: -1.25\n",
      "state : [3 2 3 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 2\n",
      "new state: [3 2 3 1 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "state : [3 2 3 1 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "action : 1\n",
      "new state: [4 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "reward: -0.75\n",
      "epReward so far: -2.75\n",
      "state : [4 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "action : 1\n",
      "new state: [4 0 3 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "reward: -0.5\n",
      "epReward so far: -3.25\n",
      "final state: [4 0 3 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "Episode : 19\n",
      "state : [4 0 3 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "action : 3\n",
      "new state: [4 0 3 1 2 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [4 0 3 1 2 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "action : 0\n",
      "new state: [3 0 4 1 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 0]\n",
      "reward: -0.5\n",
      "epReward so far: -1.0\n",
      "state : [3 0 4 1 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 0]\n",
      "action : 2\n",
      "new state: [3 1 3 1 2 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 1]\n",
      "reward: -0.5\n",
      "epReward so far: -1.5\n",
      "state : [3 1 3 1 2 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 1]\n",
      "action : 2\n",
      "new state: [3 1 4 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: -0.75\n",
      "epReward so far: -2.25\n",
      "state : [3 1 4 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 3\n",
      "new state: [4 1 4 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "reward: -0.5\n",
      "epReward so far: -2.75\n",
      "final state: [4 1 4 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "Episode : 20\n",
      "state : [4 1 4 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "action : 2\n",
      "new state: [4 1 4 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [4 1 4 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 2\n",
      "new state: [5 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [5 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "action : 0\n",
      "new state: [5 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "reward: -0.75\n",
      "epReward so far: -2.25\n",
      "state : [5 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "action : 0\n",
      "new state: [4 2 3 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: -0.5\n",
      "epReward so far: -2.75\n",
      "state : [4 2 3 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 1\n",
      "new state: [4 2 3 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: -3.5\n",
      "final state: [4 2 3 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "Episode : 21\n",
      "state : [4 2 3 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 1\n",
      "new state: [5 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "reward: 0.0\n",
      "epReward so far: 0.0\n",
      "state : [5 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "action : 0\n",
      "new state: [4 2 3 0 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [4 2 3 0 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 1\n",
      "new state: [4 1 3 0 3 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "reward: -0.5\n",
      "epReward so far: -1.0\n",
      "state : [4 1 3 0 3 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "action : 2\n",
      "new state: [4 1 3 1 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.75\n",
      "state : [4 1 3 1 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 2\n",
      "new state: [4 1 4 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: -2.5\n",
      "final state: [4 1 4 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "Episode : 22\n",
      "state : [4 1 4 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "action : 2\n",
      "new state: [4 1 4 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [4 1 4 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "action : 0\n",
      "new state: [3 1 4 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: 0.25\n",
      "epReward so far: -0.5\n",
      "state : [3 1 4 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 2\n",
      "new state: [3 1 4 1 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [3 1 4 1 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 2\n",
      "new state: [3 1 5 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "reward: 0.0\n",
      "epReward so far: -1.25\n",
      "state : [3 1 5 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "action : 2\n",
      "new state: [3 1 4 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: -0.5\n",
      "epReward so far: -1.75\n",
      "final state: [3 1 4 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "Episode : 23\n",
      "state : [3 1 4 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 2\n",
      "new state: [3 1 4 1 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [3 1 4 1 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 2\n",
      "new state: [3 1 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [3 1 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "action : 3\n",
      "new state: [3 1 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: -2.25\n",
      "state : [3 1 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "action : 3\n",
      "new state: [3 1 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "reward: -0.75\n",
      "epReward so far: -3.0\n",
      "state : [3 1 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "action : 0\n",
      "new state: [3 1 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "reward: -0.0\n",
      "epReward so far: -3.0\n",
      "final state: [3 1 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "Episode : 24\n",
      "state : [3 1 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "action : 1\n",
      "new state: [3 0 4 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [3 0 4 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 3\n",
      "new state: [3 0 4 1 1 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 3]\n",
      "reward: 0.25\n",
      "epReward so far: -0.25\n",
      "state : [3 0 4 1 1 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 3]\n",
      "action : 2\n",
      "new state: [3 1 4 1 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: -0.75\n",
      "epReward so far: -1.0\n",
      "state : [3 1 4 1 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 2\n",
      "new state: [3 1 5 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "reward: -0.0\n",
      "epReward so far: -1.0\n",
      "state : [3 1 5 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "action : 2\n",
      "new state: [3 1 4 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: -0.5\n",
      "epReward so far: -1.5\n",
      "final state: [3 1 4 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "Episode : 25\n",
      "state : [3 1 4 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 2\n",
      "new state: [3 1 3 1 1 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [3 1 3 1 1 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "action : 2\n",
      "new state: [3 2 2 1 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 3]\n",
      "reward: -0.5\n",
      "epReward so far: -1.0\n",
      "state : [3 2 2 1 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 3]\n",
      "action : 0\n",
      "new state: [3 2 2 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: -0.75\n",
      "epReward so far: -1.75\n",
      "state : [3 2 2 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 2\n",
      "new state: [3 2 2 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "reward: 0.25\n",
      "epReward so far: -1.5\n",
      "state : [3 2 2 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "action : 2\n",
      "new state: [4 2 1 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "reward: -0.75\n",
      "epReward so far: -2.25\n",
      "final state: [4 2 1 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "Episode : 26\n",
      "state : [4 2 1 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "action : 3\n",
      "new state: [6 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [6 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "action : 0\n",
      "new state: [5 2 1 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: 0.25\n",
      "epReward so far: -0.5\n",
      "state : [5 2 1 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 0\n",
      "new state: [4 3 1 1 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [4 3 1 1 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "action : 3\n",
      "new state: [4 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "state : [4 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "action : 2\n",
      "new state: [4 3 1 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: 0.25\n",
      "epReward so far: -1.75\n",
      "final state: [4 3 1 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "Episode : 27\n",
      "state : [4 3 1 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 1\n",
      "new state: [4 2 1 1 3 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 3]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [4 2 1 1 3 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 3]\n",
      "action : 0\n",
      "new state: [4 2 1 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [4 2 1 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 0\n",
      "new state: [3 2 1 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: 0.25\n",
      "epReward so far: -1.0\n",
      "state : [3 2 1 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 1\n",
      "new state: [3 2 1 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.75\n",
      "state : [3 2 1 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 2\n",
      "new state: [3 2 1 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "reward: -0.75\n",
      "epReward so far: -2.5\n",
      "final state: [3 2 1 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "Episode : 28\n",
      "state : [3 2 1 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "action : 3\n",
      "new state: [3 2 1 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 2 1 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "action : 1\n",
      "new state: [3 1 1 4 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [3 1 1 4 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 3\n",
      "new state: [3 1 1 4 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [3 1 1 4 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 3\n",
      "new state: [4 2 1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "state : [4 2 1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "action : 0\n",
      "new state: [4 2 1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "reward: -0.75\n",
      "epReward so far: -2.75\n",
      "final state: [4 2 1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "Episode : 29\n",
      "state : [4 2 1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "action : 1\n",
      "new state: [4 1 1 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [4 1 1 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 2\n",
      "new state: [4 2 0 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [4 2 0 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 0\n",
      "new state: [5 2 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "state : [5 2 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "action : 1\n",
      "new state: [5 2 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "reward: -0.75\n",
      "epReward so far: -2.75\n",
      "state : [5 2 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "action : 1\n",
      "new state: [5 1 0 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: -0.5\n",
      "epReward so far: -3.25\n",
      "final state: [5 1 0 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "Episode : 30\n",
      "state : [5 1 0 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 0\n",
      "new state: [4 1 0 3 0 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [4 1 0 3 0 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "action : 3\n",
      "new state: [5 1 0 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [5 1 0 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "action : 0\n",
      "new state: [6 1 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "reward: 0.0\n",
      "epReward so far: -1.25\n",
      "state : [6 1 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "action : 0\n",
      "new state: [5 1 0 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: -0.5\n",
      "epReward so far: -1.75\n",
      "state : [5 1 0 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 3\n",
      "new state: [5 1 0 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "reward: -0.0\n",
      "epReward so far: -1.75\n",
      "final state: [5 1 0 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "Episode : 31\n",
      "state : [5 1 0 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "action : 3\n",
      "new state: [6 1 0 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [6 1 0 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "action : 0\n",
      "new state: [5 1 0 2 3 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "reward: -0.5\n",
      "epReward so far: -1.0\n",
      "state : [5 1 0 2 3 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "action : 0\n",
      "new state: [4 1 0 3 2 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 3]\n",
      "reward: 0.25\n",
      "epReward so far: -0.75\n",
      "state : [4 1 0 3 2 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 3]\n",
      "action : 0\n",
      "new state: [5 1 0 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [5 1 0 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "action : 0\n",
      "new state: [4 1 1 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: -0.5\n",
      "epReward so far: -2.0\n",
      "final state: [4 1 1 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "Episode : 32\n",
      "state : [4 1 1 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 0\n",
      "new state: [3 1 1 3 0 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 2]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [3 1 1 3 0 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 2]\n",
      "action : 3\n",
      "new state: [4 1 1 2 2 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 2]\n",
      "reward: 0.25\n",
      "epReward so far: -0.25\n",
      "state : [4 1 1 2 2 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 2]\n",
      "action : 3\n",
      "new state: [4 1 2 1 2 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.0\n",
      "state : [4 1 2 1 2 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 1]\n",
      "action : 0\n",
      "new state: [4 1 3 1 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [4 1 3 1 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 2\n",
      "new state: [4 1 3 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "final state: [4 1 3 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "Episode : 33\n",
      "state : [4 1 3 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 1\n",
      "new state: [4 1 3 1 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [4 1 3 1 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "action : 0\n",
      "new state: [4 1 4 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "reward: -0.0\n",
      "epReward so far: -0.75\n",
      "state : [4 1 4 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "action : 2\n",
      "new state: [4 1 3 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: -0.5\n",
      "epReward so far: -1.25\n",
      "state : [4 1 3 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 0\n",
      "new state: [3 1 3 1 3 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 0]\n",
      "reward: 0.25\n",
      "epReward so far: -1.0\n",
      "state : [3 1 3 1 3 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 0]\n",
      "action : 0\n",
      "new state: [2 1 3 2 0 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 3]\n",
      "reward: -0.5\n",
      "epReward so far: -1.5\n",
      "final state: [2 1 3 2 0 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 3]\n",
      "Episode : 34\n",
      "state : [2 1 3 2 0 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 3]\n",
      "action : 3\n",
      "new state: [2 1 3 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 1 3 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 0\n",
      "new state: [3 1 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [3 1 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "action : 0\n",
      "new state: [2 1 3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "reward: -0.5\n",
      "epReward so far: -1.25\n",
      "state : [2 1 3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "action : 0\n",
      "new state: [2 1 3 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "reward: -0.0\n",
      "epReward so far: -1.25\n",
      "state : [2 1 3 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "action : 3\n",
      "new state: [2 1 4 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: -0.5\n",
      "epReward so far: -1.75\n",
      "final state: [2 1 4 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "Episode : 35\n",
      "state : [2 1 4 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 2\n",
      "new state: [2 1 3 2 2 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [2 1 3 2 2 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "action : 1\n",
      "new state: [2 1 4 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [2 1 4 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "action : 2\n",
      "new state: [2 1 4 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: -0.5\n",
      "epReward so far: -1.75\n",
      "state : [2 1 4 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 2\n",
      "new state: [2 1 4 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: -0.75\n",
      "epReward so far: -2.5\n",
      "state : [2 1 4 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 1\n",
      "new state: [3 1 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "reward: -0.75\n",
      "epReward so far: -3.25\n",
      "final state: [3 1 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "Episode : 36\n",
      "state : [3 1 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "action : 2\n",
      "new state: [3 1 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [3 1 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "action : 3\n",
      "new state: [3 1 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [3 1 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "action : 1\n",
      "new state: [3 1 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "reward: -0.75\n",
      "epReward so far: -2.25\n",
      "state : [3 1 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "action : 3\n",
      "new state: [3 1 4 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: -0.5\n",
      "epReward so far: -2.75\n",
      "state : [3 1 4 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 0\n",
      "new state: [3 1 4 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: -0.75\n",
      "epReward so far: -3.5\n",
      "final state: [3 1 4 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "Episode : 37\n",
      "state : [3 1 4 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 0\n",
      "new state: [2 2 5 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 2 5 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "action : 1\n",
      "new state: [2 1 5 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "reward: 0.25\n",
      "epReward so far: -0.5\n",
      "state : [2 1 5 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "action : 0\n",
      "new state: [1 1 5 1 0 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 3]\n",
      "reward: -0.5\n",
      "epReward so far: -1.0\n",
      "state : [1 1 5 1 0 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 3]\n",
      "action : 1\n",
      "new state: [2 1 5 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "reward: -0.75\n",
      "epReward so far: -1.75\n",
      "state : [2 1 5 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "action : 1\n",
      "new state: [3 1 5 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "reward: -0.75\n",
      "epReward so far: -2.5\n",
      "final state: [3 1 5 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "Episode : 38\n",
      "state : [3 1 5 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "action : 1\n",
      "new state: [3 1 5 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [3 1 5 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "action : 2\n",
      "new state: [3 1 4 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: 0.25\n",
      "epReward so far: -0.5\n",
      "state : [3 1 4 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 0\n",
      "new state: [3 1 4 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [3 1 4 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 0\n",
      "new state: [2 2 4 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: -0.5\n",
      "epReward so far: -1.75\n",
      "state : [2 2 4 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 0\n",
      "new state: [2 2 4 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "reward: -0.75\n",
      "epReward so far: -2.5\n",
      "final state: [2 2 4 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "Episode : 39\n",
      "state : [2 2 4 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "action : 2\n",
      "new state: [2 3 3 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [2 3 3 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 1\n",
      "new state: [2 2 3 1 1 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "reward: -0.5\n",
      "epReward so far: -0.25\n",
      "state : [2 2 3 1 1 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "action : 0\n",
      "new state: [2 3 3 1 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: -0.75\n",
      "epReward so far: -1.0\n",
      "state : [2 3 3 1 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 0\n",
      "new state: [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.75\n",
      "state : [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "action : 2\n",
      "new state: [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "reward: -0.75\n",
      "epReward so far: -2.5\n",
      "final state: [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "Episode : 40\n",
      "state : [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "action : 1\n",
      "new state: [2 2 3 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [2 2 3 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 3\n",
      "new state: [2 2 3 1 1 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "reward: -0.5\n",
      "epReward so far: -1.0\n",
      "state : [2 2 3 1 1 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "action : 3\n",
      "new state: [2 3 3 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: -0.0\n",
      "epReward so far: -1.0\n",
      "state : [2 3 3 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 1\n",
      "new state: [3 2 3 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: -0.5\n",
      "epReward so far: -1.5\n",
      "state : [3 2 3 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 2\n",
      "new state: [3 2 3 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: 0.0\n",
      "epReward so far: -1.5\n",
      "final state: [3 2 3 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "Episode : 41\n",
      "state : [3 2 3 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 1\n",
      "new state: [4 1 4 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [4 1 4 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "action : 1\n",
      "new state: [4 0 4 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: -0.5\n",
      "epReward so far: -1.25\n",
      "state : [4 0 4 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 0\n",
      "new state: [3 0 4 1 2 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1]\n",
      "reward: -0.5\n",
      "epReward so far: -1.75\n",
      "state : [3 0 4 1 2 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1]\n",
      "action : 0\n",
      "new state: [3 0 5 1 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "reward: -0.75\n",
      "epReward so far: -2.5\n",
      "state : [3 0 5 1 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "action : 2\n",
      "new state: [3 0 4 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: -0.5\n",
      "epReward so far: -3.0\n",
      "final state: [3 0 4 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "Episode : 42\n",
      "state : [3 0 4 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 3\n",
      "new state: [3 1 4 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [3 1 4 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 1\n",
      "new state: [4 0 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [4 0 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "action : 0\n",
      "new state: [4 0 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "reward: -0.75\n",
      "epReward so far: -2.25\n",
      "state : [4 0 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "action : 2\n",
      "new state: [4 0 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "reward: -0.75\n",
      "epReward so far: -3.0\n",
      "state : [4 0 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "action : 2\n",
      "new state: [4 0 3 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "reward: -0.5\n",
      "epReward so far: -3.5\n",
      "final state: [4 0 3 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "Episode : 43\n",
      "state : [4 0 3 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "action : 0\n",
      "new state: [4 0 3 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [4 0 3 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 0\n",
      "new state: [4 0 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [4 0 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "action : 2\n",
      "new state: [4 0 3 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: -0.5\n",
      "epReward so far: -2.0\n",
      "state : [4 0 3 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 2\n",
      "new state: [4 0 3 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "reward: 0.0\n",
      "epReward so far: -2.0\n",
      "state : [4 0 3 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "action : 2\n",
      "new state: [5 0 2 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "reward: 0.25\n",
      "epReward so far: -1.75\n",
      "final state: [5 0 2 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "Episode : 44\n",
      "state : [5 0 2 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "action : 0\n",
      "new state: [5 0 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [5 0 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 2\n",
      "new state: [5 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [5 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "action : 0\n",
      "new state: [4 1 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "reward: -0.5\n",
      "epReward so far: -2.0\n",
      "state : [4 1 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "action : 0\n",
      "new state: [3 1 2 2 2 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "reward: -0.5\n",
      "epReward so far: -2.5\n",
      "state : [3 1 2 2 2 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "action : 2\n",
      "new state: [3 1 3 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "reward: -0.75\n",
      "epReward so far: -3.25\n",
      "final state: [3 1 3 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "Episode : 45\n",
      "state : [3 1 3 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "action : 0\n",
      "new state: [4 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "reward: 0.0\n",
      "epReward so far: 0.0\n",
      "state : [4 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "action : 0\n",
      "new state: [3 1 3 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [3 1 3 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "action : 2\n",
      "new state: [3 1 3 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [3 1 3 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 2\n",
      "new state: [4 1 2 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "reward: 0.25\n",
      "epReward so far: -1.0\n",
      "state : [4 1 2 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "action : 2\n",
      "new state: [4 1 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.75\n",
      "final state: [4 1 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "Episode : 46\n",
      "state : [4 1 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 0\n",
      "new state: [4 1 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [4 1 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "action : 1\n",
      "new state: [4 1 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [4 1 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "action : 0\n",
      "new state: [3 1 2 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: -0.5\n",
      "epReward so far: -2.0\n",
      "state : [3 1 2 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 2\n",
      "new state: [3 1 1 3 0 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "reward: -0.5\n",
      "epReward so far: -2.5\n",
      "state : [3 1 1 3 0 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "action : 3\n",
      "new state: [4 1 1 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: -0.75\n",
      "epReward so far: -3.25\n",
      "final state: [4 1 1 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "Episode : 47\n",
      "state : [4 1 1 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 0\n",
      "new state: [3 1 1 4 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [3 1 1 4 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 2\n",
      "new state: [3 1 0 4 0 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 3]\n",
      "reward: -0.5\n",
      "epReward so far: -1.0\n",
      "state : [3 1 0 4 0 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 3]\n",
      "action : 3\n",
      "new state: [4 1 0 4 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.75\n",
      "state : [4 1 0 4 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "action : 3\n",
      "new state: [4 2 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "reward: -0.75\n",
      "epReward so far: -2.5\n",
      "state : [4 2 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "action : 0\n",
      "new state: [4 2 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "reward: 0.0\n",
      "epReward so far: -2.5\n",
      "final state: [4 2 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "Episode : 48\n",
      "state : [4 2 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "action : 3\n",
      "new state: [4 2 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [4 2 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "action : 1\n",
      "new state: [4 1 0 4 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: -0.5\n",
      "epReward so far: -1.25\n",
      "state : [4 1 0 4 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 0\n",
      "new state: [3 1 0 4 1 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 1]\n",
      "reward: 0.25\n",
      "epReward so far: -1.0\n",
      "state : [3 1 0 4 1 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 1]\n",
      "action : 3\n",
      "new state: [3 2 0 3 1 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0]\n",
      "reward: -0.5\n",
      "epReward so far: -1.5\n",
      "state : [3 2 0 3 1 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0]\n",
      "action : 1\n",
      "new state: [3 1 0 4 1 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "reward: 0.25\n",
      "epReward so far: -1.25\n",
      "final state: [3 1 0 4 1 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "Episode : 49\n",
      "state : [3 1 0 4 1 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "action : 0\n",
      "new state: [2 2 0 4 2 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [2 2 0 4 2 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "action : 0\n",
      "new state: [3 2 0 4 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [3 2 0 4 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "action : 1\n",
      "new state: [3 2 1 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "state : [3 2 1 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "action : 0\n",
      "new state: [3 2 1 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "reward: -0.75\n",
      "epReward so far: -2.75\n",
      "state : [3 2 1 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "action : 3\n",
      "new state: [3 2 1 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: -0.5\n",
      "epReward so far: -3.25\n",
      "final state: [3 2 1 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "Episode : 50\n",
      "state : [3 2 1 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 3\n",
      "new state: [3 2 1 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [3 2 1 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 0\n",
      "new state: [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "action : 1\n",
      "new state: [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: -2.25\n",
      "state : [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "action : 2\n",
      "new state: [3 3 1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "reward: -0.75\n",
      "epReward so far: -3.0\n",
      "state : [3 3 1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "action : 1\n",
      "new state: [3 2 1 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "reward: -0.5\n",
      "epReward so far: -3.5\n",
      "final state: [3 2 1 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "Episode : 51\n",
      "state : [3 2 1 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "action : 0\n",
      "new state: [2 2 1 3 1 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [2 2 1 3 1 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0]\n",
      "action : 1\n",
      "new state: [2 2 1 3 0 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "reward: 0.25\n",
      "epReward so far: -0.25\n",
      "state : [2 2 1 3 0 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "action : 3\n",
      "new state: [2 2 2 2 0 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2]\n",
      "reward: -0.5\n",
      "epReward so far: -0.75\n",
      "state : [2 2 2 2 0 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2]\n",
      "action : 0\n",
      "new state: [2 2 3 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [2 2 3 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "action : 2\n",
      "new state: [3 2 2 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "reward: -0.5\n",
      "epReward so far: -2.0\n",
      "final state: [3 2 2 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "Episode : 52\n",
      "state : [3 2 2 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "action : 2\n",
      "new state: [3 2 1 2 0 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 1]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [3 2 1 2 0 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 1]\n",
      "action : 2\n",
      "new state: [4 2 0 2 1 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 3]\n",
      "reward: -0.5\n",
      "epReward so far: -1.0\n",
      "state : [4 2 0 2 1 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 3]\n",
      "action : 3\n",
      "new state: [5 2 0 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.75\n",
      "state : [5 2 0 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "action : 3\n",
      "new state: [5 3 0 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: -0.5\n",
      "epReward so far: -2.25\n",
      "state : [5 3 0 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 1\n",
      "new state: [5 3 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: -0.75\n",
      "epReward so far: -3.0\n",
      "final state: [5 3 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "Episode : 53\n",
      "state : [5 3 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 0\n",
      "new state: [4 4 0 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [4 4 0 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 1\n",
      "new state: [4 4 0 1 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: -0.75\n",
      "epReward so far: -0.5\n",
      "state : [4 4 0 1 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 1\n",
      "new state: [4 3 1 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "reward: -0.5\n",
      "epReward so far: -1.0\n",
      "state : [4 3 1 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "action : 1\n",
      "new state: [4 2 1 1 3 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 3]\n",
      "reward: 0.25\n",
      "epReward so far: -0.75\n",
      "state : [4 2 1 1 3 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 3]\n",
      "action : 1\n",
      "new state: [4 2 1 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "final state: [4 2 1 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "Episode : 54\n",
      "state : [4 2 1 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 1\n",
      "new state: [4 1 1 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [4 1 1 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "action : 0\n",
      "new state: [4 1 1 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [4 1 1 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 3\n",
      "new state: [5 1 1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "state : [5 1 1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "action : 2\n",
      "new state: [5 1 0 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: 0.25\n",
      "epReward so far: -1.75\n",
      "state : [5 1 0 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 3\n",
      "new state: [5 1 0 2 3 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0]\n",
      "reward: 0.25\n",
      "epReward so far: -1.5\n",
      "final state: [5 1 0 2 3 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0]\n",
      "Episode : 55\n",
      "state : [5 1 0 2 3 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0]\n",
      "action : 0\n",
      "new state: [4 1 0 3 0 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 1]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [4 1 0 3 0 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 1]\n",
      "action : 1\n",
      "new state: [4 1 1 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [4 1 1 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 2\n",
      "new state: [5 1 1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "state : [5 1 1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "action : 2\n",
      "new state: [5 2 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "reward: -0.75\n",
      "epReward so far: -2.75\n",
      "state : [5 2 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "action : 3\n",
      "new state: [5 2 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "reward: -0.75\n",
      "epReward so far: -3.5\n",
      "final state: [5 2 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "Episode : 56\n",
      "state : [5 2 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "action : 3\n",
      "new state: [5 2 0 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [5 2 0 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 0\n",
      "new state: [4 2 0 2 1 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "reward: -0.5\n",
      "epReward so far: -0.25\n",
      "state : [4 2 0 2 1 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "action : 3\n",
      "new state: [5 3 0 1 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.0\n",
      "state : [5 3 0 1 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "action : 1\n",
      "new state: [5 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.75\n",
      "state : [5 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "action : 0\n",
      "new state: [4 3 1 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "reward: -0.5\n",
      "epReward so far: -2.25\n",
      "final state: [4 3 1 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "Episode : 57\n",
      "state : [4 3 1 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "action : 0\n",
      "new state: [4 3 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [4 3 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 1\n",
      "new state: [4 3 1 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: -0.5\n",
      "epReward so far: -1.25\n",
      "state : [4 3 1 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 0\n",
      "new state: [4 3 1 1 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "state : [4 3 1 1 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 1\n",
      "new state: [4 2 1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "reward: -0.75\n",
      "epReward so far: -2.75\n",
      "state : [4 2 1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "action : 1\n",
      "new state: [4 1 1 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "reward: -0.5\n",
      "epReward so far: -3.25\n",
      "final state: [4 1 1 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "Episode : 58\n",
      "state : [4 1 1 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "action : 0\n",
      "new state: [3 1 1 3 0 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [3 1 1 3 0 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2]\n",
      "action : 0\n",
      "new state: [3 1 2 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.5\n",
      "state : [3 1 2 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "action : 0\n",
      "new state: [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "action : 0\n",
      "new state: [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "state : [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "action : 2\n",
      "new state: [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "reward: 0.0\n",
      "epReward so far: -2.0\n",
      "final state: [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "Episode : 59\n",
      "state : [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "action : 1\n",
      "new state: [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "action : 0\n",
      "new state: [2 2 2 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "reward: -0.5\n",
      "epReward so far: -1.25\n",
      "state : [2 2 2 3 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "action : 0\n",
      "new state: [2 2 2 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "reward: 0.0\n",
      "epReward so far: -1.25\n",
      "state : [2 2 2 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "action : 0\n",
      "new state: [2 2 2 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "reward: -0.5\n",
      "epReward so far: -1.75\n",
      "state : [2 2 2 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "action : 0\n",
      "new state: [2 2 2 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: -0.75\n",
      "epReward so far: -2.5\n",
      "final state: [2 2 2 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "Episode : 60\n",
      "state : [2 2 2 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 3\n",
      "new state: [2 3 2 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [2 3 2 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 3\n",
      "new state: [2 3 2 1 1 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2]\n",
      "reward: -0.5\n",
      "epReward so far: -0.25\n",
      "state : [2 3 2 1 1 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2]\n",
      "action : 3\n",
      "new state: [2 4 3 0 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "reward: -0.75\n",
      "epReward so far: -1.0\n",
      "state : [2 4 3 0 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "action : 0\n",
      "new state: [1 4 4 0 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: -0.5\n",
      "epReward so far: -1.5\n",
      "state : [1 4 4 0 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 1\n",
      "new state: [1 3 5 0 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: -2.25\n",
      "final state: [1 3 5 0 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "Episode : 61\n",
      "state : [1 3 5 0 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 1\n",
      "new state: [1 3 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 3 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "action : 1\n",
      "new state: [2 2 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 2 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "action : 0\n",
      "new state: [2 2 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [2 2 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "action : 1\n",
      "new state: [2 1 6 0 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: -0.5\n",
      "epReward so far: -2.0\n",
      "state : [2 1 6 0 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 2\n",
      "new state: [2 1 5 0 3 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 0]\n",
      "reward: -0.5\n",
      "epReward so far: -2.5\n",
      "final state: [2 1 5 0 3 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 0]\n",
      "Episode : 62\n",
      "state : [2 1 5 0 3 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 0]\n",
      "action : 2\n",
      "new state: [2 1 4 1 0 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [2 1 4 1 0 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "action : 0\n",
      "new state: [1 1 5 1 0 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "reward: -0.5\n",
      "epReward so far: -1.0\n",
      "state : [1 1 5 1 0 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "action : 2\n",
      "new state: [3 1 4 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.75\n",
      "state : [3 1 4 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 3\n",
      "new state: [3 2 4 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "reward: -0.75\n",
      "epReward so far: -2.5\n",
      "state : [3 2 4 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "action : 0\n",
      "new state: [2 2 4 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: -0.5\n",
      "epReward so far: -3.0\n",
      "final state: [2 2 4 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "Episode : 63\n",
      "state : [2 2 4 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 2\n",
      "new state: [2 2 4 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 2 4 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "action : 2\n",
      "new state: [2 3 3 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "reward: -0.5\n",
      "epReward so far: -1.25\n",
      "state : [2 3 3 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "action : 2\n",
      "new state: [3 3 2 1 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "state : [3 3 2 1 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 0\n",
      "new state: [2 3 3 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: 0.25\n",
      "epReward so far: -1.75\n",
      "state : [2 3 3 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 3\n",
      "new state: [2 3 3 1 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "reward: -0.0\n",
      "epReward so far: -1.75\n",
      "final state: [2 3 3 1 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "Episode : 64\n",
      "state : [2 3 3 1 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "action : 1\n",
      "new state: [2 2 3 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [2 2 3 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 1\n",
      "new state: [2 2 3 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [2 2 3 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 0\n",
      "new state: [2 2 3 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "reward: -0.5\n",
      "epReward so far: -1.75\n",
      "state : [2 2 3 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "action : 2\n",
      "new state: [2 2 3 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "reward: -0.75\n",
      "epReward so far: -2.5\n",
      "state : [2 2 3 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "action : 1\n",
      "new state: [2 2 3 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "reward: -0.5\n",
      "epReward so far: -3.0\n",
      "final state: [2 2 3 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "Episode : 65\n",
      "state : [2 2 3 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "action : 0\n",
      "new state: [2 2 3 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 2 3 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 1\n",
      "new state: [2 2 3 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "reward: -0.5\n",
      "epReward so far: -1.25\n",
      "state : [2 2 3 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "action : 2\n",
      "new state: [2 2 2 2 2 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "reward: 0.25\n",
      "epReward so far: -1.0\n",
      "state : [2 2 2 2 2 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "action : 1\n",
      "new state: [2 2 3 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.75\n",
      "state : [2 2 3 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 0\n",
      "new state: [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "reward: -0.75\n",
      "epReward so far: -2.5\n",
      "final state: [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "Episode : 66\n",
      "state : [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "action : 2\n",
      "new state: [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "action : 1\n",
      "new state: [2 2 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [2 2 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "action : 2\n",
      "new state: [2 2 3 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "reward: -0.5\n",
      "epReward so far: -2.0\n",
      "state : [2 2 3 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "action : 1\n",
      "new state: [2 1 3 2 2 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 3]\n",
      "reward: -0.5\n",
      "epReward so far: -2.5\n",
      "state : [2 1 3 2 2 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 3]\n",
      "action : 2\n",
      "new state: [2 1 4 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: -0.75\n",
      "epReward so far: -3.25\n",
      "final state: [2 1 4 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "Episode : 67\n",
      "state : [2 1 4 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 2\n",
      "new state: [3 1 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [3 1 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "action : 2\n",
      "new state: [3 1 3 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: 0.25\n",
      "epReward so far: -0.5\n",
      "state : [3 1 3 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 0\n",
      "new state: [2 2 3 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [2 2 3 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "action : 1\n",
      "new state: [3 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "state : [3 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "action : 0\n",
      "new state: [3 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "reward: -0.75\n",
      "epReward so far: -2.75\n",
      "final state: [3 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "Episode : 68\n",
      "state : [3 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "action : 2\n",
      "new state: [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "action : 0\n",
      "new state: [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "action : 0\n",
      "new state: [1 3 2 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: 0.25\n",
      "epReward so far: -1.25\n",
      "state : [1 3 2 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 1\n",
      "new state: [1 2 3 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "state : [1 2 3 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 1\n",
      "new state: [1 2 3 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: -0.5\n",
      "epReward so far: -2.5\n",
      "final state: [1 2 3 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "Episode : 69\n",
      "state : [1 2 3 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 3\n",
      "new state: [1 2 3 2 3 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [1 2 3 2 3 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "action : 1\n",
      "new state: [1 1 3 3 2 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "reward: 0.25\n",
      "epReward so far: -0.25\n",
      "state : [1 1 3 3 2 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "action : 3\n",
      "new state: [1 1 4 2 2 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 3]\n",
      "reward: -0.5\n",
      "epReward so far: -0.75\n",
      "state : [1 1 4 2 2 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 3]\n",
      "action : 1\n",
      "new state: [1 0 5 2 3 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 3]\n",
      "reward: -0.5\n",
      "epReward so far: -1.25\n",
      "state : [1 0 5 2 3 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 3]\n",
      "action : 0\n",
      "new state: [0 0 6 2 3 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "reward: 0.25\n",
      "epReward so far: -1.0\n",
      "final state: [0 0 6 2 3 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "Episode : 70\n",
      "state : [0 0 6 2 3 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "action : 3\n",
      "new state: [0 0 6 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [0 0 6 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 2\n",
      "new state: [0 0 6 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "reward: 0.0\n",
      "epReward so far: -0.75\n",
      "state : [0 0 6 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "action : 2\n",
      "new state: [0 0 6 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [0 0 6 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "action : 3\n",
      "new state: [0 0 6 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: -0.5\n",
      "epReward so far: -2.0\n",
      "state : [0 0 6 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 2\n",
      "new state: [0 0 6 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "reward: -0.75\n",
      "epReward so far: -2.75\n",
      "final state: [0 0 6 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "Episode : 71\n",
      "state : [0 0 6 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "action : 2\n",
      "new state: [0 1 6 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [0 1 6 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "action : 2\n",
      "new state: [0 1 6 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [0 1 6 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "action : 2\n",
      "new state: [0 1 5 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: 0.25\n",
      "epReward so far: -1.25\n",
      "state : [0 1 5 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 2\n",
      "new state: [0 1 5 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "reward: -0.0\n",
      "epReward so far: -1.25\n",
      "state : [0 1 5 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "action : 1\n",
      "new state: [0 2 5 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "final state: [0 2 5 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "Episode : 72\n",
      "state : [0 2 5 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "action : 2\n",
      "new state: [0 2 4 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [0 2 4 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "action : 2\n",
      "new state: [0 2 4 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [0 2 4 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "action : 3\n",
      "new state: [0 2 4 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "state : [0 2 4 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "action : 1\n",
      "new state: [0 2 4 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "reward: -0.0\n",
      "epReward so far: -2.0\n",
      "state : [0 2 4 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "action : 2\n",
      "new state: [0 2 3 4 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "reward: 0.25\n",
      "epReward so far: -1.75\n",
      "final state: [0 2 3 4 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "Episode : 73\n",
      "state : [0 2 3 4 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "action : 2\n",
      "new state: [0 2 2 4 3 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [0 2 2 4 3 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "action : 3\n",
      "new state: [0 2 2 5 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [0 2 2 5 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "action : 2\n",
      "new state: [0 2 2 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "state : [0 2 2 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "action : 3\n",
      "new state: [0 2 2 5 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: -0.5\n",
      "epReward so far: -2.5\n",
      "state : [0 2 2 5 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 3\n",
      "new state: [0 2 2 4 0 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 2]\n",
      "reward: 0.25\n",
      "epReward so far: -2.25\n",
      "final state: [0 2 2 4 0 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 2]\n",
      "Episode : 74\n",
      "state : [0 2 2 4 0 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 2]\n",
      "action : 1\n",
      "new state: [1 2 2 4 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [1 2 2 4 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "action : 3\n",
      "new state: [1 2 3 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [1 2 3 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "action : 2\n",
      "new state: [1 2 2 4 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "reward: 0.25\n",
      "epReward so far: -1.25\n",
      "state : [1 2 2 4 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "action : 3\n",
      "new state: [1 2 2 3 0 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "reward: -0.5\n",
      "epReward so far: -1.75\n",
      "state : [1 2 2 3 0 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "action : 3\n",
      "new state: [2 2 2 2 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "reward: -0.5\n",
      "epReward so far: -2.25\n",
      "final state: [2 2 2 2 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "Episode : 75\n",
      "state : [2 2 2 2 2 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "action : 1\n",
      "new state: [2 2 2 2 2 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [2 2 2 2 2 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0]\n",
      "action : 2\n",
      "new state: [2 2 2 2 0 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1]\n",
      "reward: 0.25\n",
      "epReward so far: -0.25\n",
      "state : [2 2 2 2 0 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1]\n",
      "action : 1\n",
      "new state: [3 2 2 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "reward: 0.0\n",
      "epReward so far: -0.25\n",
      "state : [3 2 2 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "action : 0\n",
      "new state: [4 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.0\n",
      "state : [4 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "action : 0\n",
      "new state: [4 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.75\n",
      "final state: [4 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "Episode : 76\n",
      "state : [4 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "action : 3\n",
      "new state: [4 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [4 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "action : 0\n",
      "new state: [4 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [4 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "action : 0\n",
      "new state: [4 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [4 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "action : 0\n",
      "new state: [3 2 2 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "reward: -0.5\n",
      "epReward so far: -2.0\n",
      "state : [3 2 2 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "action : 0\n",
      "new state: [3 2 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: -0.0\n",
      "epReward so far: -2.0\n",
      "final state: [3 2 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "Episode : 77\n",
      "state : [3 2 2 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 2\n",
      "new state: [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "action : 2\n",
      "new state: [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "action : 2\n",
      "new state: [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "action : 3\n",
      "new state: [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "reward: -0.75\n",
      "epReward so far: -2.25\n",
      "state : [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3]\n",
      "action : 1\n",
      "new state: [3 1 2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: -0.5\n",
      "epReward so far: -2.75\n",
      "final state: [3 1 2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "Episode : 78\n",
      "state : [3 1 2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 3\n",
      "new state: [3 1 2 2 3 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 3]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [3 1 2 2 3 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 3]\n",
      "action : 0\n",
      "new state: [3 1 2 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: -0.75\n",
      "epReward so far: -0.5\n",
      "state : [3 1 2 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 0\n",
      "new state: [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [3 2 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "action : 0\n",
      "new state: [2 2 2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: -0.5\n",
      "epReward so far: -1.75\n",
      "state : [2 2 2 3 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 0\n",
      "new state: [1 2 2 3 3 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "reward: 0.25\n",
      "epReward so far: -1.5\n",
      "final state: [1 2 2 3 3 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "Episode : 79\n",
      "state : [1 2 2 3 3 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2]\n",
      "action : 1\n",
      "new state: [1 2 2 4 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [1 2 2 4 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "action : 3\n",
      "new state: [1 2 2 4 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: 0.25\n",
      "epReward so far: -0.5\n",
      "state : [1 2 2 4 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 3\n",
      "new state: [1 2 2 4 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [1 2 2 4 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 3\n",
      "new state: [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "state : [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "action : 3\n",
      "new state: [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "reward: -0.75\n",
      "epReward so far: -2.75\n",
      "final state: [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "Episode : 80\n",
      "state : [2 3 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "action : 3\n",
      "new state: [2 3 2 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [2 3 2 2 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 1\n",
      "new state: [2 2 2 2 3 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 2]\n",
      "reward: -0.5\n",
      "epReward so far: -1.0\n",
      "state : [2 2 2 2 3 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 2]\n",
      "action : 3\n",
      "new state: [2 2 2 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: -1.0\n",
      "state : [2 2 2 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 1\n",
      "new state: [2 1 3 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "reward: -0.75\n",
      "epReward so far: -1.75\n",
      "state : [2 1 3 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "action : 3\n",
      "new state: [2 1 3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "reward: -0.5\n",
      "epReward so far: -2.25\n",
      "final state: [2 1 3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "Episode : 81\n",
      "state : [2 1 3 3 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "action : 2\n",
      "new state: [2 1 3 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 1 3 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "action : 2\n",
      "new state: [2 1 3 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "reward: 0.25\n",
      "epReward so far: -0.5\n",
      "state : [2 1 3 3 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "action : 2\n",
      "new state: [2 1 2 3 1 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 3]\n",
      "reward: -0.5\n",
      "epReward so far: -1.0\n",
      "state : [2 1 2 3 1 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 3]\n",
      "action : 0\n",
      "new state: [1 2 2 3 3 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 3]\n",
      "reward: -0.5\n",
      "epReward so far: -1.5\n",
      "state : [1 2 2 3 3 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 3]\n",
      "action : 0\n",
      "new state: [0 3 2 3 3 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 1]\n",
      "reward: -0.5\n",
      "epReward so far: -2.0\n",
      "final state: [0 3 2 3 3 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 1]\n",
      "Episode : 82\n",
      "state : [0 3 2 3 3 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 1]\n",
      "action : 3\n",
      "new state: [0 3 2 4 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [0 3 2 4 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "action : 1\n",
      "new state: [0 3 2 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [0 3 2 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "action : 1\n",
      "new state: [0 3 2 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "reward: -0.75\n",
      "epReward so far: -2.25\n",
      "state : [0 3 2 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "action : 3\n",
      "new state: [0 3 2 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "reward: -0.75\n",
      "epReward so far: -3.0\n",
      "state : [0 3 2 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "action : 3\n",
      "new state: [0 3 2 4 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: -0.5\n",
      "epReward so far: -3.5\n",
      "final state: [0 3 2 4 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "Episode : 83\n",
      "state : [0 3 2 4 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 3\n",
      "new state: [0 3 2 3 3 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [0 3 2 3 3 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
      "action : 1\n",
      "new state: [0 3 2 4 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.5\n",
      "state : [0 3 2 4 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 3\n",
      "new state: [0 5 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [0 5 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "action : 3\n",
      "new state: [0 6 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "state : [0 6 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "action : 1\n",
      "new state: [0 6 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "reward: -0.75\n",
      "epReward so far: -2.75\n",
      "final state: [0 6 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "Episode : 84\n",
      "state : [0 6 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "action : 3\n",
      "new state: [0 6 2 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: 0.25\n",
      "epReward so far: 0.25\n",
      "state : [0 6 2 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 1\n",
      "new state: [0 6 2 1 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "reward: 0.0\n",
      "epReward so far: 0.25\n",
      "state : [0 6 2 1 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "action : 2\n",
      "new state: [0 6 2 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "reward: -0.5\n",
      "epReward so far: -0.25\n",
      "state : [0 6 2 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "action : 1\n",
      "new state: [0 5 2 1 0 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 0]\n",
      "reward: 0.25\n",
      "epReward so far: 0.0\n",
      "state : [0 5 2 1 0 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 0]\n",
      "action : 2\n",
      "new state: [1 5 1 1 0 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "final state: [1 5 1 1 0 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "Episode : 85\n",
      "state : [1 5 1 1 0 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "action : 1\n",
      "new state: [1 5 2 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [1 5 2 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 1\n",
      "new state: [2 5 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "reward: 0.0\n",
      "epReward so far: -0.75\n",
      "state : [2 5 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "action : 0\n",
      "new state: [2 5 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [2 5 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1]\n",
      "action : 3\n",
      "new state: [2 5 2 0 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: -0.5\n",
      "epReward so far: -2.0\n",
      "state : [2 5 2 0 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 1\n",
      "new state: [2 5 2 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: -0.75\n",
      "epReward so far: -2.75\n",
      "final state: [2 5 2 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "Episode : 86\n",
      "state : [2 5 2 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 1\n",
      "new state: [2 5 2 0 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [2 5 2 0 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "action : 0\n",
      "new state: [2 5 2 0 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "reward: -0.0\n",
      "epReward so far: -0.5\n",
      "state : [2 5 2 0 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0]\n",
      "action : 0\n",
      "new state: [2 5 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [2 5 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "action : 0\n",
      "new state: [2 5 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "state : [2 5 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "action : 2\n",
      "new state: [2 5 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "reward: -0.75\n",
      "epReward so far: -2.75\n",
      "final state: [2 5 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "Episode : 87\n",
      "state : [2 5 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "action : 1\n",
      "new state: [2 5 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 5 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "action : 2\n",
      "new state: [2 5 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [2 5 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "action : 1\n",
      "new state: [2 5 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "reward: -0.75\n",
      "epReward so far: -2.25\n",
      "state : [2 5 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3]\n",
      "action : 1\n",
      "new state: [2 4 3 0 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: 0.25\n",
      "epReward so far: -2.0\n",
      "state : [2 4 3 0 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 2\n",
      "new state: [2 4 3 0 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: -0.75\n",
      "epReward so far: -2.75\n",
      "final state: [2 4 3 0 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "Episode : 88\n",
      "state : [2 4 3 0 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 2\n",
      "new state: [2 4 2 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [2 4 2 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3]\n",
      "action : 1\n",
      "new state: [2 4 2 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [2 4 2 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "action : 2\n",
      "new state: [2 5 1 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "reward: -0.5\n",
      "epReward so far: -1.75\n",
      "state : [2 5 1 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "action : 0\n",
      "new state: [1 5 1 1 2 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 1]\n",
      "reward: -0.5\n",
      "epReward so far: -2.25\n",
      "state : [1 5 1 1 2 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 1]\n",
      "action : 1\n",
      "new state: [1 5 2 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "reward: -0.75\n",
      "epReward so far: -3.0\n",
      "final state: [1 5 2 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "Episode : 89\n",
      "state : [1 5 2 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "action : 1\n",
      "new state: [2 4 2 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [2 4 2 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 3\n",
      "new state: [2 4 2 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: 0.0\n",
      "epReward so far: -0.5\n",
      "state : [2 4 2 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 2\n",
      "new state: [2 6 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [2 6 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "action : 2\n",
      "new state: [2 6 0 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "reward: -0.5\n",
      "epReward so far: -1.75\n",
      "state : [2 6 0 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "action : 3\n",
      "new state: [2 6 0 1 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: -0.75\n",
      "epReward so far: -2.5\n",
      "final state: [2 6 0 1 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "Episode : 90\n",
      "state : [2 6 0 1 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 1\n",
      "new state: [2 5 1 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [2 5 1 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "action : 0\n",
      "new state: [1 5 1 1 1 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1]\n",
      "reward: 0.25\n",
      "epReward so far: -0.25\n",
      "state : [1 5 1 1 1 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1]\n",
      "action : 1\n",
      "new state: [1 6 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: -0.0\n",
      "epReward so far: -0.25\n",
      "state : [1 6 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 1\n",
      "new state: [1 7 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "reward: -0.0\n",
      "epReward so far: -0.25\n",
      "state : [1 7 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "action : 3\n",
      "new state: [1 7 1 0 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: 0.25\n",
      "epReward so far: 0.0\n",
      "final state: [1 7 1 0 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "Episode : 91\n",
      "state : [1 7 1 0 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 1\n",
      "new state: [1 6 1 0 1 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 1]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [1 6 1 0 1 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 1]\n",
      "action : 1\n",
      "new state: [1 7 1 0 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [1 7 1 0 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 1\n",
      "new state: [1 7 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "reward: -0.75\n",
      "epReward so far: -2.0\n",
      "state : [1 7 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "action : 1\n",
      "new state: [1 6 2 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "reward: -0.5\n",
      "epReward so far: -2.5\n",
      "state : [1 6 2 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 3]\n",
      "action : 2\n",
      "new state: [1 6 2 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "reward: -0.75\n",
      "epReward so far: -3.25\n",
      "final state: [1 6 2 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "Episode : 92\n",
      "state : [1 6 2 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "action : 2\n",
      "new state: [2 6 1 0 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [2 6 1 0 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 1\n",
      "new state: [2 5 1 0 2 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "reward: -0.5\n",
      "epReward so far: -1.0\n",
      "state : [2 5 1 0 2 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 1]\n",
      "action : 1\n",
      "new state: [2 4 2 0 1 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 1]\n",
      "reward: -0.5\n",
      "epReward so far: -1.5\n",
      "state : [2 4 2 0 1 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 1]\n",
      "action : 0\n",
      "new state: [1 4 3 0 1 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 1]\n",
      "reward: -0.5\n",
      "epReward so far: -2.0\n",
      "state : [1 4 3 0 1 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 1]\n",
      "action : 0\n",
      "new state: [1 5 3 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "reward: -0.0\n",
      "epReward so far: -2.0\n",
      "final state: [1 5 3 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "Episode : 93\n",
      "state : [1 5 3 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "action : 2\n",
      "new state: [1 6 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [1 6 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "action : 1\n",
      "new state: [1 6 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [1 6 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2]\n",
      "action : 2\n",
      "new state: [1 6 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "reward: -0.75\n",
      "epReward so far: -2.25\n",
      "state : [1 6 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "action : 1\n",
      "new state: [1 5 3 0 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "reward: -0.5\n",
      "epReward so far: -2.75\n",
      "state : [1 5 3 0 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 2]\n",
      "action : 1\n",
      "new state: [1 5 3 0 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "reward: -0.75\n",
      "epReward so far: -3.5\n",
      "final state: [1 5 3 0 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "Episode : 94\n",
      "state : [1 5 3 0 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 1]\n",
      "action : 1\n",
      "new state: [1 5 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [1 5 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "action : 2\n",
      "new state: [1 5 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [1 5 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "action : 1\n",
      "new state: [1 4 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "reward: -0.75\n",
      "epReward so far: -2.25\n",
      "state : [1 4 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "action : 2\n",
      "new state: [1 4 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "reward: -0.75\n",
      "epReward so far: -3.0\n",
      "state : [1 4 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "action : 1\n",
      "new state: [1 4 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "reward: -0.75\n",
      "epReward so far: -3.75\n",
      "final state: [1 4 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "Episode : 95\n",
      "state : [1 4 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "action : 2\n",
      "new state: [1 4 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [1 4 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "action : 0\n",
      "new state: [0 4 5 0 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "reward: 0.25\n",
      "epReward so far: -0.5\n",
      "state : [0 4 5 0 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "action : 2\n",
      "new state: [0 5 4 0 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [0 5 4 0 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
      "action : 1\n",
      "new state: [0 4 5 0 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "reward: -0.5\n",
      "epReward so far: -1.75\n",
      "state : [0 4 5 0 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1]\n",
      "action : 2\n",
      "new state: [0 4 5 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: -1.75\n",
      "final state: [0 4 5 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "Episode : 96\n",
      "state : [0 4 5 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 1\n",
      "new state: [0 5 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [0 5 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1]\n",
      "action : 1\n",
      "new state: [0 5 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [0 5 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "action : 1\n",
      "new state: [0 4 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "reward: -0.75\n",
      "epReward so far: -2.25\n",
      "state : [0 4 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "action : 2\n",
      "new state: [0 5 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "reward: -0.75\n",
      "epReward so far: -3.0\n",
      "state : [0 5 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3]\n",
      "action : 1\n",
      "new state: [0 5 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "reward: -0.75\n",
      "epReward so far: -3.75\n",
      "final state: [0 5 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "Episode : 97\n",
      "state : [0 5 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "action : 2\n",
      "new state: [0 5 4 0 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "reward: -0.5\n",
      "epReward so far: -0.5\n",
      "state : [0 5 4 0 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "action : 1\n",
      "new state: [0 5 4 0 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "state : [0 5 4 0 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "action : 2\n",
      "new state: [0 5 4 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "reward: -0.5\n",
      "epReward so far: -1.75\n",
      "state : [0 5 4 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "action : 2\n",
      "new state: [0 5 4 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "reward: -0.75\n",
      "epReward so far: -2.5\n",
      "state : [0 5 4 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 0]\n",
      "action : 2\n",
      "new state: [1 5 3 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "reward: -0.5\n",
      "epReward so far: -3.0\n",
      "final state: [1 5 3 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "Episode : 98\n",
      "state : [1 5 3 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 3]\n",
      "action : 1\n",
      "new state: [1 5 3 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 5 3 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2]\n",
      "action : 2\n",
      "new state: [2 5 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "reward: 0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 5 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2]\n",
      "action : 1\n",
      "new state: [2 5 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 5 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "action : 1\n",
      "new state: [2 4 3 0 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "reward: 0.25\n",
      "epReward so far: -0.5\n",
      "state : [2 4 3 0 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2]\n",
      "action : 1\n",
      "new state: [2 4 3 0 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "reward: -0.75\n",
      "epReward so far: -1.25\n",
      "final state: [2 4 3 0 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "Episode : 99\n",
      "state : [2 4 3 0 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 3]\n",
      "action : 1\n",
      "new state: [2 4 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "reward: -0.75\n",
      "epReward so far: -0.75\n",
      "state : [2 4 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2]\n",
      "action : 2\n",
      "new state: [2 4 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "reward: -0.75\n",
      "epReward so far: -1.5\n",
      "state : [2 4 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      "action : 1\n",
      "new state: [2 3 4 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "reward: -0.5\n",
      "epReward so far: -2.0\n",
      "state : [2 3 4 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2]\n",
      "action : 2\n",
      "new state: [2 3 3 0 0 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "reward: -0.5\n",
      "epReward so far: -2.5\n",
      "state : [2 3 3 0 0 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2]\n",
      "action : 1\n",
      "new state: [3 2 3 0 2 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 1]\n",
      "reward: 0.25\n",
      "epReward so far: -2.25\n",
      "final state: [3 2 3 0 2 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 1]\n",
      "**************************************************\n",
      "Experiment complete\n",
      "**************************************************\n",
      "**************************************************\n",
      "Saving data\n",
      "**************************************************\n",
      "[[ 0.00000000e+00  0.00000000e+00 -1.25000000e+00  1.84350000e+04\n",
      "  -4.37597579e+00]\n",
      " [ 1.00000000e+00  0.00000000e+00 -3.50000000e+00  1.75640000e+04\n",
      "  -5.02435556e+00]\n",
      " [ 2.00000000e+00  0.00000000e+00 -2.00000000e+00  1.61890000e+04\n",
      "  -4.97335962e+00]\n",
      " [ 3.00000000e+00  0.00000000e+00 -1.25000000e+00  1.67830000e+04\n",
      "  -4.46289238e+00]\n",
      " [ 4.00000000e+00  0.00000000e+00 -3.00000000e+00  1.09840000e+04\n",
      "  -4.54934744e+00]\n",
      " [ 5.00000000e+00  0.00000000e+00 -1.75000000e+00  1.80340000e+04\n",
      "  -4.41571459e+00]\n",
      " [ 6.00000000e+00  0.00000000e+00 -3.50000000e+00  1.88990000e+04\n",
      "  -5.00088493e+00]\n",
      " [ 7.00000000e+00  0.00000000e+00 -1.75000000e+00  1.22330000e+04\n",
      "  -4.42951969e+00]\n",
      " [ 8.00000000e+00  0.00000000e+00 -3.25000000e+00  2.00030000e+04\n",
      "  -4.81886446e+00]\n",
      " [ 9.00000000e+00  0.00000000e+00 -2.25000000e+00  1.15140000e+04\n",
      "  -4.64173749e+00]\n",
      " [ 1.00000000e+01  0.00000000e+00 -7.50000000e-01  2.16300000e+04\n",
      "  -4.94258796e+00]\n",
      " [ 1.10000000e+01  0.00000000e+00 -1.25000000e+00  8.11380000e+04\n",
      "  -4.88236238e+00]\n",
      " [ 1.20000000e+01  0.00000000e+00 -2.00000000e+00  1.57280000e+04\n",
      "  -5.18930238e+00]\n",
      " [ 1.30000000e+01  0.00000000e+00 -1.75000000e+00  8.29600000e+03\n",
      "  -4.93828731e+00]\n",
      " [ 1.40000000e+01  0.00000000e+00 -3.50000000e+00  1.05550000e+04\n",
      "  -4.99333453e+00]\n",
      " [ 1.50000000e+01  0.00000000e+00 -2.25000000e+00  1.55660000e+04\n",
      "  -4.86224461e+00]\n",
      " [ 1.60000000e+01  0.00000000e+00 -2.50000000e+00  9.07900000e+03\n",
      "  -5.08531108e+00]\n",
      " [ 1.70000000e+01  0.00000000e+00 -2.50000000e+00  1.19450000e+04\n",
      "  -4.70268674e+00]\n",
      " [ 1.80000000e+01  0.00000000e+00 -2.25000000e+00  1.47540000e+04\n",
      "  -4.59263766e+00]\n",
      " [ 1.90000000e+01  0.00000000e+00 -2.25000000e+00  9.79100000e+03\n",
      "  -4.93858676e+00]\n",
      " [ 2.00000000e+01  0.00000000e+00 -2.50000000e-01  1.03250000e+04\n",
      "  -4.70832739e+00]\n",
      " [ 2.10000000e+01  0.00000000e+00 -2.50000000e+00  1.01010000e+04\n",
      "  -4.51223279e+00]\n",
      " [ 2.20000000e+01  0.00000000e+00 -2.50000000e+00  1.12610000e+04\n",
      "  -1.44419804e+00]\n",
      " [ 2.30000000e+01  0.00000000e+00 -5.00000000e-01  1.23390000e+04\n",
      "  -4.19285514e+00]\n",
      " [ 2.40000000e+01  0.00000000e+00 -3.50000000e+00  1.12740000e+04\n",
      "  -5.10574975e+00]\n",
      " [ 2.50000000e+01  0.00000000e+00 -2.25000000e+00  1.55770000e+04\n",
      "  -4.54361431e+00]\n",
      " [ 2.60000000e+01  0.00000000e+00 -1.25000000e+00  1.67520000e+04\n",
      "  -4.77124602e+00]\n",
      " [ 2.70000000e+01  0.00000000e+00 -1.75000000e+00  1.50810000e+04\n",
      "  -4.93489984e+00]\n",
      " [ 2.80000000e+01  0.00000000e+00 -1.50000000e+00  1.54000000e+04\n",
      "  -4.29028922e+00]\n",
      " [ 2.90000000e+01  0.00000000e+00 -2.00000000e+00  1.64880000e+04\n",
      "  -4.65415444e+00]\n",
      " [ 3.00000000e+01  0.00000000e+00 -2.75000000e+00  1.49590000e+04\n",
      "  -4.57842384e+00]\n",
      " [ 3.10000000e+01  0.00000000e+00 -2.50000000e+00  1.81140000e+04\n",
      "  -4.86239877e+00]\n",
      " [ 3.20000000e+01  0.00000000e+00 -2.75000000e+00  1.27680000e+04\n",
      "  -5.10096280e+00]\n",
      " [ 3.30000000e+01  0.00000000e+00 -2.25000000e+00  1.80420000e+04\n",
      "  -4.59861234e+00]\n",
      " [ 3.40000000e+01  0.00000000e+00 -2.00000000e+00  1.36950000e+04\n",
      "  -4.58634744e+00]\n",
      " [ 3.50000000e+01  0.00000000e+00 -1.75000000e+00  2.04140000e+04\n",
      "  -4.86033492e+00]\n",
      " [ 3.60000000e+01  0.00000000e+00 -2.75000000e+00  1.92880000e+04\n",
      "  -4.82481606e+00]\n",
      " [ 3.70000000e+01  0.00000000e+00 -1.75000000e+00  1.20250000e+04\n",
      "  -4.52394621e+00]\n",
      " [ 3.80000000e+01  0.00000000e+00 -1.50000000e+00  1.46770000e+04\n",
      "  -4.54567895e+00]\n",
      " [ 3.90000000e+01  0.00000000e+00 -2.25000000e+00  1.12570000e+04\n",
      "  -4.54860365e+00]\n",
      " [ 4.00000000e+01  0.00000000e+00 -2.00000000e+00  1.40800000e+04\n",
      "  -4.81998692e+00]\n",
      " [ 4.10000000e+01  0.00000000e+00 -3.50000000e+00  1.18660000e+04\n",
      "  -4.43951002e+00]\n",
      " [ 4.20000000e+01  0.00000000e+00 -1.00000000e+00  3.83980000e+04\n",
      "  -5.00010609e+00]\n",
      " [ 4.30000000e+01  0.00000000e+00 -7.50000000e-01  1.32160000e+04\n",
      "  -5.37848024e+00]\n",
      " [ 4.40000000e+01  0.00000000e+00 -2.00000000e+00  1.31590000e+04\n",
      "  -5.31038328e+00]\n",
      " [ 4.50000000e+01  0.00000000e+00 -1.25000000e+00  9.02200000e+03\n",
      "  -5.09405829e+00]\n",
      " [ 4.60000000e+01  0.00000000e+00 -1.75000000e+00  1.71840000e+04\n",
      "  -4.65550745e+00]\n",
      " [ 4.70000000e+01  0.00000000e+00 -2.00000000e+00  7.53400000e+03\n",
      "  -4.81277199e+00]\n",
      " [ 4.80000000e+01  0.00000000e+00 -3.00000000e+00  1.53200000e+04\n",
      "  -5.19629647e+00]\n",
      " [ 4.90000000e+01  0.00000000e+00 -2.75000000e+00  1.31610000e+04\n",
      "  -5.26357153e+00]\n",
      " [ 5.00000000e+01  0.00000000e+00 -2.50000000e+00  1.62810000e+04\n",
      "  -4.98937052e+00]\n",
      " [ 5.10000000e+01  0.00000000e+00 -1.75000000e+00  1.29450000e+04\n",
      "  -4.83256758e+00]\n",
      " [ 5.20000000e+01  0.00000000e+00 -2.00000000e+00  1.31610000e+04\n",
      "  -5.29292081e+00]\n",
      " [ 5.30000000e+01  0.00000000e+00 -3.00000000e+00  1.36740000e+04\n",
      "  -4.79054434e+00]\n",
      " [ 5.40000000e+01  0.00000000e+00 -2.50000000e+00  5.14600000e+03\n",
      "  -4.89566255e+00]\n",
      " [ 5.50000000e+01  0.00000000e+00 -2.25000000e+00  1.91210000e+04\n",
      "  -4.61552424e+00]\n",
      " [ 5.60000000e+01  0.00000000e+00 -1.75000000e+00  1.45110000e+04\n",
      "  -4.87421671e+00]\n",
      " [ 5.70000000e+01  0.00000000e+00 -2.00000000e+00  6.74400000e+03\n",
      "  -4.21528423e+00]\n",
      " [ 5.80000000e+01  0.00000000e+00 -2.50000000e+00  1.68730000e+04\n",
      "  -4.73083509e+00]\n",
      " [ 5.90000000e+01  0.00000000e+00 -3.25000000e+00  1.43770000e+04\n",
      "  -2.07900295e+00]\n",
      " [ 6.00000000e+01  0.00000000e+00 -2.25000000e+00  1.20480000e+04\n",
      "  -4.83535489e+00]\n",
      " [ 6.10000000e+01  0.00000000e+00 -3.25000000e+00  1.59940000e+04\n",
      "  -5.12068871e+00]\n",
      " [ 6.20000000e+01  0.00000000e+00 -2.50000000e+00  1.31620000e+04\n",
      "  -5.13314217e+00]\n",
      " [ 6.30000000e+01  0.00000000e+00 -1.75000000e+00  1.31630000e+04\n",
      "  -4.91046764e+00]\n",
      " [ 6.40000000e+01  0.00000000e+00 -2.25000000e+00  1.31620000e+04\n",
      "  -5.16168039e+00]\n",
      " [ 6.50000000e+01  0.00000000e+00 -1.50000000e+00  1.00540000e+04\n",
      "  -4.62905763e+00]\n",
      " [ 6.60000000e+01  0.00000000e+00 -2.50000000e+00  9.63300000e+03\n",
      "  -4.00742593e+00]\n",
      " [ 6.70000000e+01  0.00000000e+00 -3.25000000e+00  1.04430000e+04\n",
      "  -4.45839429e+00]\n",
      " [ 6.80000000e+01  0.00000000e+00 -3.25000000e+00  1.17060000e+04\n",
      "  -4.74386911e+00]\n",
      " [ 6.90000000e+01  0.00000000e+00 -3.25000000e+00  1.31630000e+04\n",
      "  -4.83244787e+00]\n",
      " [ 7.00000000e+01  0.00000000e+00 -1.50000000e+00  1.84450000e+04\n",
      "  -4.56383715e+00]\n",
      " [ 7.10000000e+01  0.00000000e+00 -3.50000000e+00  1.31630000e+04\n",
      "  -4.65960268e+00]\n",
      " [ 7.20000000e+01  0.00000000e+00 -2.00000000e+00  1.31590000e+04\n",
      "  -4.76930561e+00]\n",
      " [ 7.30000000e+01  0.00000000e+00 -2.75000000e+00  1.84640000e+04\n",
      "  -3.40117688e+00]\n",
      " [ 7.40000000e+01  0.00000000e+00 -2.75000000e+00  4.25450000e+04\n",
      "  -3.30590766e+00]\n",
      " [ 7.50000000e+01  0.00000000e+00 -7.50000000e-01  1.63830000e+04\n",
      "  -2.87316098e+00]\n",
      " [ 7.60000000e+01  0.00000000e+00 -2.00000000e+00  1.17690000e+04\n",
      "  -4.79186531e+00]\n",
      " [ 7.70000000e+01  0.00000000e+00 -3.00000000e+00  8.09900000e+03\n",
      "  -4.25664989e+00]\n",
      " [ 7.80000000e+01  0.00000000e+00 -1.50000000e+00  1.48580000e+04\n",
      "  -4.47923834e+00]\n",
      " [ 7.90000000e+01  0.00000000e+00 -1.25000000e+00  1.46860000e+04\n",
      "  -4.33728168e+00]\n",
      " [ 8.00000000e+01  0.00000000e+00 -2.50000000e-01  1.51510000e+04\n",
      "  -4.24359442e+00]\n",
      " [ 8.10000000e+01  0.00000000e+00 -2.50000000e+00  1.33530000e+04\n",
      "  -5.26297299e+00]\n",
      " [ 8.20000000e+01  0.00000000e+00 -3.00000000e+00  1.75290000e+04\n",
      "  -4.26640590e+00]\n",
      " [ 8.30000000e+01  0.00000000e+00 -1.50000000e+00  1.68570000e+04\n",
      "  -4.42036120e+00]\n",
      " [ 8.40000000e+01  0.00000000e+00 -1.25000000e+00  1.28790000e+04\n",
      "  -5.22672371e+00]\n",
      " [ 8.50000000e+01  0.00000000e+00 -5.00000000e-01  1.19780000e+04\n",
      "  -4.83370554e+00]\n",
      " [ 8.60000000e+01  0.00000000e+00 -1.00000000e+00  2.09960000e+04\n",
      "  -4.58193511e+00]\n",
      " [ 8.70000000e+01  0.00000000e+00 -2.25000000e+00  1.73270000e+04\n",
      "  -4.71376082e+00]\n",
      " [ 8.80000000e+01  0.00000000e+00 -3.50000000e+00  1.29150000e+04\n",
      "  -4.59560873e+00]\n",
      " [ 8.90000000e+01  0.00000000e+00 -3.00000000e+00  4.30190000e+04\n",
      "  -4.30550904e+00]\n",
      " [ 9.00000000e+01  0.00000000e+00 -1.25000000e+00  1.43590000e+04\n",
      "  -5.28521992e+00]\n",
      " [ 9.10000000e+01  0.00000000e+00 -3.00000000e+00  1.31610000e+04\n",
      "  -5.38625996e+00]\n",
      " [ 9.20000000e+01  0.00000000e+00 -2.25000000e+00  1.73700000e+04\n",
      "  -5.41644116e+00]\n",
      " [ 9.30000000e+01  0.00000000e+00 -2.25000000e+00  7.76600000e+03\n",
      "  -4.92967513e+00]\n",
      " [ 9.40000000e+01  0.00000000e+00 -1.50000000e+00  1.53730000e+04\n",
      "  -4.85364901e+00]\n",
      " [ 9.50000000e+01  0.00000000e+00 -2.50000000e+00  1.32460000e+04\n",
      "  -4.87412311e+00]\n",
      " [ 9.60000000e+01  0.00000000e+00 -2.25000000e+00  1.55530000e+04\n",
      "  -1.24938457e+00]\n",
      " [ 9.70000000e+01  0.00000000e+00 -3.50000000e+00  2.05140000e+04\n",
      "  -3.48590404e+00]\n",
      " [ 9.80000000e+01  0.00000000e+00 -2.75000000e+00  1.31630000e+04\n",
      "  -4.84191920e+00]\n",
      " [ 9.90000000e+01  0.00000000e+00 -2.00000000e+00  1.36800000e+04\n",
      "  -4.87399832e+00]\n",
      " [ 0.00000000e+00  1.00000000e+00 -2.00000000e+00  1.10070000e+04\n",
      "  -4.82481606e+00]\n",
      " [ 1.00000000e+00  1.00000000e+00 -3.50000000e+00  1.69300000e+04\n",
      "  -4.46558440e+00]\n",
      " [ 2.00000000e+00  1.00000000e+00 -2.00000000e+00  1.63430000e+04\n",
      "  -3.91102310e+00]\n",
      " [ 3.00000000e+00  1.00000000e+00 -2.25000000e+00  1.75490000e+04\n",
      "  -4.28439012e+00]\n",
      " [ 4.00000000e+00  1.00000000e+00 -1.50000000e+00  8.47200000e+03\n",
      "  -5.17486905e+00]\n",
      " [ 5.00000000e+00  1.00000000e+00 -1.25000000e+00  1.84490000e+04\n",
      "  -4.92510118e+00]\n",
      " [ 6.00000000e+00  1.00000000e+00 -3.00000000e+00  2.07930000e+04\n",
      "  -4.55011434e+00]\n",
      " [ 7.00000000e+00  1.00000000e+00 -3.50000000e+00  2.24990000e+04\n",
      "  -4.81980961e+00]\n",
      " [ 8.00000000e+00  1.00000000e+00 -1.75000000e+00  1.51920000e+04\n",
      "  -4.02207642e+00]\n",
      " [ 9.00000000e+00  1.00000000e+00 -5.00000000e-01  1.91700000e+04\n",
      "  -4.59393347e+00]\n",
      " [ 1.00000000e+01  1.00000000e+00 -2.00000000e+00  1.73140000e+04\n",
      "  -4.97608522e+00]\n",
      " [ 1.10000000e+01  1.00000000e+00 -1.00000000e+00  1.61980000e+04\n",
      "  -4.55763403e+00]\n",
      " [ 1.20000000e+01  1.00000000e+00 -7.50000000e-01  5.25780000e+04\n",
      "  -4.40019219e+00]\n",
      " [ 1.30000000e+01  1.00000000e+00 -2.00000000e+00  1.42560000e+04\n",
      "  -5.28842535e+00]\n",
      " [ 1.40000000e+01  1.00000000e+00 -2.50000000e+00  1.46230000e+04\n",
      "  -4.57210679e+00]\n",
      " [ 1.50000000e+01  1.00000000e+00 -2.75000000e+00  6.21800000e+03\n",
      "  -5.16272078e+00]\n",
      " [ 1.60000000e+01  1.00000000e+00 -2.25000000e+00  1.63440000e+04\n",
      "  -4.65613447e+00]\n",
      " [ 1.70000000e+01  1.00000000e+00 -1.75000000e+00  9.73800000e+03\n",
      "  -5.17676743e+00]\n",
      " [ 1.80000000e+01  1.00000000e+00 -3.25000000e+00  9.89900000e+03\n",
      "  -5.15666016e+00]\n",
      " [ 1.90000000e+01  1.00000000e+00 -2.75000000e+00  9.30300000e+03\n",
      "  -5.13942755e+00]\n",
      " [ 2.00000000e+01  1.00000000e+00 -3.50000000e+00  9.73900000e+03\n",
      "  -4.75785265e+00]\n",
      " [ 2.10000000e+01  1.00000000e+00 -2.50000000e+00  1.36930000e+04\n",
      "  -4.18842907e+00]\n",
      " [ 2.20000000e+01  1.00000000e+00 -1.75000000e+00  1.12570000e+04\n",
      "  -4.40435740e+00]\n",
      " [ 2.30000000e+01  1.00000000e+00 -3.00000000e+00  1.10420000e+04\n",
      "  -4.88459835e+00]\n",
      " [ 2.40000000e+01  1.00000000e+00 -1.50000000e+00  1.38060000e+04\n",
      "  -3.98803559e+00]\n",
      " [ 2.50000000e+01  1.00000000e+00 -2.25000000e+00  2.18160000e+04\n",
      "  -4.11218483e+00]\n",
      " [ 2.60000000e+01  1.00000000e+00 -1.75000000e+00  1.65950000e+04\n",
      "  -5.04660925e+00]\n",
      " [ 2.70000000e+01  1.00000000e+00 -2.50000000e+00  1.37610000e+04\n",
      "  -4.58447741e+00]\n",
      " [ 2.80000000e+01  1.00000000e+00 -2.75000000e+00  2.00650000e+04\n",
      "  -5.00180617e+00]\n",
      " [ 2.90000000e+01  1.00000000e+00 -3.25000000e+00  1.62750000e+04\n",
      "  -4.40067789e+00]\n",
      " [ 3.00000000e+01  1.00000000e+00 -1.75000000e+00  2.29360000e+04\n",
      "  -4.28198841e+00]\n",
      " [ 3.10000000e+01  1.00000000e+00 -2.00000000e+00  6.48700000e+03\n",
      "  -4.97694930e+00]\n",
      " [ 3.20000000e+01  1.00000000e+00 -5.00000000e-01  1.83470000e+04\n",
      "  -4.62141954e+00]\n",
      " [ 3.30000000e+01  1.00000000e+00 -1.50000000e+00  1.87180000e+04\n",
      "  -3.31134547e+00]\n",
      " [ 3.40000000e+01  1.00000000e+00 -1.75000000e+00  1.08800000e+04\n",
      "  -5.22650179e+00]\n",
      " [ 3.50000000e+01  1.00000000e+00 -3.25000000e+00  6.07400000e+03\n",
      "  -5.00251539e+00]\n",
      " [ 3.60000000e+01  1.00000000e+00 -3.50000000e+00  1.11230000e+04\n",
      "  -5.17078922e+00]\n",
      " [ 3.70000000e+01  1.00000000e+00 -2.50000000e+00  1.60970000e+04\n",
      "  -4.45364993e+00]\n",
      " [ 3.80000000e+01  1.00000000e+00 -2.50000000e+00  1.23060000e+04\n",
      "  -4.16151105e+00]\n",
      " [ 3.90000000e+01  1.00000000e+00 -2.50000000e+00  1.48180000e+04\n",
      "  -4.12250454e+00]\n",
      " [ 4.00000000e+01  1.00000000e+00 -1.50000000e+00  2.14270000e+04\n",
      "  -3.96887528e+00]\n",
      " [ 4.10000000e+01  1.00000000e+00 -3.00000000e+00  1.76340000e+04\n",
      "  -4.44048019e+00]\n",
      " [ 4.20000000e+01  1.00000000e+00 -3.50000000e+00  1.72900000e+04\n",
      "  -4.14030370e+00]\n",
      " [ 4.30000000e+01  1.00000000e+00 -1.75000000e+00  1.95990000e+04\n",
      "  -4.66695575e+00]\n",
      " [ 4.40000000e+01  1.00000000e+00 -3.25000000e+00  2.10810000e+04\n",
      "  -4.43755235e+00]\n",
      " [ 4.50000000e+01  1.00000000e+00 -1.75000000e+00  1.51980000e+04\n",
      "  -4.31400826e+00]\n",
      " [ 4.60000000e+01  1.00000000e+00 -3.25000000e+00  1.47930000e+04\n",
      "  -5.02460939e+00]\n",
      " [ 4.70000000e+01  1.00000000e+00 -2.50000000e+00  2.26220000e+04\n",
      "  -4.47801988e+00]\n",
      " [ 4.80000000e+01  1.00000000e+00 -1.25000000e+00  9.44800000e+03\n",
      "  -5.03639248e+00]\n",
      " [ 4.90000000e+01  1.00000000e+00 -3.25000000e+00  2.46820000e+04\n",
      "  -4.22988778e+00]\n",
      " [ 5.00000000e+01  1.00000000e+00 -3.50000000e+00  1.62740000e+04\n",
      "  -4.41844062e+00]\n",
      " [ 5.10000000e+01  1.00000000e+00 -2.00000000e+00  2.23750000e+04\n",
      "  -4.65829440e+00]\n",
      " [ 5.20000000e+01  1.00000000e+00 -3.00000000e+00  1.28180000e+04\n",
      "  -5.32653620e+00]\n",
      " [ 5.30000000e+01  1.00000000e+00 -1.50000000e+00  1.57440000e+04\n",
      "  -5.15823405e+00]\n",
      " [ 5.40000000e+01  1.00000000e+00 -1.50000000e+00  1.31610000e+04\n",
      "  -5.33377244e+00]\n",
      " [ 5.50000000e+01  1.00000000e+00 -3.50000000e+00  6.77000000e+03\n",
      "  -5.16886025e+00]\n",
      " [ 5.60000000e+01  1.00000000e+00 -2.25000000e+00  8.11300000e+03\n",
      "  -4.85193872e+00]\n",
      " [ 5.70000000e+01  1.00000000e+00 -3.25000000e+00  1.62110000e+04\n",
      "  -4.92277289e+00]\n",
      " [ 5.80000000e+01  1.00000000e+00 -2.00000000e+00  1.16870000e+04\n",
      "  -5.11527382e+00]\n",
      " [ 5.90000000e+01  1.00000000e+00 -2.50000000e+00  1.18970000e+04\n",
      "  -4.91591796e+00]\n",
      " [ 6.00000000e+01  1.00000000e+00 -2.25000000e+00  1.13520000e+04\n",
      "  -5.10787590e+00]\n",
      " [ 6.10000000e+01  1.00000000e+00 -2.50000000e+00  9.62200000e+03\n",
      "  -5.01767044e+00]\n",
      " [ 6.20000000e+01  1.00000000e+00 -3.00000000e+00  8.98300000e+03\n",
      "  -4.78750700e+00]\n",
      " [ 6.30000000e+01  1.00000000e+00 -1.75000000e+00  1.47140000e+04\n",
      "  -4.57425427e+00]\n",
      " [ 6.40000000e+01  1.00000000e+00 -3.00000000e+00  2.11050000e+04\n",
      "  -4.43821793e+00]\n",
      " [ 6.50000000e+01  1.00000000e+00 -2.50000000e+00  1.66820000e+04\n",
      "  -4.37643090e+00]\n",
      " [ 6.60000000e+01  1.00000000e+00 -3.25000000e+00  2.24180000e+04\n",
      "  -4.69321692e+00]\n",
      " [ 6.70000000e+01  1.00000000e+00 -2.75000000e+00  1.64190000e+04\n",
      "  -4.35715763e+00]\n",
      " [ 6.80000000e+01  1.00000000e+00 -2.50000000e+00  2.48170000e+04\n",
      "  -4.61220533e+00]\n",
      " [ 6.90000000e+01  1.00000000e+00 -1.00000000e+00  9.67100000e+03\n",
      "  -4.80776693e+00]\n",
      " [ 7.00000000e+01  1.00000000e+00 -2.75000000e+00  2.90970000e+04\n",
      "  -2.79610996e+00]\n",
      " [ 7.10000000e+01  1.00000000e+00 -2.00000000e+00  9.23400000e+03\n",
      "  -4.66885966e+00]\n",
      " [ 7.20000000e+01  1.00000000e+00 -1.75000000e+00  5.79230000e+04\n",
      "  -4.43670588e+00]\n",
      " [ 7.30000000e+01  1.00000000e+00 -2.25000000e+00  1.42090000e+04\n",
      "  -4.90424272e+00]\n",
      " [ 7.40000000e+01  1.00000000e+00 -2.25000000e+00  1.31630000e+04\n",
      "  -4.60961559e+00]\n",
      " [ 7.50000000e+01  1.00000000e+00 -1.75000000e+00  1.31610000e+04\n",
      "  -4.74143445e+00]\n",
      " [ 7.60000000e+01  1.00000000e+00 -2.00000000e+00  7.49400000e+03\n",
      "  -4.28643351e+00]\n",
      " [ 7.70000000e+01  1.00000000e+00 -2.75000000e+00  1.22170000e+04\n",
      "  -3.90370210e+00]\n",
      " [ 7.80000000e+01  1.00000000e+00 -1.50000000e+00  1.31610000e+04\n",
      "  -4.80119026e+00]\n",
      " [ 7.90000000e+01  1.00000000e+00 -2.75000000e+00  1.44350000e+04\n",
      "  -4.83724749e+00]\n",
      " [ 8.00000000e+01  1.00000000e+00 -2.25000000e+00  1.30790000e+04\n",
      "  -4.64871082e+00]\n",
      " [ 8.10000000e+01  1.00000000e+00 -2.00000000e+00  1.33180000e+04\n",
      "  -4.73449140e+00]\n",
      " [ 8.20000000e+01  1.00000000e+00 -3.50000000e+00  1.43620000e+04\n",
      "  -4.68064270e+00]\n",
      " [ 8.30000000e+01  1.00000000e+00 -2.75000000e+00  1.31630000e+04\n",
      "  -4.79901726e+00]\n",
      " [ 8.40000000e+01  1.00000000e+00 -5.00000000e-01  1.14350000e+04\n",
      "  -3.72958519e+00]\n",
      " [ 8.50000000e+01  1.00000000e+00 -2.75000000e+00  1.05690000e+04\n",
      "  -3.42200554e+00]\n",
      " [ 8.60000000e+01  1.00000000e+00 -2.75000000e+00  3.39830000e+04\n",
      "  -2.58735254e+00]\n",
      " [ 8.70000000e+01  1.00000000e+00 -2.75000000e+00  1.32760000e+04\n",
      "  -4.58270411e+00]\n",
      " [ 8.80000000e+01  1.00000000e+00 -3.00000000e+00  1.33650000e+04\n",
      "  -1.39137507e+00]\n",
      " [ 8.90000000e+01  1.00000000e+00 -2.50000000e+00  1.61920000e+04\n",
      "  -3.80173675e+00]\n",
      " [ 9.00000000e+01  1.00000000e+00  0.00000000e+00  1.81740000e+04\n",
      "  -4.65580837e+00]\n",
      " [ 9.10000000e+01  1.00000000e+00 -3.25000000e+00  1.80410000e+04\n",
      "  -4.36702964e+00]\n",
      " [ 9.20000000e+01  1.00000000e+00 -2.00000000e+00  2.16290000e+04\n",
      "  -4.26763000e+00]\n",
      " [ 9.30000000e+01  1.00000000e+00 -3.50000000e+00  2.48590000e+04\n",
      "  -4.81774325e+00]\n",
      " [ 9.40000000e+01  1.00000000e+00 -3.75000000e+00  1.90600000e+04\n",
      "  -4.26272565e+00]\n",
      " [ 9.50000000e+01  1.00000000e+00 -1.75000000e+00  2.11870000e+04\n",
      "  -4.58527175e+00]\n",
      " [ 9.60000000e+01  1.00000000e+00 -3.75000000e+00  2.04680000e+04\n",
      "  -4.77796807e+00]\n",
      " [ 9.70000000e+01  1.00000000e+00 -3.00000000e+00  2.21930000e+04\n",
      "  -4.21117593e+00]\n",
      " [ 9.80000000e+01  1.00000000e+00 -1.25000000e+00  1.79490000e+04\n",
      "  -4.78790763e+00]\n",
      " [ 9.90000000e+01  1.00000000e+00 -2.25000000e+00  2.26800000e+04\n",
      "  -4.61574107e+00]]\n",
      "Writing to file data.csv\n",
      "**************************************************\n",
      "Data save complete\n",
      "**************************************************\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Step 6: Generate Figures\n",
    "\n",
    "Create a chart to compare the different heuristic functions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "fig_path = '../figures/'\n",
    "fig_name = 'rideshare_'+'_line_plot'+'.pdf'\n",
    "or_suite.plots.plot_line_plots(path_list_line, algo_list_line, fig_path, fig_name, int(nEps / 40)+1)\n",
    "\n",
    "additional_metric = {}\n",
    "fig_name = 'rideshare_'+'_'+'_radar_plot'+'.pdf'\n",
    "or_suite.plots.plot_radar_plots(path_list_radar, algo_list_radar,\n",
    "fig_path, fig_name,\n",
    "additional_metric\n",
    ")\n",
    "\n",
    "# TODO: Import figures and display\n"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "Failed to process string with tex because latex could not be found",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda3/envs/ORSuite/lib/python3.8/site-packages/matplotlib/texmanager.py\u001b[0m in \u001b[0;36m_run_checked_subprocess\u001b[0;34m(self, command, tex, cwd)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m             report = subprocess.check_output(\n\u001b[0m\u001b[1;32m    253\u001b[0m                 \u001b[0mcommand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcwd\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcwd\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexcache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/ORSuite/lib/python3.8/subprocess.py\u001b[0m in \u001b[0;36mcheck_output\u001b[0;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n\u001b[0m\u001b[1;32m    412\u001b[0m                **kwargs).stdout\n",
      "\u001b[0;32m/opt/anaconda3/envs/ORSuite/lib/python3.8/subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/ORSuite/lib/python3.8/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001b[0m\n\u001b[1;32m    853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 854\u001b[0;31m             self._execute_child(args, executable, preexec_fn, close_fds,\n\u001b[0m\u001b[1;32m    855\u001b[0m                                 \u001b[0mpass_fds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/ORSuite/lib/python3.8/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)\u001b[0m\n\u001b[1;32m   1701\u001b[0m                         \u001b[0merr_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1702\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1703\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'latex'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/l5/43sgrxps7lbcv1k2xnpw4g5h0000gn/T/ipykernel_14111/1450781.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfig_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../figures/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfig_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'rideshare_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_line_plot'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.pdf'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mor_suite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplots\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_line_plots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_list_line\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malgo_list_line\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfig_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfig_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnEps\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0madditional_metric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Developer/ORSuite/or_suite/plots.py\u001b[0m in \u001b[0;36mplot_line_plots\u001b[0;34m(path_list, algo_list, fig_path, fig_name, plot_freq)\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_ylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Observed Memory Usage (B)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Episode'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtight_layout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbbox_to_anchor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'upper left'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/ORSuite/lib/python3.8/site-packages/matplotlib/_api/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    469\u001b[0m                 \u001b[0;34m\"parameter will become keyword-only %(removal)s.\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m                 name=name, obj_type=f\"parameter of {func.__name__}()\")\n\u001b[0;32m--> 471\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/ORSuite/lib/python3.8/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mtight_layout\u001b[0;34m(pad, h_pad, w_pad, rect)\u001b[0m\n\u001b[1;32m   1659\u001b[0m         \u001b[0msubplots\u001b[0m \u001b[0marea\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mincluding\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m     \"\"\"\n\u001b[0;32m-> 1661\u001b[0;31m     \u001b[0mgcf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtight_layout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_pad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mh_pad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_pad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw_pad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1662\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/ORSuite/lib/python3.8/site-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36mtight_layout\u001b[0;34m(self, pad, h_pad, w_pad, rect)\u001b[0m\n\u001b[1;32m   3162\u001b[0m                else suppress())\n\u001b[1;32m   3163\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3164\u001b[0;31m             kwargs = get_tight_layout_figure(\n\u001b[0m\u001b[1;32m   3165\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubplotspec_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3166\u001b[0m                 pad=pad, h_pad=h_pad, w_pad=w_pad, rect=rect)\n",
      "\u001b[0;32m/opt/anaconda3/envs/ORSuite/lib/python3.8/site-packages/matplotlib/tight_layout.py\u001b[0m in \u001b[0;36mget_tight_layout_figure\u001b[0;34m(fig, axes_list, subplotspec_list, renderer, pad, h_pad, w_pad, rect)\u001b[0m\n\u001b[1;32m    310\u001b[0m                               (colNum2 + 1) * div_col - 1))\n\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m     kwargs = auto_adjust_subplotpars(fig, renderer,\n\u001b[0m\u001b[1;32m    313\u001b[0m                                      \u001b[0mnrows_ncols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_nrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_ncols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m                                      \u001b[0mnum1num2_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum1num2_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/ORSuite/lib/python3.8/site-packages/matplotlib/tight_layout.py\u001b[0m in \u001b[0;36mauto_adjust_subplotpars\u001b[0;34m(fig, renderer, nrows_ncols, num1num2_list, subplot_list, ax_bbox_list, pad, h_pad, w_pad, rect)\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_visible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                     \u001b[0mbb\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tightbbox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfor_layout_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                     \u001b[0mbb\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tightbbox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/ORSuite/lib/python3.8/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36mget_tightbbox\u001b[0;34m(self, renderer, call_axes_locator, bbox_extra_artists, for_layout_only)\u001b[0m\n\u001b[1;32m   4428\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxaxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_visible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4429\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4430\u001b[0;31m                     bb_xaxis = self.xaxis.get_tightbbox(\n\u001b[0m\u001b[1;32m   4431\u001b[0m                         renderer, for_layout_only=for_layout_only)\n\u001b[1;32m   4432\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/ORSuite/lib/python3.8/site-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36mget_tightbbox\u001b[0;34m(self, renderer, for_layout_only)\u001b[0m\n\u001b[1;32m   1086\u001b[0m         \u001b[0mticks_to_draw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_ticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1088\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_label_position\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1089\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m         \u001b[0;31m# go back to just this axis's tick labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/ORSuite/lib/python3.8/site-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36m_update_label_position\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   2083\u001b[0m         \u001b[0;31m# get bounding boxes for this axis and any siblings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2084\u001b[0m         \u001b[0;31m# that have been set by `fig.align_xlabels()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2085\u001b[0;31m         \u001b[0mbboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbboxes2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tick_boxes_siblings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2086\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2087\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_position\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/ORSuite/lib/python3.8/site-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36m_get_tick_boxes_siblings\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   1871\u001b[0m             \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1872\u001b[0m             \u001b[0mticks_to_draw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_ticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1873\u001b[0;31m             \u001b[0mtlb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtlb2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tick_bboxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mticks_to_draw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1874\u001b[0m             \u001b[0mbboxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtlb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1875\u001b[0m             \u001b[0mbboxes2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtlb2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/ORSuite/lib/python3.8/site-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36m_get_tick_bboxes\u001b[0;34m(self, ticks, renderer)\u001b[0m\n\u001b[1;32m   1066\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_tick_bboxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mticks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m         \u001b[0;34m\"\"\"Return lists of bboxes for ticks' label1's and label2's.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1068\u001b[0;31m         return ([tick.label1.get_window_extent(renderer)\n\u001b[0m\u001b[1;32m   1069\u001b[0m                  for tick in ticks if tick.label1.get_visible()],\n\u001b[1;32m   1070\u001b[0m                 [tick.label2.get_window_extent(renderer)\n",
      "\u001b[0;32m/opt/anaconda3/envs/ORSuite/lib/python3.8/site-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1066\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_tick_bboxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mticks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m         \u001b[0;34m\"\"\"Return lists of bboxes for ticks' label1's and label2's.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1068\u001b[0;31m         return ([tick.label1.get_window_extent(renderer)\n\u001b[0m\u001b[1;32m   1069\u001b[0m                  for tick in ticks if tick.label1.get_visible()],\n\u001b[1;32m   1070\u001b[0m                 [tick.label2.get_window_extent(renderer)\n",
      "\u001b[0;32m/opt/anaconda3/envs/ORSuite/lib/python3.8/site-packages/matplotlib/text.py\u001b[0m in \u001b[0;36mget_window_extent\u001b[0;34m(self, renderer, dpi)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m             \u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_layout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_renderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_unitless_position\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/ORSuite/lib/python3.8/site-packages/matplotlib/text.py\u001b[0m in \u001b[0;36m_get_layout\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0;31m# Full vertical extent of font, including ascenders and descenders:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m         _, lp_h, lp_d = renderer.get_text_width_height_descent(\n\u001b[0m\u001b[1;32m    307\u001b[0m             \u001b[0;34m\"lp\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fontproperties\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m             ismath=\"TeX\" if self.get_usetex() else False)\n",
      "\u001b[0;32m/opt/anaconda3/envs/ORSuite/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mget_text_width_height_descent\u001b[0;34m(self, s, prop, ismath)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mtexmanager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_texmanager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mfontsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_size_in_points\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             w, h, d = texmanager.get_text_width_height_descent(\n\u001b[0m\u001b[1;32m    230\u001b[0m                 s, fontsize, renderer=self)\n\u001b[1;32m    231\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/ORSuite/lib/python3.8/site-packages/matplotlib/texmanager.py\u001b[0m in \u001b[0;36mget_text_width_height_descent\u001b[0;34m(self, tex, fontsize, renderer)\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m             \u001b[0;31m# use dviread.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m             \u001b[0mdvifile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_dvi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mdviread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDvi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdvifile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m72\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdpi_fraction\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdvi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m                 \u001b[0mpage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdvi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/ORSuite/lib/python3.8/site-packages/matplotlib/texmanager.py\u001b[0m in \u001b[0;36mmake_dvi\u001b[0;34m(self, tex, fontsize)\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0;31m# and thus replace() works atomically.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mTemporaryDirectory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdvifile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtmpdir\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m                 self._run_checked_subprocess(\n\u001b[0m\u001b[1;32m    292\u001b[0m                     [\"latex\", \"-interaction=nonstopmode\", \"--halt-on-error\",\n\u001b[1;32m    293\u001b[0m                      texfile], tex, cwd=tmpdir)\n",
      "\u001b[0;32m/opt/anaconda3/envs/ORSuite/lib/python3.8/site-packages/matplotlib/texmanager.py\u001b[0m in \u001b[0;36m_run_checked_subprocess\u001b[0;34m(self, command, tex, cwd)\u001b[0m\n\u001b[1;32m    254\u001b[0m                 stderr=subprocess.STDOUT)\n\u001b[1;32m    255\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mFileNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m    257\u001b[0m                 \u001b[0;34m'Failed to process string with tex because {} could not be '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m                 'found'.format(command[0])) from exc\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to process string with tex because latex could not be found"
     ]
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "Failed to process string with tex because latex could not be found",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda3/envs/ORSuite/lib/python3.8/site-packages/matplotlib/texmanager.py\u001b[0m in \u001b[0;36m_run_checked_subprocess\u001b[0;34m(self, command, tex, cwd)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m             report = subprocess.check_output(\n\u001b[0m\u001b[1;32m    253\u001b[0m                 \u001b[0mcommand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcwd\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcwd\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexcache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/ORSuite/lib/python3.8/subprocess.py\u001b[0m in \u001b[0;36mcheck_output\u001b[0;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n\u001b[0m\u001b[1;32m    412\u001b[0m                **kwargs).stdout\n",
      "\u001b[0;32m/opt/anaconda3/envs/ORSuite/lib/python3.8/subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/ORSuite/lib/python3.8/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001b[0m\n\u001b[1;32m    853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 854\u001b[0;31m             self._execute_child(args, executable, preexec_fn, close_fds,\n\u001b[0m\u001b[1;32m    855\u001b[0m                                 \u001b[0mpass_fds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/ORSuite/lib/python3.8/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)\u001b[0m\n\u001b[1;32m   1701\u001b[0m                         \u001b[0merr_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1702\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1703\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'latex'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda3/envs/ORSuite/lib/python3.8/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    339\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mprinter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m             \u001b[0;31m# Finally look for special method names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/ORSuite/lib/python3.8/site-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(fig)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'png'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'retina'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'png2x'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mretina_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/ORSuite/lib/python3.8/site-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(fig, fmt, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mFigureCanvasBase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_io\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytes_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfmt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'svg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/ORSuite/lib/python3.8/site-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2228\u001b[0m                        else suppress())\n\u001b[1;32m   2229\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2230\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2232\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbbox_inches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/ORSuite/lib/python3.8/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdraw_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rasterizing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_rasterizing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/ORSuite/lib/python3.8/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/ORSuite/lib/python3.8/site-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   2782\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tight_layout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2783\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2784\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtight_layout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tight_parameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2785\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2786\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/ORSuite/lib/python3.8/site-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36mtight_layout\u001b[0;34m(self, pad, h_pad, w_pad, rect)\u001b[0m\n\u001b[1;32m   3162\u001b[0m                else suppress())\n\u001b[1;32m   3163\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3164\u001b[0;31m             kwargs = get_tight_layout_figure(\n\u001b[0m\u001b[1;32m   3165\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubplotspec_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3166\u001b[0m                 pad=pad, h_pad=h_pad, w_pad=w_pad, rect=rect)\n",
      "\u001b[0;32m/opt/anaconda3/envs/ORSuite/lib/python3.8/site-packages/matplotlib/tight_layout.py\u001b[0m in \u001b[0;36mget_tight_layout_figure\u001b[0;34m(fig, axes_list, subplotspec_list, renderer, pad, h_pad, w_pad, rect)\u001b[0m\n\u001b[1;32m    310\u001b[0m                               (colNum2 + 1) * div_col - 1))\n\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m     kwargs = auto_adjust_subplotpars(fig, renderer,\n\u001b[0m\u001b[1;32m    313\u001b[0m                                      \u001b[0mnrows_ncols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_nrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_ncols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m                                      \u001b[0mnum1num2_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum1num2_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/ORSuite/lib/python3.8/site-packages/matplotlib/tight_layout.py\u001b[0m in \u001b[0;36mauto_adjust_subplotpars\u001b[0;34m(fig, renderer, nrows_ncols, num1num2_list, subplot_list, ax_bbox_list, pad, h_pad, w_pad, rect)\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_visible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                     \u001b[0mbb\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tightbbox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfor_layout_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                     \u001b[0mbb\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tightbbox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/ORSuite/lib/python3.8/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36mget_tightbbox\u001b[0;34m(self, renderer, call_axes_locator, bbox_extra_artists, for_layout_only)\u001b[0m\n\u001b[1;32m   4428\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxaxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_visible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4429\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4430\u001b[0;31m                     bb_xaxis = self.xaxis.get_tightbbox(\n\u001b[0m\u001b[1;32m   4431\u001b[0m                         renderer, for_layout_only=for_layout_only)\n\u001b[1;32m   4432\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/ORSuite/lib/python3.8/site-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36mget_tightbbox\u001b[0;34m(self, renderer, for_layout_only)\u001b[0m\n\u001b[1;32m   1086\u001b[0m         \u001b[0mticks_to_draw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_ticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1088\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_label_position\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1089\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m         \u001b[0;31m# go back to just this axis's tick labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/ORSuite/lib/python3.8/site-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36m_update_label_position\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   2083\u001b[0m         \u001b[0;31m# get bounding boxes for this axis and any siblings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2084\u001b[0m         \u001b[0;31m# that have been set by `fig.align_xlabels()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2085\u001b[0;31m         \u001b[0mbboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbboxes2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tick_boxes_siblings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2086\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2087\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_position\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/ORSuite/lib/python3.8/site-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36m_get_tick_boxes_siblings\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   1871\u001b[0m             \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1872\u001b[0m             \u001b[0mticks_to_draw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_ticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1873\u001b[0;31m             \u001b[0mtlb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtlb2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tick_bboxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mticks_to_draw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1874\u001b[0m             \u001b[0mbboxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtlb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1875\u001b[0m             \u001b[0mbboxes2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtlb2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/ORSuite/lib/python3.8/site-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36m_get_tick_bboxes\u001b[0;34m(self, ticks, renderer)\u001b[0m\n\u001b[1;32m   1066\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_tick_bboxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mticks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m         \u001b[0;34m\"\"\"Return lists of bboxes for ticks' label1's and label2's.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1068\u001b[0;31m         return ([tick.label1.get_window_extent(renderer)\n\u001b[0m\u001b[1;32m   1069\u001b[0m                  for tick in ticks if tick.label1.get_visible()],\n\u001b[1;32m   1070\u001b[0m                 [tick.label2.get_window_extent(renderer)\n",
      "\u001b[0;32m/opt/anaconda3/envs/ORSuite/lib/python3.8/site-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1066\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_tick_bboxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mticks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m         \u001b[0;34m\"\"\"Return lists of bboxes for ticks' label1's and label2's.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1068\u001b[0;31m         return ([tick.label1.get_window_extent(renderer)\n\u001b[0m\u001b[1;32m   1069\u001b[0m                  for tick in ticks if tick.label1.get_visible()],\n\u001b[1;32m   1070\u001b[0m                 [tick.label2.get_window_extent(renderer)\n",
      "\u001b[0;32m/opt/anaconda3/envs/ORSuite/lib/python3.8/site-packages/matplotlib/text.py\u001b[0m in \u001b[0;36mget_window_extent\u001b[0;34m(self, renderer, dpi)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m             \u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_layout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_renderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_unitless_position\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/ORSuite/lib/python3.8/site-packages/matplotlib/text.py\u001b[0m in \u001b[0;36m_get_layout\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0;31m# Full vertical extent of font, including ascenders and descenders:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m         _, lp_h, lp_d = renderer.get_text_width_height_descent(\n\u001b[0m\u001b[1;32m    307\u001b[0m             \u001b[0;34m\"lp\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fontproperties\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m             ismath=\"TeX\" if self.get_usetex() else False)\n",
      "\u001b[0;32m/opt/anaconda3/envs/ORSuite/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mget_text_width_height_descent\u001b[0;34m(self, s, prop, ismath)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mtexmanager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_texmanager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mfontsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_size_in_points\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             w, h, d = texmanager.get_text_width_height_descent(\n\u001b[0m\u001b[1;32m    230\u001b[0m                 s, fontsize, renderer=self)\n\u001b[1;32m    231\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/ORSuite/lib/python3.8/site-packages/matplotlib/texmanager.py\u001b[0m in \u001b[0;36mget_text_width_height_descent\u001b[0;34m(self, tex, fontsize, renderer)\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m             \u001b[0;31m# use dviread.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m             \u001b[0mdvifile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_dvi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mdviread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDvi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdvifile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m72\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdpi_fraction\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdvi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m                 \u001b[0mpage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdvi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/ORSuite/lib/python3.8/site-packages/matplotlib/texmanager.py\u001b[0m in \u001b[0;36mmake_dvi\u001b[0;34m(self, tex, fontsize)\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0;31m# and thus replace() works atomically.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mTemporaryDirectory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdvifile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtmpdir\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m                 self._run_checked_subprocess(\n\u001b[0m\u001b[1;32m    292\u001b[0m                     [\"latex\", \"-interaction=nonstopmode\", \"--halt-on-error\",\n\u001b[1;32m    293\u001b[0m                      texfile], tex, cwd=tmpdir)\n",
      "\u001b[0;32m/opt/anaconda3/envs/ORSuite/lib/python3.8/site-packages/matplotlib/texmanager.py\u001b[0m in \u001b[0;36m_run_checked_subprocess\u001b[0;34m(self, command, tex, cwd)\u001b[0m\n\u001b[1;32m    254\u001b[0m                 stderr=subprocess.STDOUT)\n\u001b[1;32m    255\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mFileNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m    257\u001b[0m                 \u001b[0;34m'Failed to process string with tex because {} could not be '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m                 'found'.format(command[0])) from exc\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to process string with tex because latex could not be found"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 1080x360 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4c142dd05be15695d7d0f22ccf2092ca7b90c6a6af78118fc1f80db7c62802c3"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('ORSuite': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}