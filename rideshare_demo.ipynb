{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# OR Suite \n",
    "\n",
    "Reinforcement learning (RL) is a natural model for problems involving real-time sequential decision making, including inventory control, resource allocation, ridesharing systems, and ambulance routing. In these models, an agent interacts with a system that has stochastic transitions and rewards, and aims to control the system by maximizing their cumulative rewards across the trajectory. Reinforcement learning has been shown in practice to be an effective technique for learning complex control policies."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Ridesharing Code Demo\n",
    "\n",
    "Reinforcement learning (RL) is a natural model for problems involving real-time sequential decision making. In these models, a principal interacts with a system having stochastic transitions and rewards and aims to control the system online (by exploring available actions using real-time feedback) or offline (by exploiting known properties of the system).\n",
    "\n",
    "This project revolves around providing a unified landscape on scaling reinforcement learning algorithms to operations research domains.\n",
    "\n",
    "In this notebook, we walk through the Ambulance Routing problem with a 1-dimensional reinforcement learning environment in the space $X = [0, 1]$. Each ambulance in the problem can be located anywhere in $X$, so the state space is $S = X^k$, where $k$ is the number of ambulances. For this example there will be only one ambulance, so $k = 1$.\n",
    "\n",
    "The default distribution for call arrivals is $Beta(5, 2)$ over $[0,1]$, however any probability distribution defined over the interval $[0,1]$ is valid. The probability distribution can also change with each timestep.\n",
    "\n",
    "For example, in a problem with two ambulances, imagine the ambulances are initially located at $0.4$ and $0.6$, and the distance function being used is the $\\ell_1$ norm. The agent could choose to move the ambulances to $0.342$ and $0.887$. If a call arrived at $0.115$, ambulance 1, which was at $0.342$, would respond to that call, and the state at the end of the iteration would be ambulance 1 at $0.115$ and ambulance 2 at $0.887$. The agent could then choose new locations to move the ambulances to, and the cycle would repeat."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Step 1: Package Installation\n",
    "First we import the necessary packages"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "import or_suite\n",
    "import numpy as np\n",
    "import itertools as it\n",
    "\n",
    "import copy\n",
    "\n",
    "import os\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import gym"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Step 2: Pick problem parameters for the environment\n",
    "\n",
    "Here we use the ambulance metric environment as outlined in `or_suite/envs/ambulance/ambulance_metric.py`.  The package has default specifications for all of the environments in the file `or_suite/envs/env_configs.py`, and so we use one the default for the ambulance problem in a metric space.\n",
    "\n",
    "In addition, we need to specify the number of episodes for learning, and the number of iterations (in order to plot average results with confidence intervals)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "CONFIG =  or_suite.envs.env_configs.rideshare_graph_default_config\n",
    "\n",
    "epLen = CONFIG['epLen']\n",
    "nEps = 100\n",
    "numIters = 2\n",
    "\n",
    "epsilon = (nEps * epLen)**(-1 / 4)\n",
    "action_net = np.arange(start=0, stop=1, step=epsilon)\n",
    "state_net = np.arange(start=0, stop=1, step=epsilon)\n",
    "\n",
    "scaling_list = [0.1]\n",
    "\n",
    "def beta(step):\n",
    "    return np.random.beta(5,2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Step 3: Pick simulation parameters\n",
    "\n",
    "Next we need to specify parameters for the simulation. This includes setting a seed, the frequency to record the metrics, directory path for saving the data files, a deBug mode which prints the trajectory, etc."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "DEFAULT_SETTINGS = {'seed': 1, \n",
    "                    'recFreq': 1, \n",
    "                    'dirPath': '../data/rideshare/', \n",
    "                    'deBug': True, \n",
    "                    'nEps': nEps, \n",
    "                    'numIters': numIters, \n",
    "                    'saveTrajectory': False, \n",
    "                    'epLen' : 5,\n",
    "                    'render': False,\n",
    "                    'pickle': False\n",
    "                    }\n",
    "\n",
    "starting_state = CONFIG['starting_state']\n",
    "num_cars = CONFIG['num_cars']\n",
    "\n",
    "rideshare_env = gym.make('Rideshare-v0', config=CONFIG)\n",
    "mon_env = Monitor(rideshare_env)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Step 4: Pick list of algorithms\n",
    "\n",
    "We have several heuristics implemented for each of the environments defined, in addition to a `Random` policy, and some `RL discretization based` algorithms. \n",
    "\n",
    "The `Stable` agent only moves ambulances when responding to an incoming call and not in between calls. This means the policy $\\pi$ chosen by the agent for any given state $X$ will be $\\pi_h(X) = X$\n",
    "\n",
    "The `Median` agent takes a list of all past call arrivals sorted by arrival location, and partitions it into $k$ quantiles where $k$ is the number of ambulances. The algorithm then selects the middle data point in each quantile as the locations to station the ambulances."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "agents = { # 'SB PPO': PPO(MlpPolicy, mon_env, gamma=1, verbose=0, n_steps=epLen),\n",
    "'Random': or_suite.agents.rl.random.randomAgent(),\n",
    "'max_weight_fixed' : or_suite.agents.rideshare.max_weight_fixed.maxWeightFixedAgent(CONFIG['epLen'], num_cars, [1 for _ in range(len(starting_state))]),\n",
    "'closest_car' : or_suite.agents.rideshare.closest_car.closetCarAgent(CONFIG['epLen'], CONFIG)\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Step 5: Run Simulations\n",
    "\n",
    "Run the different heuristics in the environment"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "path_list_line = []\n",
    "algo_list_line = []\n",
    "path_list_radar = []\n",
    "algo_list_radar= []\n",
    "\n",
    "linspace_alpha = []\n",
    "\n",
    "# param_list = [list(p) for p in it.product(np.linspace(0,1,4),repeat = CONFIG['num_cars'])]\n",
    "\n",
    "for agent in agents:\n",
    "    print(agent)\n",
    "    DEFAULT_SETTINGS['dirPath'] = '../data/rideshare'+str(agent)+'_'+str(num_cars)\n",
    "   # if agent == 'max_weight_fixed':\n",
    "   #     or_suite.utils.run_single_algo_tune(rideshare_env,agents[agent], param_list, DEFAULT_SETTINGS)\n",
    "    if agent == 'SB PPO':\n",
    "        or_suite.utils.run_single_sb_algo(mon_env, agents[agent], DEFAULT_SETTINGS)\n",
    "    elif agent == 'AdaQL' or agent == 'Unif QL' or agent == 'AdaMB' or agent == 'Unif MB':\n",
    "        or_suite.utils.run_single_algo_tune(rideshare_env, agents[agent], scaling_list, DEFAULT_SETTINGS)\n",
    "    else:\n",
    "        or_suite.utils.run_single_algo(rideshare_env, agents[agent], DEFAULT_SETTINGS)\n",
    "\n",
    "    path_list_line.append('../data/rideshare_metric_'+str(agent)+'_'+str(num_cars))\n",
    "    algo_list_line.append(str(agent))\n",
    "    path_list_radar.append('../data/rideshare_metric_'+str(agent)+'_'+str(num_cars))\n",
    "    algo_list_radar.append(str(agent))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Random\n",
      "**************************************************\n",
      "Running experiment\n",
      "**************************************************\n",
      "Episode : 0\n",
      "state : [1 2 3 4 1 3]\n",
      "action : 1\n",
      "new state: [1 1 3 5 0 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 1 3 5 0 0]\n",
      "action : 2\n",
      "new state: [1 1 3 5 3 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [1 1 3 5 3 1]\n",
      "action : 1\n",
      "new state: [1 1 3 5 0 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [1 1 3 5 0 1]\n",
      "action : 0\n",
      "new state: [0 2 3 5 1 0]\n",
      "reward: -0.0\n",
      "epReward so far: -20000.0\n",
      "state : [0 2 3 5 1 0]\n",
      "action : 0\n",
      "new state: [0 2 3 5 2 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -30000.0\n",
      "final state: [0 2 3 5 2 1]\n",
      "Episode : 1\n",
      "state : [0 2 3 5 2 1]\n",
      "action : 1\n",
      "new state: [0 2 3 5 2 1]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [0 2 3 5 2 1]\n",
      "action : 0\n",
      "new state: [0 2 3 5 2 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "final state: [0 2 3 5 2 0]\n",
      "Episode : 2\n",
      "state : [0 2 3 5 2 0]\n",
      "action : 3\n",
      "new state: [0 2 3 5 2 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 2 3 5 2 0]\n",
      "action : 3\n",
      "new state: [0 2 3 5 2 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [0 2 3 5 2 0]\n",
      "action : 1\n",
      "new state: [0 2 3 5 1 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -30000.0\n",
      "state : [0 2 3 5 1 1]\n",
      "action : 2\n",
      "new state: [0 2 3 5 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -40000.0\n",
      "state : [0 2 3 5 0 2]\n",
      "action : 2\n",
      "new state: [0 2 3 5 1 3]\n",
      "reward: -1.0\n",
      "epReward so far: -40001.0\n",
      "final state: [0 2 3 5 1 3]\n",
      "Episode : 3\n",
      "state : [0 2 3 5 1 3]\n",
      "action : 3\n",
      "new state: [0 2 3 5 2 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 2 3 5 2 1]\n",
      "action : 2\n",
      "new state: [0 2 3 5 0 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [0 2 3 5 0 1]\n",
      "action : 3\n",
      "new state: [0 2 3 5 1 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -30000.0\n",
      "state : [0 2 3 5 1 3]\n",
      "action : 1\n",
      "new state: [0 1 3 6 1 0]\n",
      "reward: -0.0\n",
      "epReward so far: -30000.0\n",
      "state : [0 1 3 6 1 0]\n",
      "action : 2\n",
      "new state: [0 1 3 6 3 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -40000.0\n",
      "final state: [0 1 3 6 3 2]\n",
      "Episode : 4\n",
      "state : [0 1 3 6 3 2]\n",
      "action : 0\n",
      "new state: [0 1 3 6 1 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "final state: [0 1 3 6 1 1]\n",
      "Episode : 5\n",
      "state : [0 1 3 6 1 1]\n",
      "action : 1\n",
      "new state: [0 1 3 6 0 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 1 3 6 0 0]\n",
      "action : 2\n",
      "new state: [0 1 3 6 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 1 3 6 0 2]\n",
      "action : 3\n",
      "new state: [0 1 4 5 1 3]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [0 1 4 5 1 3]\n",
      "action : 3\n",
      "new state: [0 1 4 5 3 0]\n",
      "reward: -1.0\n",
      "epReward so far: -10002.0\n",
      "state : [0 1 4 5 3 0]\n",
      "action : 1\n",
      "new state: [1 0 4 5 3 2]\n",
      "reward: -1.0\n",
      "epReward so far: -10003.0\n",
      "final state: [1 0 4 5 3 2]\n",
      "Episode : 6\n",
      "state : [1 0 4 5 3 2]\n",
      "action : 2\n",
      "new state: [1 0 4 5 2 1]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [1 0 4 5 2 1]\n",
      "action : 3\n",
      "new state: [1 0 4 5 1 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "state : [1 0 4 5 1 3]\n",
      "action : 3\n",
      "new state: [1 0 4 5 3 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "state : [1 0 4 5 3 1]\n",
      "action : 1\n",
      "new state: [1 0 4 5 0 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -30001.0\n",
      "final state: [1 0 4 5 0 3]\n",
      "Episode : 7\n",
      "state : [1 0 4 5 0 3]\n",
      "action : 3\n",
      "new state: [1 0 4 5 3 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [1 0 4 5 3 2]\n",
      "action : 3\n",
      "new state: [1 0 5 4 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [1 0 5 4 2 2]\n",
      "action : 1\n",
      "new state: [1 0 5 4 3 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "final state: [1 0 5 4 3 3]\n",
      "Episode : 8\n",
      "state : [1 0 5 4 3 3]\n",
      "action : 3\n",
      "new state: [1 0 5 4 1 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 0 5 4 1 3]\n",
      "action : 2\n",
      "new state: [1 0 5 4 2 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [1 0 5 4 2 2]\n",
      "action : 3\n",
      "new state: [1 0 6 3 0 3]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [1 0 6 3 0 3]\n",
      "action : 1\n",
      "new state: [1 0 6 3 3 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "final state: [1 0 6 3 3 1]\n",
      "Episode : 9\n",
      "state : [1 0 6 3 3 1]\n",
      "action : 1\n",
      "new state: [1 0 6 3 1 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "final state: [1 0 6 3 1 1]\n",
      "Episode : 10\n",
      "state : [1 0 6 3 1 1]\n",
      "action : 3\n",
      "new state: [1 0 6 3 0 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [1 0 6 3 0 1]\n",
      "action : 0\n",
      "new state: [0 1 6 3 0 1]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 1 6 3 0 1]\n",
      "action : 2\n",
      "new state: [0 2 5 3 0 3]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [0 2 5 3 0 3]\n",
      "action : 1\n",
      "new state: [0 1 5 4 3 1]\n",
      "reward: -1.0\n",
      "epReward so far: -10002.0\n",
      "state : [0 1 5 4 3 1]\n",
      "action : 3\n",
      "new state: [0 2 5 3 0 3]\n",
      "reward: -0.0\n",
      "epReward so far: -10002.0\n",
      "final state: [0 2 5 3 0 3]\n",
      "Episode : 11\n",
      "state : [0 2 5 3 0 3]\n",
      "action : 3\n",
      "new state: [0 2 5 3 0 3]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [0 2 5 3 0 3]\n",
      "action : 3\n",
      "new state: [0 2 5 3 0 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "state : [0 2 5 3 0 1]\n",
      "action : 2\n",
      "new state: [0 2 5 3 2 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "state : [0 2 5 3 2 0]\n",
      "action : 1\n",
      "new state: [1 1 5 3 3 0]\n",
      "reward: -1.0\n",
      "epReward so far: -20002.0\n",
      "state : [1 1 5 3 3 0]\n",
      "action : 1\n",
      "new state: [2 0 5 3 1 0]\n",
      "reward: -1.0\n",
      "epReward so far: -20003.0\n",
      "final state: [2 0 5 3 1 0]\n",
      "Episode : 12\n",
      "state : [2 0 5 3 1 0]\n",
      "action : 0\n",
      "new state: [2 0 5 3 0 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 0 5 3 0 0]\n",
      "action : 2\n",
      "new state: [3 0 4 3 2 0]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [3 0 4 3 2 0]\n",
      "action : 0\n",
      "new state: [3 0 4 3 2 3]\n",
      "reward: -1.0\n",
      "epReward so far: -10002.0\n",
      "state : [3 0 4 3 2 3]\n",
      "action : 3\n",
      "new state: [3 0 4 3 0 0]\n",
      "reward: -1.0\n",
      "epReward so far: -10003.0\n",
      "state : [3 0 4 3 0 0]\n",
      "action : 2\n",
      "new state: [4 0 3 3 2 2]\n",
      "reward: -1.0\n",
      "epReward so far: -10004.0\n",
      "final state: [4 0 3 3 2 2]\n",
      "Episode : 13\n",
      "state : [4 0 3 3 2 2]\n",
      "action : 2\n",
      "new state: [4 0 3 3 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [4 0 3 3 2 2]\n",
      "action : 0\n",
      "new state: [4 0 3 3 3 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [4 0 3 3 3 3]\n",
      "action : 3\n",
      "new state: [4 0 3 3 3 1]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [4 0 3 3 3 1]\n",
      "action : 3\n",
      "new state: [4 1 3 2 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [4 1 3 2 0 2]\n",
      "action : 0\n",
      "new state: [3 1 4 2 1 1]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "final state: [3 1 4 2 1 1]\n",
      "Episode : 14\n",
      "state : [3 1 4 2 1 1]\n",
      "action : 2\n",
      "new state: [3 1 4 2 1 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [3 1 4 2 1 1]\n",
      "action : 2\n",
      "new state: [3 2 3 2 0 3]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [3 2 3 2 0 3]\n",
      "action : 1\n",
      "new state: [3 1 3 3 1 1]\n",
      "reward: -1.0\n",
      "epReward so far: -10002.0\n",
      "state : [3 1 3 3 1 1]\n",
      "action : 3\n",
      "new state: [3 2 3 2 1 3]\n",
      "reward: -1.0\n",
      "epReward so far: -10003.0\n",
      "state : [3 2 3 2 1 3]\n",
      "action : 1\n",
      "new state: [3 1 3 3 2 1]\n",
      "reward: -0.0\n",
      "epReward so far: -10003.0\n",
      "final state: [3 1 3 3 2 1]\n",
      "Episode : 15\n",
      "state : [3 1 3 3 2 1]\n",
      "action : 0\n",
      "new state: [2 2 3 3 3 2]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [2 2 3 3 3 2]\n",
      "action : 0\n",
      "new state: [1 2 4 3 3 3]\n",
      "reward: -1.0\n",
      "epReward so far: -2.0\n",
      "state : [1 2 4 3 3 3]\n",
      "action : 1\n",
      "new state: [1 1 4 4 1 2]\n",
      "reward: -1.0\n",
      "epReward so far: -3.0\n",
      "state : [1 1 4 4 1 2]\n",
      "action : 3\n",
      "new state: [1 1 4 4 1 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10003.0\n",
      "state : [1 1 4 4 1 0]\n",
      "action : 3\n",
      "new state: [1 1 4 4 1 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -20003.0\n",
      "final state: [1 1 4 4 1 0]\n",
      "Episode : 16\n",
      "state : [1 1 4 4 1 0]\n",
      "action : 1\n",
      "new state: [2 0 4 4 1 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 0 4 4 1 1]\n",
      "action : 1\n",
      "new state: [2 0 4 4 3 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "final state: [2 0 4 4 3 1]\n",
      "Episode : 17\n",
      "state : [2 0 4 4 3 1]\n",
      "action : 2\n",
      "new state: [2 1 3 4 1 3]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [2 1 3 4 1 3]\n",
      "action : 0\n",
      "new state: [2 1 3 4 2 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 1 3 4 2 1]\n",
      "action : 3\n",
      "new state: [2 2 3 3 2 2]\n",
      "reward: -1.0\n",
      "epReward so far: -10002.0\n",
      "state : [2 2 3 3 2 2]\n",
      "action : 1\n",
      "new state: [2 1 4 3 1 3]\n",
      "reward: -1.0\n",
      "epReward so far: -10003.0\n",
      "state : [2 1 4 3 1 3]\n",
      "action : 3\n",
      "new state: [2 1 4 3 1 1]\n",
      "reward: -1.0\n",
      "epReward so far: -10004.0\n",
      "final state: [2 1 4 3 1 1]\n",
      "Episode : 18\n",
      "state : [2 1 4 3 1 1]\n",
      "action : 0\n",
      "new state: [1 2 4 3 0 3]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [1 2 4 3 0 3]\n",
      "action : 0\n",
      "new state: [1 2 4 3 0 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "state : [1 2 4 3 0 1]\n",
      "action : 1\n",
      "new state: [1 2 4 3 2 3]\n",
      "reward: -1.0\n",
      "epReward so far: -10002.0\n",
      "state : [1 2 4 3 2 3]\n",
      "action : 3\n",
      "new state: [1 2 4 3 2 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -20002.0\n",
      "state : [1 2 4 3 2 0]\n",
      "action : 1\n",
      "new state: [1 2 4 3 0 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -30002.0\n",
      "final state: [1 2 4 3 0 0]\n",
      "Episode : 19\n",
      "state : [1 2 4 3 0 0]\n",
      "action : 2\n",
      "new state: [1 2 4 3 2 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [1 2 4 3 2 3]\n",
      "action : 2\n",
      "new state: [1 2 3 4 0 3]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [1 2 3 4 0 3]\n",
      "action : 1\n",
      "new state: [1 2 3 4 2 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [1 2 3 4 2 0]\n",
      "action : 2\n",
      "new state: [2 2 2 4 2 3]\n",
      "reward: -0.0\n",
      "epReward so far: -20000.0\n",
      "state : [2 2 2 4 2 3]\n",
      "action : 2\n",
      "new state: [2 2 1 5 3 3]\n",
      "reward: -0.0\n",
      "epReward so far: -20000.0\n",
      "final state: [2 2 1 5 3 3]\n",
      "Episode : 20\n",
      "state : [2 2 1 5 3 3]\n",
      "action : 2\n",
      "new state: [2 2 1 5 2 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 2 1 5 2 3]\n",
      "action : 0\n",
      "new state: [1 2 1 6 3 2]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [1 2 1 6 3 2]\n",
      "action : 0\n",
      "new state: [1 2 1 6 2 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "state : [1 2 1 6 2 0]\n",
      "action : 0\n",
      "new state: [1 2 1 6 3 3]\n",
      "reward: -1.0\n",
      "epReward so far: -20002.0\n",
      "state : [1 2 1 6 3 3]\n",
      "action : 2\n",
      "new state: [1 2 1 6 1 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -30002.0\n",
      "final state: [1 2 1 6 1 2]\n",
      "Episode : 21\n",
      "state : [1 2 1 6 1 2]\n",
      "action : 1\n",
      "new state: [1 1 2 6 3 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 1 2 6 3 1]\n",
      "action : 3\n",
      "new state: [1 2 2 5 1 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 2 2 5 1 3]\n",
      "action : 0\n",
      "new state: [1 2 2 5 2 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [1 2 2 5 2 2]\n",
      "action : 1\n",
      "new state: [1 1 3 5 2 1]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [1 1 3 5 2 1]\n",
      "action : 0\n",
      "new state: [0 2 3 5 0 0]\n",
      "reward: -1.0\n",
      "epReward so far: -10002.0\n",
      "final state: [0 2 3 5 0 0]\n",
      "Episode : 22\n",
      "state : [0 2 3 5 0 0]\n",
      "action : 0\n",
      "new state: [0 2 3 5 2 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "final state: [0 2 3 5 2 1]\n",
      "Episode : 23\n",
      "state : [0 2 3 5 2 1]\n",
      "action : 0\n",
      "new state: [0 2 3 5 3 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "final state: [0 2 3 5 3 3]\n",
      "Episode : 24\n",
      "state : [0 2 3 5 3 3]\n",
      "action : 2\n",
      "new state: [0 2 3 5 2 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 2 3 5 2 2]\n",
      "action : 0\n",
      "new state: [0 2 3 5 1 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "final state: [0 2 3 5 1 2]\n",
      "Episode : 25\n",
      "state : [0 2 3 5 1 2]\n",
      "action : 0\n",
      "new state: [0 2 3 5 1 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "final state: [0 2 3 5 1 2]\n",
      "Episode : 26\n",
      "state : [0 2 3 5 1 2]\n",
      "action : 2\n",
      "new state: [0 2 3 5 0 0]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [0 2 3 5 0 0]\n",
      "action : 0\n",
      "new state: [0 2 3 5 2 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "final state: [0 2 3 5 2 2]\n",
      "Episode : 27\n",
      "state : [0 2 3 5 2 2]\n",
      "action : 2\n",
      "new state: [0 2 3 5 2 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 2 3 5 2 0]\n",
      "action : 2\n",
      "new state: [1 2 2 5 0 3]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [1 2 2 5 0 3]\n",
      "action : 0\n",
      "new state: [1 2 2 5 0 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [1 2 2 5 0 3]\n",
      "action : 3\n",
      "new state: [1 2 2 5 3 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -30000.0\n",
      "state : [1 2 2 5 3 0]\n",
      "action : 3\n",
      "new state: [1 2 2 5 3 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -40000.0\n",
      "final state: [1 2 2 5 3 1]\n",
      "Episode : 28\n",
      "state : [1 2 2 5 3 1]\n",
      "action : 3\n",
      "new state: [1 2 2 5 3 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [1 2 2 5 3 0]\n",
      "action : 1\n",
      "new state: [1 2 2 5 2 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [1 2 2 5 2 2]\n",
      "action : 3\n",
      "new state: [1 2 3 4 0 2]\n",
      "reward: -1.0\n",
      "epReward so far: -20001.0\n",
      "state : [1 2 3 4 0 2]\n",
      "action : 1\n",
      "new state: [1 1 4 4 1 1]\n",
      "reward: -1.0\n",
      "epReward so far: -20002.0\n",
      "state : [1 1 4 4 1 1]\n",
      "action : 0\n",
      "new state: [0 2 4 4 2 1]\n",
      "reward: -1.0\n",
      "epReward so far: -20003.0\n",
      "final state: [0 2 4 4 2 1]\n",
      "Episode : 29\n",
      "state : [0 2 4 4 2 1]\n",
      "action : 3\n",
      "new state: [0 2 4 4 1 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 2 4 4 1 0]\n",
      "action : 2\n",
      "new state: [1 2 3 4 3 2]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [1 2 3 4 3 2]\n",
      "action : 3\n",
      "new state: [1 2 3 4 0 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "state : [1 2 3 4 0 3]\n",
      "action : 1\n",
      "new state: [1 1 3 5 3 3]\n",
      "reward: -1.0\n",
      "epReward so far: -20002.0\n",
      "state : [1 1 3 5 3 3]\n",
      "action : 3\n",
      "new state: [1 1 3 5 3 0]\n",
      "reward: -0.0\n",
      "epReward so far: -20002.0\n",
      "final state: [1 1 3 5 3 0]\n",
      "Episode : 30\n",
      "state : [1 1 3 5 3 0]\n",
      "action : 1\n",
      "new state: [2 0 3 5 3 0]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [2 0 3 5 3 0]\n",
      "action : 3\n",
      "new state: [2 0 3 5 2 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 0 3 5 2 1]\n",
      "action : 1\n",
      "new state: [2 0 3 5 3 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "final state: [2 0 3 5 3 2]\n",
      "Episode : 31\n",
      "state : [2 0 3 5 3 2]\n",
      "action : 0\n",
      "new state: [2 0 3 5 1 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 0 3 5 1 0]\n",
      "action : 0\n",
      "new state: [2 0 3 5 3 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [2 0 3 5 3 1]\n",
      "action : 1\n",
      "new state: [2 0 3 5 2 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -30000.0\n",
      "final state: [2 0 3 5 2 3]\n",
      "Episode : 32\n",
      "state : [2 0 3 5 2 3]\n",
      "action : 3\n",
      "new state: [2 0 3 5 2 0]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [2 0 3 5 2 0]\n",
      "action : 2\n",
      "new state: [2 0 3 5 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 0 3 5 0 2]\n",
      "action : 2\n",
      "new state: [2 0 3 5 3 2]\n",
      "reward: -1.0\n",
      "epReward so far: -10002.0\n",
      "state : [2 0 3 5 3 2]\n",
      "action : 1\n",
      "new state: [2 0 3 5 0 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20002.0\n",
      "final state: [2 0 3 5 0 3]\n",
      "Episode : 33\n",
      "state : [2 0 3 5 0 3]\n",
      "action : 2\n",
      "new state: [2 0 2 6 3 0]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [2 0 2 6 3 0]\n",
      "action : 1\n",
      "new state: [2 0 2 6 1 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "final state: [2 0 2 6 1 2]\n",
      "Episode : 34\n",
      "state : [2 0 2 6 1 2]\n",
      "action : 2\n",
      "new state: [2 0 2 6 3 2]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [2 0 2 6 3 2]\n",
      "action : 1\n",
      "new state: [2 0 2 6 2 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "final state: [2 0 2 6 2 2]\n",
      "Episode : 35\n",
      "state : [2 0 2 6 2 2]\n",
      "action : 0\n",
      "new state: [2 0 2 6 2 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 0 2 6 2 3]\n",
      "action : 3\n",
      "new state: [2 0 2 6 1 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [2 0 2 6 1 1]\n",
      "action : 3\n",
      "new state: [2 1 2 5 2 1]\n",
      "reward: -1.0\n",
      "epReward so far: -20001.0\n",
      "state : [2 1 2 5 2 1]\n",
      "action : 1\n",
      "new state: [2 1 2 5 2 2]\n",
      "reward: -1.0\n",
      "epReward so far: -20002.0\n",
      "state : [2 1 2 5 2 2]\n",
      "action : 2\n",
      "new state: [2 1 2 5 0 0]\n",
      "reward: -0.0\n",
      "epReward so far: -20002.0\n",
      "final state: [2 1 2 5 0 0]\n",
      "Episode : 36\n",
      "state : [2 1 2 5 0 0]\n",
      "action : 3\n",
      "new state: [3 1 2 4 3 2]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [3 1 2 4 3 2]\n",
      "action : 1\n",
      "new state: [3 1 2 4 3 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "state : [3 1 2 4 3 1]\n",
      "action : 1\n",
      "new state: [3 1 2 4 1 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "state : [3 1 2 4 1 1]\n",
      "action : 2\n",
      "new state: [3 2 1 4 2 0]\n",
      "reward: -1.0\n",
      "epReward so far: -20002.0\n",
      "state : [3 2 1 4 2 0]\n",
      "action : 2\n",
      "new state: [4 2 0 4 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: -20002.0\n",
      "final state: [4 2 0 4 0 2]\n",
      "Episode : 37\n",
      "state : [4 2 0 4 0 2]\n",
      "action : 1\n",
      "new state: [4 2 0 4 1 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [4 2 0 4 1 1]\n",
      "action : 1\n",
      "new state: [4 2 0 4 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [4 2 0 4 0 2]\n",
      "action : 1\n",
      "new state: [4 1 1 4 3 3]\n",
      "reward: -1.0\n",
      "epReward so far: -20001.0\n",
      "state : [4 1 1 4 3 3]\n",
      "action : 1\n",
      "new state: [4 0 1 5 1 3]\n",
      "reward: -1.0\n",
      "epReward so far: -20002.0\n",
      "state : [4 0 1 5 1 3]\n",
      "action : 3\n",
      "new state: [4 0 1 5 1 2]\n",
      "reward: -1.0\n",
      "epReward so far: -20003.0\n",
      "final state: [4 0 1 5 1 2]\n",
      "Episode : 38\n",
      "state : [4 0 1 5 1 2]\n",
      "action : 1\n",
      "new state: [4 0 1 5 2 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "final state: [4 0 1 5 2 0]\n",
      "Episode : 39\n",
      "state : [4 0 1 5 2 0]\n",
      "action : 3\n",
      "new state: [4 0 1 5 2 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [4 0 1 5 2 2]\n",
      "action : 0\n",
      "new state: [4 0 1 5 1 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [4 0 1 5 1 2]\n",
      "action : 0\n",
      "new state: [4 0 1 5 0 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -30000.0\n",
      "state : [4 0 1 5 0 1]\n",
      "action : 3\n",
      "new state: [4 1 1 4 0 2]\n",
      "reward: -1.0\n",
      "epReward so far: -30001.0\n",
      "state : [4 1 1 4 0 2]\n",
      "action : 0\n",
      "new state: [3 1 2 4 2 0]\n",
      "reward: -0.0\n",
      "epReward so far: -30001.0\n",
      "final state: [3 1 2 4 2 0]\n",
      "Episode : 40\n",
      "state : [3 1 2 4 2 0]\n",
      "action : 2\n",
      "new state: [3 1 2 4 3 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [3 1 2 4 3 3]\n",
      "action : 3\n",
      "new state: [3 1 2 4 3 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [3 1 2 4 3 0]\n",
      "action : 0\n",
      "new state: [3 1 2 4 1 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -30000.0\n",
      "state : [3 1 2 4 1 2]\n",
      "action : 3\n",
      "new state: [3 1 3 3 2 1]\n",
      "reward: -1.0\n",
      "epReward so far: -30001.0\n",
      "state : [3 1 3 3 2 1]\n",
      "action : 2\n",
      "new state: [3 1 3 3 2 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -40001.0\n",
      "final state: [3 1 3 3 2 3]\n",
      "Episode : 41\n",
      "state : [3 1 3 3 2 3]\n",
      "action : 1\n",
      "new state: [3 0 3 4 0 2]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [3 0 3 4 0 2]\n",
      "action : 1\n",
      "new state: [3 0 3 4 2 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "final state: [3 0 3 4 2 2]\n",
      "Episode : 42\n",
      "state : [3 0 3 4 2 2]\n",
      "action : 1\n",
      "new state: [3 0 3 4 1 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "final state: [3 0 3 4 1 2]\n",
      "Episode : 43\n",
      "state : [3 0 3 4 1 2]\n",
      "action : 0\n",
      "new state: [3 0 3 4 1 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [3 0 3 4 1 0]\n",
      "action : 0\n",
      "new state: [3 0 3 4 3 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [3 0 3 4 3 1]\n",
      "action : 3\n",
      "new state: [3 1 3 3 2 0]\n",
      "reward: -0.0\n",
      "epReward so far: -20000.0\n",
      "state : [3 1 3 3 2 0]\n",
      "action : 2\n",
      "new state: [4 1 2 3 0 0]\n",
      "reward: -0.0\n",
      "epReward so far: -20000.0\n",
      "state : [4 1 2 3 0 0]\n",
      "action : 0\n",
      "new state: [4 1 2 3 2 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -30000.0\n",
      "final state: [4 1 2 3 2 0]\n",
      "Episode : 44\n",
      "state : [4 1 2 3 2 0]\n",
      "action : 3\n",
      "new state: [5 1 2 2 0 1]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [5 1 2 2 0 1]\n",
      "action : 0\n",
      "new state: [4 2 2 2 2 0]\n",
      "reward: -0.0\n",
      "epReward so far: -1.0\n",
      "state : [4 2 2 2 2 0]\n",
      "action : 1\n",
      "new state: [4 2 2 2 1 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "state : [4 2 2 2 1 1]\n",
      "action : 0\n",
      "new state: [4 2 2 2 2 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "state : [4 2 2 2 2 2]\n",
      "action : 3\n",
      "new state: [4 2 3 1 0 2]\n",
      "reward: -1.0\n",
      "epReward so far: -20002.0\n",
      "final state: [4 2 3 1 0 2]\n",
      "Episode : 45\n",
      "state : [4 2 3 1 0 2]\n",
      "action : 0\n",
      "new state: [3 2 4 1 1 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 2 4 1 1 1]\n",
      "action : 2\n",
      "new state: [3 2 4 1 0 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [3 2 4 1 0 1]\n",
      "action : 0\n",
      "new state: [2 3 4 1 1 1]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 3 4 1 1 1]\n",
      "action : 1\n",
      "new state: [2 3 4 1 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [2 3 4 1 0 2]\n",
      "action : 3\n",
      "new state: [2 3 5 0 0 3]\n",
      "reward: -1.0\n",
      "epReward so far: -20001.0\n",
      "final state: [2 3 5 0 0 3]\n",
      "Episode : 46\n",
      "state : [2 3 5 0 0 3]\n",
      "action : 0\n",
      "new state: [1 3 5 1 2 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 3 5 1 2 0]\n",
      "action : 2\n",
      "new state: [2 3 4 1 1 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 3 4 1 1 3]\n",
      "action : 2\n",
      "new state: [2 3 3 2 2 0]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [2 3 3 2 2 0]\n",
      "action : 3\n",
      "new state: [3 3 3 1 3 1]\n",
      "reward: -1.0\n",
      "epReward so far: -2.0\n",
      "state : [3 3 3 1 3 1]\n",
      "action : 1\n",
      "new state: [3 3 3 1 0 0]\n",
      "reward: -1.0\n",
      "epReward so far: -3.0\n",
      "final state: [3 3 3 1 0 0]\n",
      "Episode : 47\n",
      "state : [3 3 3 1 0 0]\n",
      "action : 1\n",
      "new state: [4 2 3 1 3 3]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [4 2 3 1 3 3]\n",
      "action : 2\n",
      "new state: [4 2 3 1 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "state : [4 2 3 1 0 2]\n",
      "action : 3\n",
      "new state: [4 2 4 0 3 1]\n",
      "reward: -1.0\n",
      "epReward so far: -10002.0\n",
      "state : [4 2 4 0 3 1]\n",
      "action : 0\n",
      "new state: [3 3 4 0 3 0]\n",
      "reward: -1.0\n",
      "epReward so far: -10003.0\n",
      "state : [3 3 4 0 3 0]\n",
      "action : 3\n",
      "new state: [3 3 4 0 3 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20003.0\n",
      "final state: [3 3 4 0 3 2]\n",
      "Episode : 48\n",
      "state : [3 3 4 0 3 2]\n",
      "action : 1\n",
      "new state: [3 2 5 0 2 2]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [3 2 5 0 2 2]\n",
      "action : 2\n",
      "new state: [3 2 5 0 2 0]\n",
      "reward: -0.0\n",
      "epReward so far: -1.0\n",
      "state : [3 2 5 0 2 0]\n",
      "action : 3\n",
      "new state: [3 2 5 0 2 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "final state: [3 2 5 0 2 1]\n",
      "Episode : 49\n",
      "state : [3 2 5 0 2 1]\n",
      "action : 2\n",
      "new state: [3 3 4 0 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 3 4 0 2 2]\n",
      "action : 2\n",
      "new state: [3 3 4 0 0 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 3 4 0 0 0]\n",
      "action : 2\n",
      "new state: [4 3 3 0 3 2]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [4 3 3 0 3 2]\n",
      "action : 2\n",
      "new state: [4 3 3 0 2 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "state : [4 3 3 0 2 1]\n",
      "action : 2\n",
      "new state: [4 4 2 0 1 2]\n",
      "reward: -0.0\n",
      "epReward so far: -10001.0\n",
      "final state: [4 4 2 0 1 2]\n",
      "Episode : 50\n",
      "state : [4 4 2 0 1 2]\n",
      "action : 2\n",
      "new state: [4 4 2 0 1 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [4 4 2 0 1 3]\n",
      "action : 3\n",
      "new state: [4 4 2 0 3 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "final state: [4 4 2 0 3 3]\n",
      "Episode : 51\n",
      "state : [4 4 2 0 3 3]\n",
      "action : 2\n",
      "new state: [4 4 2 0 3 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [4 4 2 0 3 3]\n",
      "action : 1\n",
      "new state: [4 3 2 1 1 0]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [4 3 2 1 1 0]\n",
      "action : 1\n",
      "new state: [4 3 2 1 2 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "state : [4 3 2 1 2 2]\n",
      "action : 1\n",
      "new state: [4 2 3 1 0 0]\n",
      "reward: -1.0\n",
      "epReward so far: -20002.0\n",
      "state : [4 2 3 1 0 0]\n",
      "action : 3\n",
      "new state: [5 2 3 0 2 3]\n",
      "reward: -1.0\n",
      "epReward so far: -20003.0\n",
      "final state: [5 2 3 0 2 3]\n",
      "Episode : 52\n",
      "state : [5 2 3 0 2 3]\n",
      "action : 0\n",
      "new state: [5 2 3 0 2 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [5 2 3 0 2 0]\n",
      "action : 2\n",
      "new state: [6 2 2 0 3 0]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [6 2 2 0 3 0]\n",
      "action : 1\n",
      "new state: [6 2 2 0 3 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [6 2 2 0 3 3]\n",
      "action : 1\n",
      "new state: [6 1 2 1 0 2]\n",
      "reward: -1.0\n",
      "epReward so far: -20001.0\n",
      "state : [6 1 2 1 0 2]\n",
      "action : 1\n",
      "new state: [6 0 3 1 3 3]\n",
      "reward: -1.0\n",
      "epReward so far: -20002.0\n",
      "final state: [6 0 3 1 3 3]\n",
      "Episode : 53\n",
      "state : [6 0 3 1 3 3]\n",
      "action : 3\n",
      "new state: [6 0 3 1 3 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [6 0 3 1 3 0]\n",
      "action : 0\n",
      "new state: [6 0 3 1 0 2]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [6 0 3 1 0 2]\n",
      "action : 2\n",
      "new state: [6 0 3 1 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "state : [6 0 3 1 0 2]\n",
      "action : 0\n",
      "new state: [6 0 3 1 3 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "state : [6 0 3 1 3 3]\n",
      "action : 2\n",
      "new state: [6 0 3 1 0 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -30001.0\n",
      "final state: [6 0 3 1 0 0]\n",
      "Episode : 54\n",
      "state : [6 0 3 1 0 0]\n",
      "action : 1\n",
      "new state: [6 0 3 1 1 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "final state: [6 0 3 1 1 2]\n",
      "Episode : 55\n",
      "state : [6 0 3 1 1 2]\n",
      "action : 3\n",
      "new state: [6 0 4 0 1 1]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [6 0 4 0 1 1]\n",
      "action : 0\n",
      "new state: [6 0 4 0 2 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "state : [6 0 4 0 2 3]\n",
      "action : 0\n",
      "new state: [5 0 4 1 3 1]\n",
      "reward: -1.0\n",
      "epReward so far: -10002.0\n",
      "state : [5 0 4 1 3 1]\n",
      "action : 1\n",
      "new state: [5 0 4 1 1 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20002.0\n",
      "final state: [5 0 4 1 1 1]\n",
      "Episode : 56\n",
      "state : [5 0 4 1 1 1]\n",
      "action : 2\n",
      "new state: [5 1 3 1 2 3]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [5 1 3 1 2 3]\n",
      "action : 1\n",
      "new state: [5 0 3 2 0 3]\n",
      "reward: -1.0\n",
      "epReward so far: -2.0\n",
      "state : [5 0 3 2 0 3]\n",
      "action : 0\n",
      "new state: [4 0 3 3 3 3]\n",
      "reward: -0.0\n",
      "epReward so far: -2.0\n",
      "state : [4 0 3 3 3 3]\n",
      "action : 2\n",
      "new state: [4 0 3 3 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10002.0\n",
      "state : [4 0 3 3 0 2]\n",
      "action : 3\n",
      "new state: [4 0 3 3 1 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20002.0\n",
      "final state: [4 0 3 3 1 1]\n",
      "Episode : 57\n",
      "state : [4 0 3 3 1 1]\n",
      "action : 1\n",
      "new state: [4 0 3 3 1 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "final state: [4 0 3 3 1 3]\n",
      "Episode : 58\n",
      "state : [4 0 3 3 1 3]\n",
      "action : 1\n",
      "new state: [4 0 3 3 3 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "final state: [4 0 3 3 3 2]\n",
      "Episode : 59\n",
      "state : [4 0 3 3 3 2]\n",
      "action : 2\n",
      "new state: [4 0 3 3 3 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [4 0 3 3 3 3]\n",
      "action : 3\n",
      "new state: [4 0 3 3 1 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [4 0 3 3 1 3]\n",
      "action : 2\n",
      "new state: [4 0 2 4 0 0]\n",
      "reward: -1.0\n",
      "epReward so far: -20001.0\n",
      "state : [4 0 2 4 0 0]\n",
      "action : 2\n",
      "new state: [5 0 1 4 0 2]\n",
      "reward: -1.0\n",
      "epReward so far: -20002.0\n",
      "state : [5 0 1 4 0 2]\n",
      "action : 1\n",
      "new state: [5 0 1 4 0 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -30002.0\n",
      "final state: [5 0 1 4 0 0]\n",
      "Episode : 60\n",
      "state : [5 0 1 4 0 0]\n",
      "action : 2\n",
      "new state: [6 0 0 4 3 3]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [6 0 0 4 3 3]\n",
      "action : 3\n",
      "new state: [6 0 0 4 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: -1.0\n",
      "state : [6 0 0 4 0 2]\n",
      "action : 3\n",
      "new state: [6 0 0 4 0 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "state : [6 0 0 4 0 0]\n",
      "action : 2\n",
      "new state: [6 0 0 4 1 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "final state: [6 0 0 4 1 2]\n",
      "Episode : 61\n",
      "state : [6 0 0 4 1 2]\n",
      "action : 1\n",
      "new state: [6 0 0 4 2 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "final state: [6 0 0 4 2 2]\n",
      "Episode : 62\n",
      "state : [6 0 0 4 2 2]\n",
      "action : 0\n",
      "new state: [5 0 1 4 3 0]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [5 0 1 4 3 0]\n",
      "action : 1\n",
      "new state: [5 0 1 4 2 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "final state: [5 0 1 4 2 2]\n",
      "Episode : 63\n",
      "state : [5 0 1 4 2 2]\n",
      "action : 2\n",
      "new state: [5 0 1 4 3 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [5 0 1 4 3 1]\n",
      "action : 2\n",
      "new state: [5 0 1 4 0 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [5 0 1 4 0 1]\n",
      "action : 2\n",
      "new state: [5 1 0 4 0 3]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [5 1 0 4 0 3]\n",
      "action : 3\n",
      "new state: [5 1 0 4 1 2]\n",
      "reward: -1.0\n",
      "epReward so far: -10002.0\n",
      "state : [5 1 0 4 1 2]\n",
      "action : 1\n",
      "new state: [5 1 0 4 0 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20002.0\n",
      "final state: [5 1 0 4 0 1]\n",
      "Episode : 64\n",
      "state : [5 1 0 4 0 1]\n",
      "action : 0\n",
      "new state: [4 2 0 4 3 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [4 2 0 4 3 2]\n",
      "action : 1\n",
      "new state: [4 1 1 4 2 0]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [4 1 1 4 2 0]\n",
      "action : 1\n",
      "new state: [5 0 1 4 1 0]\n",
      "reward: -1.0\n",
      "epReward so far: -2.0\n",
      "state : [5 0 1 4 1 0]\n",
      "action : 1\n",
      "new state: [5 0 1 4 3 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10002.0\n",
      "final state: [5 0 1 4 3 0]\n",
      "Episode : 65\n",
      "state : [5 0 1 4 3 0]\n",
      "action : 2\n",
      "new state: [6 0 0 4 1 3]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [6 0 0 4 1 3]\n",
      "action : 1\n",
      "new state: [6 0 0 4 1 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "final state: [6 0 0 4 1 1]\n",
      "Episode : 66\n",
      "state : [6 0 0 4 1 1]\n",
      "action : 1\n",
      "new state: [6 0 0 4 1 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "final state: [6 0 0 4 1 0]\n",
      "Episode : 67\n",
      "state : [6 0 0 4 1 0]\n",
      "action : 2\n",
      "new state: [6 0 0 4 2 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "final state: [6 0 0 4 2 1]\n",
      "Episode : 68\n",
      "state : [6 0 0 4 2 1]\n",
      "action : 1\n",
      "new state: [6 0 0 4 3 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "final state: [6 0 0 4 3 2]\n",
      "Episode : 69\n",
      "state : [6 0 0 4 3 2]\n",
      "action : 2\n",
      "new state: [6 0 0 4 0 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "final state: [6 0 0 4 0 1]\n",
      "Episode : 70\n",
      "state : [6 0 0 4 0 1]\n",
      "action : 3\n",
      "new state: [6 1 0 3 3 1]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [6 1 0 3 3 1]\n",
      "action : 3\n",
      "new state: [6 1 0 3 2 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "state : [6 1 0 3 2 0]\n",
      "action : 0\n",
      "new state: [6 1 0 3 3 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "state : [6 1 0 3 3 3]\n",
      "action : 3\n",
      "new state: [6 1 0 3 3 0]\n",
      "reward: -0.0\n",
      "epReward so far: -20001.0\n",
      "state : [6 1 0 3 3 0]\n",
      "action : 2\n",
      "new state: [6 1 0 3 2 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -30001.0\n",
      "final state: [6 1 0 3 2 0]\n",
      "Episode : 71\n",
      "state : [6 1 0 3 2 0]\n",
      "action : 0\n",
      "new state: [6 1 0 3 0 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [6 1 0 3 0 1]\n",
      "action : 1\n",
      "new state: [6 1 0 3 3 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [6 1 0 3 3 0]\n",
      "action : 2\n",
      "new state: [6 1 0 3 0 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -30000.0\n",
      "final state: [6 1 0 3 0 0]\n",
      "Episode : 72\n",
      "state : [6 1 0 3 0 0]\n",
      "action : 2\n",
      "new state: [6 1 0 3 1 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "final state: [6 1 0 3 1 0]\n",
      "Episode : 73\n",
      "state : [6 1 0 3 1 0]\n",
      "action : 3\n",
      "new state: [6 1 0 3 0 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [6 1 0 3 0 1]\n",
      "action : 2\n",
      "new state: [6 1 0 3 1 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "final state: [6 1 0 3 1 3]\n",
      "Episode : 74\n",
      "state : [6 1 0 3 1 3]\n",
      "action : 1\n",
      "new state: [6 0 0 4 2 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [6 0 0 4 2 0]\n",
      "action : 0\n",
      "new state: [6 0 0 4 0 1]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [6 0 0 4 0 1]\n",
      "action : 2\n",
      "new state: [6 0 0 4 1 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "final state: [6 0 0 4 1 0]\n",
      "Episode : 75\n",
      "state : [6 0 0 4 1 0]\n",
      "action : 0\n",
      "new state: [6 0 0 4 2 1]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [6 0 0 4 2 1]\n",
      "action : 1\n",
      "new state: [6 0 0 4 1 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "final state: [6 0 0 4 1 0]\n",
      "Episode : 76\n",
      "state : [6 0 0 4 1 0]\n",
      "action : 3\n",
      "new state: [6 0 0 4 2 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [6 0 0 4 2 2]\n",
      "action : 3\n",
      "new state: [6 0 1 3 3 3]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [6 0 1 3 3 3]\n",
      "action : 2\n",
      "new state: [6 0 1 3 3 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "state : [6 0 1 3 3 3]\n",
      "action : 1\n",
      "new state: [6 0 1 3 0 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -30001.0\n",
      "final state: [6 0 1 3 0 3]\n",
      "Episode : 77\n",
      "state : [6 0 1 3 0 3]\n",
      "action : 3\n",
      "new state: [6 0 1 3 3 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [6 0 1 3 3 1]\n",
      "action : 2\n",
      "new state: [6 0 1 3 2 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [6 0 1 3 2 2]\n",
      "action : 1\n",
      "new state: [6 0 1 3 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -30000.0\n",
      "final state: [6 0 1 3 0 2]\n",
      "Episode : 78\n",
      "state : [6 0 1 3 0 2]\n",
      "action : 0\n",
      "new state: [5 0 2 3 1 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [5 0 2 3 1 0]\n",
      "action : 1\n",
      "new state: [5 0 2 3 1 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "final state: [5 0 2 3 1 0]\n",
      "Episode : 79\n",
      "state : [5 0 2 3 1 0]\n",
      "action : 3\n",
      "new state: [6 0 2 2 1 0]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [6 0 2 2 1 0]\n",
      "action : 3\n",
      "new state: [7 0 2 1 0 2]\n",
      "reward: -1.0\n",
      "epReward so far: -2.0\n",
      "state : [7 0 2 1 0 2]\n",
      "action : 0\n",
      "new state: [6 0 3 1 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: -2.0\n",
      "state : [6 0 3 1 0 2]\n",
      "action : 2\n",
      "new state: [6 0 3 1 0 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10002.0\n",
      "state : [6 0 3 1 0 0]\n",
      "action : 0\n",
      "new state: [6 0 3 1 3 2]\n",
      "reward: -0.0\n",
      "epReward so far: -10002.0\n",
      "final state: [6 0 3 1 3 2]\n",
      "Episode : 80\n",
      "state : [6 0 3 1 3 2]\n",
      "action : 2\n",
      "new state: [6 0 3 1 1 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [6 0 3 1 1 3]\n",
      "action : 2\n",
      "new state: [6 0 3 1 3 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [6 0 3 1 3 3]\n",
      "action : 2\n",
      "new state: [6 0 3 1 1 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -30000.0\n",
      "state : [6 0 3 1 1 3]\n",
      "action : 2\n",
      "new state: [6 0 3 1 3 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -40000.0\n",
      "state : [6 0 3 1 3 2]\n",
      "action : 1\n",
      "new state: [6 0 3 1 2 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -50000.0\n",
      "final state: [6 0 3 1 2 3]\n",
      "Episode : 81\n",
      "state : [6 0 3 1 2 3]\n",
      "action : 0\n",
      "new state: [5 0 3 2 2 0]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [5 0 3 2 2 0]\n",
      "action : 0\n",
      "new state: [5 0 3 2 0 2]\n",
      "reward: -1.0\n",
      "epReward so far: -2.0\n",
      "state : [5 0 3 2 0 2]\n",
      "action : 2\n",
      "new state: [5 0 3 2 0 0]\n",
      "reward: -1.0\n",
      "epReward so far: -3.0\n",
      "state : [5 0 3 2 0 0]\n",
      "action : 3\n",
      "new state: [6 0 3 1 0 2]\n",
      "reward: -1.0\n",
      "epReward so far: -4.0\n",
      "state : [6 0 3 1 0 2]\n",
      "action : 1\n",
      "new state: [6 0 3 1 0 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10004.0\n",
      "final state: [6 0 3 1 0 3]\n",
      "Episode : 82\n",
      "state : [6 0 3 1 0 3]\n",
      "action : 3\n",
      "new state: [6 0 3 1 0 3]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [6 0 3 1 0 3]\n",
      "action : 0\n",
      "new state: [5 0 3 2 1 2]\n",
      "reward: -0.0\n",
      "epReward so far: -1.0\n",
      "state : [5 0 3 2 1 2]\n",
      "action : 2\n",
      "new state: [5 0 3 2 3 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "state : [5 0 3 2 3 1]\n",
      "action : 2\n",
      "new state: [5 0 3 2 3 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "state : [5 0 3 2 3 0]\n",
      "action : 0\n",
      "new state: [5 0 3 2 3 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -30001.0\n",
      "final state: [5 0 3 2 3 1]\n",
      "Episode : 83\n",
      "state : [5 0 3 2 3 1]\n",
      "action : 2\n",
      "new state: [5 1 2 2 1 3]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [5 1 2 2 1 3]\n",
      "action : 0\n",
      "new state: [4 1 2 3 0 0]\n",
      "reward: -1.0\n",
      "epReward so far: -2.0\n",
      "state : [4 1 2 3 0 0]\n",
      "action : 0\n",
      "new state: [4 1 2 3 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: -2.0\n",
      "state : [4 1 2 3 2 2]\n",
      "action : 0\n",
      "new state: [4 1 2 3 2 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10002.0\n",
      "state : [4 1 2 3 2 3]\n",
      "action : 0\n",
      "new state: [3 1 2 4 1 3]\n",
      "reward: -1.0\n",
      "epReward so far: -10003.0\n",
      "final state: [3 1 2 4 1 3]\n",
      "Episode : 84\n",
      "state : [3 1 2 4 1 3]\n",
      "action : 2\n",
      "new state: [3 1 2 4 1 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [3 1 2 4 1 0]\n",
      "action : 1\n",
      "new state: [4 0 2 4 3 3]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [4 0 2 4 3 3]\n",
      "action : 2\n",
      "new state: [4 0 2 4 0 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [4 0 2 4 0 0]\n",
      "action : 3\n",
      "new state: [4 0 2 4 1 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -30000.0\n",
      "state : [4 0 2 4 1 2]\n",
      "action : 2\n",
      "new state: [4 0 2 4 0 0]\n",
      "reward: -1.0\n",
      "epReward so far: -30001.0\n",
      "final state: [4 0 2 4 0 0]\n",
      "Episode : 85\n",
      "state : [4 0 2 4 0 0]\n",
      "action : 2\n",
      "new state: [4 0 2 4 0 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [4 0 2 4 0 1]\n",
      "action : 0\n",
      "new state: [3 1 2 4 3 2]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [3 1 2 4 3 2]\n",
      "action : 0\n",
      "new state: [2 1 3 4 2 3]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 1 3 4 2 3]\n",
      "action : 3\n",
      "new state: [2 1 3 4 1 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "state : [2 1 3 4 1 1]\n",
      "action : 0\n",
      "new state: [1 2 3 4 0 0]\n",
      "reward: -1.0\n",
      "epReward so far: -20002.0\n",
      "final state: [1 2 3 4 0 0]\n",
      "Episode : 86\n",
      "state : [1 2 3 4 0 0]\n",
      "action : 2\n",
      "new state: [1 2 3 4 1 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [1 2 3 4 1 0]\n",
      "action : 1\n",
      "new state: [2 1 3 4 3 3]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 1 3 4 3 3]\n",
      "action : 2\n",
      "new state: [2 1 2 5 3 1]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 1 2 5 3 1]\n",
      "action : 2\n",
      "new state: [2 1 2 5 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "state : [2 1 2 5 0 2]\n",
      "action : 1\n",
      "new state: [2 0 3 5 2 1]\n",
      "reward: -1.0\n",
      "epReward so far: -20002.0\n",
      "final state: [2 0 3 5 2 1]\n",
      "Episode : 87\n",
      "state : [2 0 3 5 2 1]\n",
      "action : 2\n",
      "new state: [2 1 2 5 1 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 1 2 5 1 3]\n",
      "action : 0\n",
      "new state: [1 1 2 6 2 2]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [1 1 2 6 2 2]\n",
      "action : 3\n",
      "new state: [1 1 3 5 1 1]\n",
      "reward: -1.0\n",
      "epReward so far: -2.0\n",
      "state : [1 1 3 5 1 1]\n",
      "action : 3\n",
      "new state: [1 1 3 5 2 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10002.0\n",
      "state : [1 1 3 5 2 3]\n",
      "action : 3\n",
      "new state: [1 1 3 5 1 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20002.0\n",
      "final state: [1 1 3 5 1 1]\n",
      "Episode : 88\n",
      "state : [1 1 3 5 1 1]\n",
      "action : 3\n",
      "new state: [1 1 3 5 3 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [1 1 3 5 3 1]\n",
      "action : 2\n",
      "new state: [1 1 3 5 2 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [1 1 3 5 2 0]\n",
      "action : 1\n",
      "new state: [1 1 3 5 0 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -30000.0\n",
      "state : [1 1 3 5 0 1]\n",
      "action : 0\n",
      "new state: [1 1 3 5 3 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -40000.0\n",
      "state : [1 1 3 5 3 3]\n",
      "action : 1\n",
      "new state: [1 0 3 6 1 1]\n",
      "reward: -1.0\n",
      "epReward so far: -40001.0\n",
      "final state: [1 0 3 6 1 1]\n",
      "Episode : 89\n",
      "state : [1 0 3 6 1 1]\n",
      "action : 3\n",
      "new state: [1 0 3 6 3 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [1 0 3 6 3 1]\n",
      "action : 0\n",
      "new state: [1 0 3 6 1 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [1 0 3 6 1 0]\n",
      "action : 3\n",
      "new state: [1 0 3 6 3 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -30000.0\n",
      "state : [1 0 3 6 3 3]\n",
      "action : 0\n",
      "new state: [1 0 3 6 0 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -40000.0\n",
      "state : [1 0 3 6 0 0]\n",
      "action : 2\n",
      "new state: [2 0 2 6 0 2]\n",
      "reward: -1.0\n",
      "epReward so far: -40001.0\n",
      "final state: [2 0 2 6 0 2]\n",
      "Episode : 90\n",
      "state : [2 0 2 6 0 2]\n",
      "action : 2\n",
      "new state: [2 0 2 6 0 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 0 2 6 0 1]\n",
      "action : 1\n",
      "new state: [2 0 2 6 2 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "final state: [2 0 2 6 2 3]\n",
      "Episode : 91\n",
      "state : [2 0 2 6 2 3]\n",
      "action : 2\n",
      "new state: [2 0 1 7 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 0 1 7 2 2]\n",
      "action : 3\n",
      "new state: [2 0 1 7 2 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 0 1 7 2 2]\n",
      "action : 3\n",
      "new state: [2 0 1 7 1 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [2 0 1 7 1 1]\n",
      "action : 0\n",
      "new state: [1 1 1 7 2 3]\n",
      "reward: -1.0\n",
      "epReward so far: -20001.0\n",
      "state : [1 1 1 7 2 3]\n",
      "action : 0\n",
      "new state: [1 1 1 7 1 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -30001.0\n",
      "final state: [1 1 1 7 1 0]\n",
      "Episode : 92\n",
      "state : [1 1 1 7 1 0]\n",
      "action : 0\n",
      "new state: [1 1 1 7 2 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [1 1 1 7 2 1]\n",
      "action : 0\n",
      "new state: [1 1 1 7 0 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [1 1 1 7 0 1]\n",
      "action : 0\n",
      "new state: [0 2 1 7 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: -20000.0\n",
      "state : [0 2 1 7 2 2]\n",
      "action : 0\n",
      "new state: [0 2 1 7 2 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -30000.0\n",
      "final state: [0 2 1 7 2 1]\n",
      "Episode : 93\n",
      "state : [0 2 1 7 2 1]\n",
      "action : 0\n",
      "new state: [0 2 1 7 0 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "final state: [0 2 1 7 0 3]\n",
      "Episode : 94\n",
      "state : [0 2 1 7 0 3]\n",
      "action : 3\n",
      "new state: [0 2 1 7 3 1]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [0 2 1 7 3 1]\n",
      "action : 0\n",
      "new state: [0 2 1 7 2 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "final state: [0 2 1 7 2 0]\n",
      "Episode : 95\n",
      "state : [0 2 1 7 2 0]\n",
      "action : 2\n",
      "new state: [0 2 1 7 3 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 2 1 7 3 3]\n",
      "action : 3\n",
      "new state: [0 2 1 7 1 3]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 2 1 7 1 3]\n",
      "action : 2\n",
      "new state: [0 2 0 8 1 1]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [0 2 0 8 1 1]\n",
      "action : 1\n",
      "new state: [0 2 0 8 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: -10001.0\n",
      "state : [0 2 0 8 2 2]\n",
      "action : 2\n",
      "new state: [0 2 0 8 3 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "final state: [0 2 0 8 3 1]\n",
      "Episode : 96\n",
      "state : [0 2 0 8 3 1]\n",
      "action : 2\n",
      "new state: [0 2 0 8 2 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "final state: [0 2 0 8 2 3]\n",
      "Episode : 97\n",
      "state : [0 2 0 8 2 3]\n",
      "action : 2\n",
      "new state: [0 2 0 8 3 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "final state: [0 2 0 8 3 3]\n",
      "Episode : 98\n",
      "state : [0 2 0 8 3 3]\n",
      "action : 1\n",
      "new state: [0 2 0 8 1 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 2 0 8 1 1]\n",
      "action : 2\n",
      "new state: [0 2 0 8 3 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "final state: [0 2 0 8 3 0]\n",
      "Episode : 99\n",
      "state : [0 2 0 8 3 0]\n",
      "action : 1\n",
      "new state: [1 1 0 8 0 2]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [1 1 0 8 0 2]\n",
      "action : 0\n",
      "new state: [0 1 1 8 0 1]\n",
      "reward: -0.0\n",
      "epReward so far: -1.0\n",
      "state : [0 1 1 8 0 1]\n",
      "action : 1\n",
      "new state: [0 1 1 8 0 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "state : [0 1 1 8 0 3]\n",
      "action : 0\n",
      "new state: [0 1 1 8 3 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "final state: [0 1 1 8 3 2]\n",
      "Episode : 0\n",
      "state : [0 1 1 8 3 2]\n",
      "action : 2\n",
      "new state: [0 1 1 8 2 0]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [0 1 1 8 2 0]\n",
      "action : 3\n",
      "new state: [0 1 1 8 2 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "state : [0 1 1 8 2 0]\n",
      "action : 3\n",
      "new state: [1 1 1 7 3 1]\n",
      "reward: -1.0\n",
      "epReward so far: -10002.0\n",
      "state : [1 1 1 7 3 1]\n",
      "action : 1\n",
      "new state: [1 1 1 7 3 1]\n",
      "reward: -1.0\n",
      "epReward so far: -10003.0\n",
      "state : [1 1 1 7 3 1]\n",
      "action : 2\n",
      "new state: [1 2 0 7 2 1]\n",
      "reward: -1.0\n",
      "epReward so far: -10004.0\n",
      "final state: [1 2 0 7 2 1]\n",
      "Episode : 1\n",
      "state : [1 2 0 7 2 1]\n",
      "action : 3\n",
      "new state: [1 3 0 6 3 1]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [1 3 0 6 3 1]\n",
      "action : 2\n",
      "new state: [1 3 0 6 1 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "final state: [1 3 0 6 1 1]\n",
      "Episode : 2\n",
      "state : [1 3 0 6 1 1]\n",
      "action : 3\n",
      "new state: [1 4 0 5 2 1]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [1 4 0 5 2 1]\n",
      "action : 1\n",
      "new state: [1 4 0 5 2 3]\n",
      "reward: -1.0\n",
      "epReward so far: -2.0\n",
      "state : [1 4 0 5 2 3]\n",
      "action : 1\n",
      "new state: [1 3 0 6 2 1]\n",
      "reward: -1.0\n",
      "epReward so far: -3.0\n",
      "state : [1 3 0 6 2 1]\n",
      "action : 1\n",
      "new state: [1 3 0 6 1 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10003.0\n",
      "state : [1 3 0 6 1 3]\n",
      "action : 1\n",
      "new state: [1 2 0 7 3 3]\n",
      "reward: -0.0\n",
      "epReward so far: -10003.0\n",
      "final state: [1 2 0 7 3 3]\n",
      "Episode : 3\n",
      "state : [1 2 0 7 3 3]\n",
      "action : 3\n",
      "new state: [1 2 0 7 0 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 2 0 7 0 1]\n",
      "action : 1\n",
      "new state: [1 2 0 7 3 0]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [1 2 0 7 3 0]\n",
      "action : 3\n",
      "new state: [1 2 0 7 2 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "state : [1 2 0 7 2 3]\n",
      "action : 2\n",
      "new state: [1 2 0 7 1 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "final state: [1 2 0 7 1 1]\n",
      "Episode : 4\n",
      "state : [1 2 0 7 1 1]\n",
      "action : 3\n",
      "new state: [1 3 0 6 0 3]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [1 3 0 6 0 3]\n",
      "action : 0\n",
      "new state: [0 3 0 7 0 1]\n",
      "reward: -0.0\n",
      "epReward so far: -1.0\n",
      "state : [0 3 0 7 0 1]\n",
      "action : 2\n",
      "new state: [0 3 0 7 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "final state: [0 3 0 7 0 2]\n",
      "Episode : 5\n",
      "state : [0 3 0 7 0 2]\n",
      "action : 2\n",
      "new state: [0 3 0 7 0 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "final state: [0 3 0 7 0 3]\n",
      "Episode : 6\n",
      "state : [0 3 0 7 0 3]\n",
      "action : 1\n",
      "new state: [0 3 0 7 3 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 3 0 7 3 1]\n",
      "action : 3\n",
      "new state: [0 3 0 7 2 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [0 3 0 7 2 2]\n",
      "action : 3\n",
      "new state: [0 3 0 7 0 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -30000.0\n",
      "state : [0 3 0 7 0 3]\n",
      "action : 3\n",
      "new state: [0 3 0 7 2 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -40000.0\n",
      "state : [0 3 0 7 2 2]\n",
      "action : 1\n",
      "new state: [0 2 1 7 0 3]\n",
      "reward: -1.0\n",
      "epReward so far: -40001.0\n",
      "final state: [0 2 1 7 0 3]\n",
      "Episode : 7\n",
      "state : [0 2 1 7 0 3]\n",
      "action : 1\n",
      "new state: [0 2 1 7 2 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 2 1 7 2 3]\n",
      "action : 2\n",
      "new state: [0 2 0 8 1 3]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 2 0 8 1 3]\n",
      "action : 3\n",
      "new state: [0 2 0 8 3 2]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [0 2 0 8 3 2]\n",
      "action : 1\n",
      "new state: [0 2 0 8 2 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "state : [0 2 0 8 2 3]\n",
      "action : 2\n",
      "new state: [0 2 0 8 1 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -30001.0\n",
      "final state: [0 2 0 8 1 1]\n",
      "Episode : 8\n",
      "state : [0 2 0 8 1 1]\n",
      "action : 3\n",
      "new state: [0 2 0 8 2 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 2 0 8 2 1]\n",
      "action : 2\n",
      "new state: [0 2 0 8 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "final state: [0 2 0 8 0 2]\n",
      "Episode : 9\n",
      "state : [0 2 0 8 0 2]\n",
      "action : 0\n",
      "new state: [0 2 0 8 3 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "final state: [0 2 0 8 3 3]\n",
      "Episode : 10\n",
      "state : [0 2 0 8 3 3]\n",
      "action : 0\n",
      "new state: [0 2 0 8 0 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "final state: [0 2 0 8 0 1]\n",
      "Episode : 11\n",
      "state : [0 2 0 8 0 1]\n",
      "action : 1\n",
      "new state: [0 2 0 8 2 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 2 0 8 2 1]\n",
      "action : 1\n",
      "new state: [0 2 0 8 2 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [0 2 0 8 2 0]\n",
      "action : 1\n",
      "new state: [0 2 0 8 1 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -30000.0\n",
      "state : [0 2 0 8 1 1]\n",
      "action : 1\n",
      "new state: [0 2 0 8 0 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -40000.0\n",
      "state : [0 2 0 8 0 3]\n",
      "action : 2\n",
      "new state: [0 2 0 8 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -50000.0\n",
      "final state: [0 2 0 8 0 2]\n",
      "Episode : 12\n",
      "state : [0 2 0 8 0 2]\n",
      "action : 2\n",
      "new state: [0 2 0 8 1 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "final state: [0 2 0 8 1 2]\n",
      "Episode : 13\n",
      "state : [0 2 0 8 1 2]\n",
      "action : 2\n",
      "new state: [0 2 0 8 3 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "final state: [0 2 0 8 3 3]\n",
      "Episode : 14\n",
      "state : [0 2 0 8 3 3]\n",
      "action : 3\n",
      "new state: [0 2 0 8 1 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 2 0 8 1 2]\n",
      "action : 3\n",
      "new state: [0 2 1 7 2 0]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [0 2 1 7 2 0]\n",
      "action : 1\n",
      "new state: [1 1 1 7 2 0]\n",
      "reward: -1.0\n",
      "epReward so far: -2.0\n",
      "state : [1 1 1 7 2 0]\n",
      "action : 1\n",
      "new state: [1 1 1 7 2 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10002.0\n",
      "state : [1 1 1 7 2 0]\n",
      "action : 0\n",
      "new state: [1 1 1 7 0 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20002.0\n",
      "final state: [1 1 1 7 0 3]\n",
      "Episode : 15\n",
      "state : [1 1 1 7 0 3]\n",
      "action : 1\n",
      "new state: [1 0 1 8 1 1]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [1 0 1 8 1 1]\n",
      "action : 1\n",
      "new state: [1 0 1 8 0 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "final state: [1 0 1 8 0 3]\n",
      "Episode : 16\n",
      "state : [1 0 1 8 0 3]\n",
      "action : 1\n",
      "new state: [1 0 1 8 1 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "final state: [1 0 1 8 1 3]\n",
      "Episode : 17\n",
      "state : [1 0 1 8 1 3]\n",
      "action : 0\n",
      "new state: [1 0 1 8 1 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [1 0 1 8 1 2]\n",
      "action : 3\n",
      "new state: [1 0 2 7 3 1]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [1 0 2 7 3 1]\n",
      "action : 1\n",
      "new state: [1 0 2 7 0 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "final state: [1 0 2 7 0 0]\n",
      "Episode : 18\n",
      "state : [1 0 2 7 0 0]\n",
      "action : 0\n",
      "new state: [1 0 2 7 3 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [1 0 2 7 3 1]\n",
      "action : 0\n",
      "new state: [0 1 2 7 2 0]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [0 1 2 7 2 0]\n",
      "action : 3\n",
      "new state: [0 1 2 7 1 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "state : [0 1 2 7 1 3]\n",
      "action : 1\n",
      "new state: [0 0 2 8 0 3]\n",
      "reward: -0.0\n",
      "epReward so far: -20001.0\n",
      "state : [0 0 2 8 0 3]\n",
      "action : 3\n",
      "new state: [0 0 2 8 0 1]\n",
      "reward: -1.0\n",
      "epReward so far: -20002.0\n",
      "final state: [0 0 2 8 0 1]\n",
      "Episode : 19\n",
      "state : [0 0 2 8 0 1]\n",
      "action : 1\n",
      "new state: [0 0 2 8 0 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "final state: [0 0 2 8 0 0]\n",
      "Episode : 20\n",
      "state : [0 0 2 8 0 0]\n",
      "action : 1\n",
      "new state: [0 0 2 8 2 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "final state: [0 0 2 8 2 3]\n",
      "Episode : 21\n",
      "state : [0 0 2 8 2 3]\n",
      "action : 0\n",
      "new state: [0 0 2 8 0 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "final state: [0 0 2 8 0 3]\n",
      "Episode : 22\n",
      "state : [0 0 2 8 0 3]\n",
      "action : 0\n",
      "new state: [0 0 2 8 3 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "final state: [0 0 2 8 3 3]\n",
      "Episode : 23\n",
      "state : [0 0 2 8 3 3]\n",
      "action : 3\n",
      "new state: [0 0 2 8 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 0 2 8 0 2]\n",
      "action : 1\n",
      "new state: [0 0 2 8 2 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "final state: [0 0 2 8 2 3]\n",
      "Episode : 24\n",
      "state : [0 0 2 8 2 3]\n",
      "action : 2\n",
      "new state: [0 0 2 8 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 0 2 8 0 2]\n",
      "action : 2\n",
      "new state: [0 0 2 8 0 1]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [0 0 2 8 0 1]\n",
      "action : 1\n",
      "new state: [0 0 2 8 2 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "final state: [0 0 2 8 2 3]\n",
      "Episode : 25\n",
      "state : [0 0 2 8 2 3]\n",
      "action : 3\n",
      "new state: [0 0 2 8 2 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 0 2 8 2 3]\n",
      "action : 3\n",
      "new state: [0 0 2 8 3 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [0 0 2 8 3 1]\n",
      "action : 3\n",
      "new state: [0 1 2 7 1 2]\n",
      "reward: -0.0\n",
      "epReward so far: -20000.0\n",
      "state : [0 1 2 7 1 2]\n",
      "action : 2\n",
      "new state: [0 1 2 7 0 2]\n",
      "reward: -1.0\n",
      "epReward so far: -20001.0\n",
      "state : [0 1 2 7 0 2]\n",
      "action : 1\n",
      "new state: [0 0 3 7 1 0]\n",
      "reward: -1.0\n",
      "epReward so far: -20002.0\n",
      "final state: [0 0 3 7 1 0]\n",
      "Episode : 26\n",
      "state : [0 0 3 7 1 0]\n",
      "action : 0\n",
      "new state: [0 0 3 7 3 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "final state: [0 0 3 7 3 0]\n",
      "Episode : 27\n",
      "state : [0 0 3 7 3 0]\n",
      "action : 0\n",
      "new state: [0 0 3 7 2 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "final state: [0 0 3 7 2 2]\n",
      "Episode : 28\n",
      "state : [0 0 3 7 2 2]\n",
      "action : 2\n",
      "new state: [0 0 3 7 2 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 0 3 7 2 2]\n",
      "action : 2\n",
      "new state: [0 0 3 7 1 0]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 0 3 7 1 0]\n",
      "action : 0\n",
      "new state: [0 0 3 7 2 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "final state: [0 0 3 7 2 0]\n",
      "Episode : 29\n",
      "state : [0 0 3 7 2 0]\n",
      "action : 0\n",
      "new state: [0 0 3 7 1 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "final state: [0 0 3 7 1 2]\n",
      "Episode : 30\n",
      "state : [0 0 3 7 1 2]\n",
      "action : 2\n",
      "new state: [0 0 3 7 1 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 0 3 7 1 1]\n",
      "action : 1\n",
      "new state: [0 0 3 7 2 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "final state: [0 0 3 7 2 2]\n",
      "Episode : 31\n",
      "state : [0 0 3 7 2 2]\n",
      "action : 0\n",
      "new state: [0 0 3 7 3 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "final state: [0 0 3 7 3 2]\n",
      "Episode : 32\n",
      "state : [0 0 3 7 3 2]\n",
      "action : 0\n",
      "new state: [0 0 3 7 3 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "final state: [0 0 3 7 3 0]\n",
      "Episode : 33\n",
      "state : [0 0 3 7 3 0]\n",
      "action : 1\n",
      "new state: [0 0 3 7 1 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "final state: [0 0 3 7 1 1]\n",
      "Episode : 34\n",
      "state : [0 0 3 7 1 1]\n",
      "action : 1\n",
      "new state: [0 0 3 7 3 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "final state: [0 0 3 7 3 2]\n",
      "Episode : 35\n",
      "state : [0 0 3 7 3 2]\n",
      "action : 3\n",
      "new state: [0 0 4 6 0 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 0 4 6 0 1]\n",
      "action : 3\n",
      "new state: [0 1 4 5 2 0]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [0 1 4 5 2 0]\n",
      "action : 3\n",
      "new state: [1 1 4 4 1 0]\n",
      "reward: -1.0\n",
      "epReward so far: -2.0\n",
      "state : [1 1 4 4 1 0]\n",
      "action : 2\n",
      "new state: [1 1 4 4 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10002.0\n",
      "state : [1 1 4 4 0 2]\n",
      "action : 1\n",
      "new state: [1 0 5 4 2 0]\n",
      "reward: -1.0\n",
      "epReward so far: -10003.0\n",
      "final state: [1 0 5 4 2 0]\n",
      "Episode : 36\n",
      "state : [1 0 5 4 2 0]\n",
      "action : 2\n",
      "new state: [1 0 5 4 3 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [1 0 5 4 3 1]\n",
      "action : 1\n",
      "new state: [1 0 5 4 3 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "final state: [1 0 5 4 3 2]\n",
      "Episode : 37\n",
      "state : [1 0 5 4 3 2]\n",
      "action : 0\n",
      "new state: [1 0 5 4 2 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [1 0 5 4 2 2]\n",
      "action : 0\n",
      "new state: [0 0 6 4 3 2]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [0 0 6 4 3 2]\n",
      "action : 2\n",
      "new state: [0 0 6 4 0 3]\n",
      "reward: -1.0\n",
      "epReward so far: -10002.0\n",
      "state : [0 0 6 4 0 3]\n",
      "action : 1\n",
      "new state: [0 0 6 4 1 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -20002.0\n",
      "final state: [0 0 6 4 1 0]\n",
      "Episode : 38\n",
      "state : [0 0 6 4 1 0]\n",
      "action : 0\n",
      "new state: [0 0 6 4 3 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "final state: [0 0 6 4 3 3]\n",
      "Episode : 39\n",
      "state : [0 0 6 4 3 3]\n",
      "action : 3\n",
      "new state: [0 0 6 4 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 0 6 4 2 2]\n",
      "action : 3\n",
      "new state: [0 0 7 3 2 0]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [0 0 7 3 2 0]\n",
      "action : 3\n",
      "new state: [0 0 7 3 1 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "state : [0 0 7 3 1 2]\n",
      "action : 0\n",
      "new state: [0 0 7 3 2 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "final state: [0 0 7 3 2 1]\n",
      "Episode : 40\n",
      "state : [0 0 7 3 2 1]\n",
      "action : 3\n",
      "new state: [0 0 7 3 1 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 0 7 3 1 2]\n",
      "action : 1\n",
      "new state: [0 0 7 3 3 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "final state: [0 0 7 3 3 0]\n",
      "Episode : 41\n",
      "state : [0 0 7 3 3 0]\n",
      "action : 1\n",
      "new state: [0 0 7 3 0 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "final state: [0 0 7 3 0 3]\n",
      "Episode : 42\n",
      "state : [0 0 7 3 0 3]\n",
      "action : 2\n",
      "new state: [0 0 7 3 3 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 0 7 3 3 2]\n",
      "action : 2\n",
      "new state: [0 0 7 3 2 2]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [0 0 7 3 2 2]\n",
      "action : 3\n",
      "new state: [0 0 7 3 0 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "state : [0 0 7 3 0 3]\n",
      "action : 3\n",
      "new state: [0 0 7 3 3 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -30001.0\n",
      "state : [0 0 7 3 3 1]\n",
      "action : 1\n",
      "new state: [0 0 7 3 3 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -40001.0\n",
      "final state: [0 0 7 3 3 1]\n",
      "Episode : 43\n",
      "state : [0 0 7 3 3 1]\n",
      "action : 0\n",
      "new state: [0 0 7 3 0 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "final state: [0 0 7 3 0 3]\n",
      "Episode : 44\n",
      "state : [0 0 7 3 0 3]\n",
      "action : 3\n",
      "new state: [0 0 7 3 3 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 0 7 3 3 2]\n",
      "action : 1\n",
      "new state: [0 0 7 3 3 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "final state: [0 0 7 3 3 1]\n",
      "Episode : 45\n",
      "state : [0 0 7 3 3 1]\n",
      "action : 2\n",
      "new state: [0 1 6 3 3 1]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [0 1 6 3 3 1]\n",
      "action : 2\n",
      "new state: [0 1 6 3 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "state : [0 1 6 3 0 2]\n",
      "action : 1\n",
      "new state: [0 0 7 3 1 2]\n",
      "reward: -1.0\n",
      "epReward so far: -10002.0\n",
      "state : [0 0 7 3 1 2]\n",
      "action : 0\n",
      "new state: [0 0 7 3 1 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20002.0\n",
      "final state: [0 0 7 3 1 3]\n",
      "Episode : 46\n",
      "state : [0 0 7 3 1 3]\n",
      "action : 0\n",
      "new state: [0 0 7 3 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "final state: [0 0 7 3 0 2]\n",
      "Episode : 47\n",
      "state : [0 0 7 3 0 2]\n",
      "action : 1\n",
      "new state: [0 0 7 3 1 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "final state: [0 0 7 3 1 2]\n",
      "Episode : 48\n",
      "state : [0 0 7 3 1 2]\n",
      "action : 3\n",
      "new state: [0 0 7 3 0 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 0 7 3 0 3]\n",
      "action : 3\n",
      "new state: [0 0 7 3 3 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [0 0 7 3 3 1]\n",
      "action : 3\n",
      "new state: [0 0 7 3 2 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -30000.0\n",
      "state : [0 0 7 3 2 1]\n",
      "action : 1\n",
      "new state: [0 0 7 3 2 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -40000.0\n",
      "final state: [0 0 7 3 2 3]\n",
      "Episode : 49\n",
      "state : [0 0 7 3 2 3]\n",
      "action : 0\n",
      "new state: [0 0 7 3 2 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "final state: [0 0 7 3 2 3]\n",
      "Episode : 50\n",
      "state : [0 0 7 3 2 3]\n",
      "action : 2\n",
      "new state: [0 0 6 4 3 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 0 6 4 3 1]\n",
      "action : 3\n",
      "new state: [0 1 6 3 0 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 1 6 3 0 3]\n",
      "action : 2\n",
      "new state: [0 1 5 4 0 3]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [0 1 5 4 0 3]\n",
      "action : 1\n",
      "new state: [0 1 5 4 3 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "state : [0 1 5 4 3 3]\n",
      "action : 1\n",
      "new state: [0 0 5 5 3 2]\n",
      "reward: -1.0\n",
      "epReward so far: -10002.0\n",
      "final state: [0 0 5 5 3 2]\n",
      "Episode : 51\n",
      "state : [0 0 5 5 3 2]\n",
      "action : 2\n",
      "new state: [0 0 5 5 3 3]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [0 0 5 5 3 3]\n",
      "action : 0\n",
      "new state: [0 0 5 5 3 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "final state: [0 0 5 5 3 0]\n",
      "Episode : 52\n",
      "state : [0 0 5 5 3 0]\n",
      "action : 3\n",
      "new state: [1 0 5 4 0 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 0 5 4 0 3]\n",
      "action : 1\n",
      "new state: [1 0 5 4 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "final state: [1 0 5 4 0 2]\n",
      "Episode : 53\n",
      "state : [1 0 5 4 0 2]\n",
      "action : 2\n",
      "new state: [1 0 5 4 1 2]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [1 0 5 4 1 2]\n",
      "action : 0\n",
      "new state: [0 0 6 4 1 3]\n",
      "reward: -1.0\n",
      "epReward so far: -2.0\n",
      "state : [0 0 6 4 1 3]\n",
      "action : 0\n",
      "new state: [0 0 6 4 3 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10002.0\n",
      "final state: [0 0 6 4 3 0]\n",
      "Episode : 54\n",
      "state : [0 0 6 4 3 0]\n",
      "action : 3\n",
      "new state: [1 0 6 3 0 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 0 6 3 0 3]\n",
      "action : 0\n",
      "new state: [0 0 6 4 2 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 0 6 4 2 3]\n",
      "action : 3\n",
      "new state: [0 0 6 4 3 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 0 6 4 3 1]\n",
      "action : 0\n",
      "new state: [0 0 6 4 0 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "final state: [0 0 6 4 0 0]\n",
      "Episode : 55\n",
      "state : [0 0 6 4 0 0]\n",
      "action : 3\n",
      "new state: [0 0 6 4 1 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 0 6 4 1 2]\n",
      "action : 3\n",
      "new state: [0 0 6 4 2 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [0 0 6 4 2 0]\n",
      "action : 0\n",
      "new state: [0 0 6 4 1 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -30000.0\n",
      "final state: [0 0 6 4 1 0]\n",
      "Episode : 56\n",
      "state : [0 0 6 4 1 0]\n",
      "action : 2\n",
      "new state: [0 0 6 4 1 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 0 6 4 1 3]\n",
      "action : 2\n",
      "new state: [0 0 5 5 3 2]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [0 0 5 5 3 2]\n",
      "action : 0\n",
      "new state: [0 0 5 5 3 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "final state: [0 0 5 5 3 0]\n",
      "Episode : 57\n",
      "state : [0 0 5 5 3 0]\n",
      "action : 2\n",
      "new state: [0 0 5 5 1 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 0 5 5 1 3]\n",
      "action : 3\n",
      "new state: [0 0 5 5 3 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [0 0 5 5 3 2]\n",
      "action : 1\n",
      "new state: [0 0 5 5 1 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -30000.0\n",
      "final state: [0 0 5 5 1 3]\n",
      "Episode : 58\n",
      "state : [0 0 5 5 1 3]\n",
      "action : 0\n",
      "new state: [0 0 5 5 1 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "final state: [0 0 5 5 1 1]\n",
      "Episode : 59\n",
      "state : [0 0 5 5 1 1]\n",
      "action : 2\n",
      "new state: [0 0 5 5 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 0 5 5 0 2]\n",
      "action : 3\n",
      "new state: [0 0 5 5 0 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [0 0 5 5 0 3]\n",
      "action : 3\n",
      "new state: [0 0 5 5 1 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -30000.0\n",
      "state : [0 0 5 5 1 2]\n",
      "action : 1\n",
      "new state: [0 0 5 5 3 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -40000.0\n",
      "final state: [0 0 5 5 3 1]\n",
      "Episode : 60\n",
      "state : [0 0 5 5 3 1]\n",
      "action : 2\n",
      "new state: [0 1 4 5 3 2]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [0 1 4 5 3 2]\n",
      "action : 2\n",
      "new state: [0 1 4 5 1 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "state : [0 1 4 5 1 0]\n",
      "action : 0\n",
      "new state: [0 1 4 5 1 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "final state: [0 1 4 5 1 3]\n",
      "Episode : 61\n",
      "state : [0 1 4 5 1 3]\n",
      "action : 3\n",
      "new state: [0 1 4 5 0 2]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [0 1 4 5 0 2]\n",
      "action : 0\n",
      "new state: [0 1 4 5 2 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "final state: [0 1 4 5 2 3]\n",
      "Episode : 62\n",
      "state : [0 1 4 5 2 3]\n",
      "action : 3\n",
      "new state: [0 1 4 5 2 3]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [0 1 4 5 2 3]\n",
      "action : 3\n",
      "new state: [0 1 4 5 3 0]\n",
      "reward: -1.0\n",
      "epReward so far: -2.0\n",
      "state : [0 1 4 5 3 0]\n",
      "action : 3\n",
      "new state: [1 1 4 4 3 0]\n",
      "reward: -0.0\n",
      "epReward so far: -2.0\n",
      "state : [1 1 4 4 3 0]\n",
      "action : 3\n",
      "new state: [2 1 4 3 0 3]\n",
      "reward: -0.0\n",
      "epReward so far: -2.0\n",
      "state : [2 1 4 3 0 3]\n",
      "action : 2\n",
      "new state: [2 1 3 4 1 0]\n",
      "reward: -1.0\n",
      "epReward so far: -3.0\n",
      "final state: [2 1 3 4 1 0]\n",
      "Episode : 63\n",
      "state : [2 1 3 4 1 0]\n",
      "action : 0\n",
      "new state: [2 1 3 4 3 2]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [2 1 3 4 3 2]\n",
      "action : 3\n",
      "new state: [2 1 4 3 2 0]\n",
      "reward: -0.0\n",
      "epReward so far: -1.0\n",
      "state : [2 1 4 3 2 0]\n",
      "action : 2\n",
      "new state: [3 1 3 3 3 1]\n",
      "reward: -0.0\n",
      "epReward so far: -1.0\n",
      "state : [3 1 3 3 3 1]\n",
      "action : 3\n",
      "new state: [3 2 3 2 1 3]\n",
      "reward: -0.0\n",
      "epReward so far: -1.0\n",
      "state : [3 2 3 2 1 3]\n",
      "action : 2\n",
      "new state: [3 2 3 2 1 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "final state: [3 2 3 2 1 3]\n",
      "Episode : 64\n",
      "state : [3 2 3 2 1 3]\n",
      "action : 1\n",
      "new state: [3 2 3 2 2 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [3 2 3 2 2 0]\n",
      "action : 0\n",
      "new state: [3 2 3 2 2 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [3 2 3 2 2 2]\n",
      "action : 3\n",
      "new state: [3 2 3 2 3 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -30000.0\n",
      "state : [3 2 3 2 3 3]\n",
      "action : 0\n",
      "new state: [3 2 3 2 3 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -40000.0\n",
      "state : [3 2 3 2 3 3]\n",
      "action : 2\n",
      "new state: [3 2 2 3 2 3]\n",
      "reward: -1.0\n",
      "epReward so far: -40001.0\n",
      "final state: [3 2 2 3 2 3]\n",
      "Episode : 65\n",
      "state : [3 2 2 3 2 3]\n",
      "action : 1\n",
      "new state: [3 2 2 3 3 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [3 2 2 3 3 0]\n",
      "action : 0\n",
      "new state: [3 2 2 3 1 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [3 2 2 3 1 2]\n",
      "action : 1\n",
      "new state: [3 1 3 3 2 0]\n",
      "reward: -0.0\n",
      "epReward so far: -20000.0\n",
      "state : [3 1 3 3 2 0]\n",
      "action : 2\n",
      "new state: [4 1 2 3 0 3]\n",
      "reward: -0.0\n",
      "epReward so far: -20000.0\n",
      "state : [4 1 2 3 0 3]\n",
      "action : 0\n",
      "new state: [3 1 2 4 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: -20000.0\n",
      "final state: [3 1 2 4 2 2]\n",
      "Episode : 66\n",
      "state : [3 1 2 4 2 2]\n",
      "action : 2\n",
      "new state: [3 1 2 4 1 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 1 2 4 1 0]\n",
      "action : 2\n",
      "new state: [3 1 2 4 2 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [3 1 2 4 2 0]\n",
      "action : 2\n",
      "new state: [4 1 1 4 2 3]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [4 1 1 4 2 3]\n",
      "action : 1\n",
      "new state: [4 1 1 4 2 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [4 1 1 4 2 1]\n",
      "action : 0\n",
      "new state: [4 1 1 4 0 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -30000.0\n",
      "final state: [4 1 1 4 0 0]\n",
      "Episode : 67\n",
      "state : [4 1 1 4 0 0]\n",
      "action : 0\n",
      "new state: [4 1 1 4 3 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [4 1 1 4 3 2]\n",
      "action : 1\n",
      "new state: [4 1 1 4 2 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [4 1 1 4 2 1]\n",
      "action : 0\n",
      "new state: [3 2 1 4 3 3]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [3 2 1 4 3 3]\n",
      "action : 1\n",
      "new state: [3 1 1 5 2 0]\n",
      "reward: -1.0\n",
      "epReward so far: -10002.0\n",
      "state : [3 1 1 5 2 0]\n",
      "action : 2\n",
      "new state: [4 1 0 5 0 3]\n",
      "reward: -0.0\n",
      "epReward so far: -10002.0\n",
      "final state: [4 1 0 5 0 3]\n",
      "Episode : 68\n",
      "state : [4 1 0 5 0 3]\n",
      "action : 1\n",
      "new state: [4 1 0 5 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [4 1 0 5 0 2]\n",
      "action : 3\n",
      "new state: [4 1 1 4 2 0]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [4 1 1 4 2 0]\n",
      "action : 1\n",
      "new state: [5 0 1 4 3 1]\n",
      "reward: -1.0\n",
      "epReward so far: -10002.0\n",
      "state : [5 0 1 4 3 1]\n",
      "action : 1\n",
      "new state: [5 0 1 4 1 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20002.0\n",
      "final state: [5 0 1 4 1 3]\n",
      "Episode : 69\n",
      "state : [5 0 1 4 1 3]\n",
      "action : 3\n",
      "new state: [5 0 1 4 0 3]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [5 0 1 4 0 3]\n",
      "action : 0\n",
      "new state: [4 0 1 5 3 0]\n",
      "reward: -0.0\n",
      "epReward so far: -1.0\n",
      "state : [4 0 1 5 3 0]\n",
      "action : 1\n",
      "new state: [4 0 1 5 0 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "final state: [4 0 1 5 0 1]\n",
      "Episode : 70\n",
      "state : [4 0 1 5 0 1]\n",
      "action : 3\n",
      "new state: [4 0 1 5 0 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [4 0 1 5 0 0]\n",
      "action : 0\n",
      "new state: [4 0 1 5 3 2]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [4 0 1 5 3 2]\n",
      "action : 3\n",
      "new state: [4 0 1 5 0 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [4 0 1 5 0 3]\n",
      "action : 2\n",
      "new state: [4 0 1 5 3 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -30000.0\n",
      "state : [4 0 1 5 3 1]\n",
      "action : 3\n",
      "new state: [4 1 1 4 1 3]\n",
      "reward: -0.0\n",
      "epReward so far: -30000.0\n",
      "final state: [4 1 1 4 1 3]\n",
      "Episode : 71\n",
      "state : [4 1 1 4 1 3]\n",
      "action : 0\n",
      "new state: [3 1 1 5 0 2]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [3 1 1 5 0 2]\n",
      "action : 3\n",
      "new state: [3 1 2 4 3 3]\n",
      "reward: -1.0\n",
      "epReward so far: -2.0\n",
      "state : [3 1 2 4 3 3]\n",
      "action : 2\n",
      "new state: [3 1 1 5 0 0]\n",
      "reward: -1.0\n",
      "epReward so far: -3.0\n",
      "state : [3 1 1 5 0 0]\n",
      "action : 2\n",
      "new state: [4 1 0 5 0 0]\n",
      "reward: -1.0\n",
      "epReward so far: -4.0\n",
      "state : [4 1 0 5 0 0]\n",
      "action : 2\n",
      "new state: [4 1 0 5 3 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10004.0\n",
      "final state: [4 1 0 5 3 3]\n",
      "Episode : 72\n",
      "state : [4 1 0 5 3 3]\n",
      "action : 3\n",
      "new state: [4 1 0 5 1 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [4 1 0 5 1 2]\n",
      "action : 0\n",
      "new state: [3 1 1 5 1 3]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [3 1 1 5 1 3]\n",
      "action : 0\n",
      "new state: [2 1 1 6 3 0]\n",
      "reward: -1.0\n",
      "epReward so far: -2.0\n",
      "state : [2 1 1 6 3 0]\n",
      "action : 0\n",
      "new state: [2 1 1 6 3 3]\n",
      "reward: -1.0\n",
      "epReward so far: -3.0\n",
      "state : [2 1 1 6 3 3]\n",
      "action : 2\n",
      "new state: [2 1 1 6 1 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10003.0\n",
      "final state: [2 1 1 6 1 3]\n",
      "Episode : 73\n",
      "state : [2 1 1 6 1 3]\n",
      "action : 2\n",
      "new state: [2 1 1 6 1 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 1 1 6 1 0]\n",
      "action : 2\n",
      "new state: [3 1 0 6 3 1]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [3 1 0 6 3 1]\n",
      "action : 2\n",
      "new state: [3 1 0 6 1 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "final state: [3 1 0 6 1 0]\n",
      "Episode : 74\n",
      "state : [3 1 0 6 1 0]\n",
      "action : 3\n",
      "new state: [4 1 0 5 2 0]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [4 1 0 5 2 0]\n",
      "action : 2\n",
      "new state: [4 1 0 5 1 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "final state: [4 1 0 5 1 1]\n",
      "Episode : 75\n",
      "state : [4 1 0 5 1 1]\n",
      "action : 3\n",
      "new state: [4 1 0 5 0 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [4 1 0 5 0 0]\n",
      "action : 1\n",
      "new state: [4 1 0 5 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [4 1 0 5 0 2]\n",
      "action : 2\n",
      "new state: [4 1 0 5 0 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -30000.0\n",
      "final state: [4 1 0 5 0 3]\n",
      "Episode : 76\n",
      "state : [4 1 0 5 0 3]\n",
      "action : 3\n",
      "new state: [4 1 0 5 0 2]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [4 1 0 5 0 2]\n",
      "action : 3\n",
      "new state: [4 1 0 5 1 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "state : [4 1 0 5 1 3]\n",
      "action : 3\n",
      "new state: [4 1 0 5 1 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "state : [4 1 0 5 1 3]\n",
      "action : 2\n",
      "new state: [4 1 0 5 2 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -30001.0\n",
      "final state: [4 1 0 5 2 3]\n",
      "Episode : 77\n",
      "state : [4 1 0 5 2 3]\n",
      "action : 1\n",
      "new state: [4 0 0 6 1 0]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [4 0 0 6 1 0]\n",
      "action : 0\n",
      "new state: [4 0 0 6 3 3]\n",
      "reward: -1.0\n",
      "epReward so far: -2.0\n",
      "state : [4 0 0 6 3 3]\n",
      "action : 0\n",
      "new state: [4 0 0 6 3 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10002.0\n",
      "state : [4 0 0 6 3 1]\n",
      "action : 1\n",
      "new state: [4 0 0 6 1 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20002.0\n",
      "final state: [4 0 0 6 1 3]\n",
      "Episode : 78\n",
      "state : [4 0 0 6 1 3]\n",
      "action : 1\n",
      "new state: [4 0 0 6 2 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "final state: [4 0 0 6 2 2]\n",
      "Episode : 79\n",
      "state : [4 0 0 6 2 2]\n",
      "action : 1\n",
      "new state: [4 0 0 6 1 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "final state: [4 0 0 6 1 3]\n",
      "Episode : 80\n",
      "state : [4 0 0 6 1 3]\n",
      "action : 3\n",
      "new state: [4 0 0 6 2 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [4 0 0 6 2 3]\n",
      "action : 1\n",
      "new state: [4 0 0 6 0 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "final state: [4 0 0 6 0 3]\n",
      "Episode : 81\n",
      "state : [4 0 0 6 0 3]\n",
      "action : 0\n",
      "new state: [4 0 0 6 0 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [4 0 0 6 0 1]\n",
      "action : 3\n",
      "new state: [4 0 0 6 1 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [4 0 0 6 1 1]\n",
      "action : 3\n",
      "new state: [4 1 0 5 3 0]\n",
      "reward: -1.0\n",
      "epReward so far: -20001.0\n",
      "state : [4 1 0 5 3 0]\n",
      "action : 2\n",
      "new state: [4 1 0 5 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -30001.0\n",
      "final state: [4 1 0 5 0 2]\n",
      "Episode : 82\n",
      "state : [4 1 0 5 0 2]\n",
      "action : 2\n",
      "new state: [4 1 0 5 0 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "final state: [4 1 0 5 0 0]\n",
      "Episode : 83\n",
      "state : [4 1 0 5 0 0]\n",
      "action : 1\n",
      "new state: [4 1 0 5 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [4 1 0 5 0 2]\n",
      "action : 1\n",
      "new state: [4 1 0 5 3 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [4 1 0 5 3 3]\n",
      "action : 2\n",
      "new state: [4 1 0 5 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -30000.0\n",
      "final state: [4 1 0 5 0 2]\n",
      "Episode : 84\n",
      "state : [4 1 0 5 0 2]\n",
      "action : 0\n",
      "new state: [3 1 1 5 3 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 1 1 5 3 2]\n",
      "action : 2\n",
      "new state: [3 1 1 5 3 1]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [3 1 1 5 3 1]\n",
      "action : 3\n",
      "new state: [3 2 1 4 1 1]\n",
      "reward: -0.0\n",
      "epReward so far: -1.0\n",
      "state : [3 2 1 4 1 1]\n",
      "action : 3\n",
      "new state: [3 2 1 4 1 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "state : [3 2 1 4 1 0]\n",
      "action : 3\n",
      "new state: [4 2 1 3 1 3]\n",
      "reward: -1.0\n",
      "epReward so far: -10002.0\n",
      "final state: [4 2 1 3 1 3]\n",
      "Episode : 85\n",
      "state : [4 2 1 3 1 3]\n",
      "action : 2\n",
      "new state: [4 2 1 3 0 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [4 2 1 3 0 1]\n",
      "action : 2\n",
      "new state: [4 3 0 3 2 0]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [4 3 0 3 2 0]\n",
      "action : 2\n",
      "new state: [4 3 0 3 2 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "final state: [4 3 0 3 2 2]\n",
      "Episode : 86\n",
      "state : [4 3 0 3 2 2]\n",
      "action : 2\n",
      "new state: [4 3 0 3 0 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "final state: [4 3 0 3 0 0]\n",
      "Episode : 87\n",
      "state : [4 3 0 3 0 0]\n",
      "action : 1\n",
      "new state: [5 2 0 3 1 0]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [5 2 0 3 1 0]\n",
      "action : 0\n",
      "new state: [5 2 0 3 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "state : [5 2 0 3 0 2]\n",
      "action : 1\n",
      "new state: [5 2 0 3 0 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "state : [5 2 0 3 0 3]\n",
      "action : 0\n",
      "new state: [5 2 0 3 1 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -30001.0\n",
      "state : [5 2 0 3 1 3]\n",
      "action : 3\n",
      "new state: [5 2 0 3 3 3]\n",
      "reward: -1.0\n",
      "epReward so far: -30002.0\n",
      "final state: [5 2 0 3 3 3]\n",
      "Episode : 88\n",
      "state : [5 2 0 3 3 3]\n",
      "action : 0\n",
      "new state: [4 2 0 4 2 3]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [4 2 0 4 2 3]\n",
      "action : 0\n",
      "new state: [4 2 0 4 2 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "state : [4 2 0 4 2 2]\n",
      "action : 3\n",
      "new state: [4 2 1 3 2 1]\n",
      "reward: -1.0\n",
      "epReward so far: -10002.0\n",
      "state : [4 2 1 3 2 1]\n",
      "action : 2\n",
      "new state: [4 3 0 3 1 0]\n",
      "reward: -0.0\n",
      "epReward so far: -10002.0\n",
      "state : [4 3 0 3 1 0]\n",
      "action : 1\n",
      "new state: [5 2 0 3 2 3]\n",
      "reward: -0.0\n",
      "epReward so far: -10002.0\n",
      "final state: [5 2 0 3 2 3]\n",
      "Episode : 89\n",
      "state : [5 2 0 3 2 3]\n",
      "action : 3\n",
      "new state: [5 2 0 3 3 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [5 2 0 3 3 0]\n",
      "action : 1\n",
      "new state: [6 1 0 3 0 2]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [6 1 0 3 0 2]\n",
      "action : 2\n",
      "new state: [6 1 0 3 1 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "final state: [6 1 0 3 1 0]\n",
      "Episode : 90\n",
      "state : [6 1 0 3 1 0]\n",
      "action : 0\n",
      "new state: [6 1 0 3 3 3]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [6 1 0 3 3 3]\n",
      "action : 2\n",
      "new state: [6 1 0 3 0 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "final state: [6 1 0 3 0 3]\n",
      "Episode : 91\n",
      "state : [6 1 0 3 0 3]\n",
      "action : 2\n",
      "new state: [6 1 0 3 3 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "final state: [6 1 0 3 3 0]\n",
      "Episode : 92\n",
      "state : [6 1 0 3 3 0]\n",
      "action : 1\n",
      "new state: [7 0 0 3 3 2]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [7 0 0 3 3 2]\n",
      "action : 3\n",
      "new state: [7 0 1 2 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: -1.0\n",
      "state : [7 0 1 2 0 2]\n",
      "action : 2\n",
      "new state: [7 0 1 2 2 0]\n",
      "reward: -1.0\n",
      "epReward so far: -2.0\n",
      "state : [7 0 1 2 2 0]\n",
      "action : 0\n",
      "new state: [7 0 1 2 2 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10002.0\n",
      "state : [7 0 1 2 2 1]\n",
      "action : 0\n",
      "new state: [6 1 1 2 0 0]\n",
      "reward: -1.0\n",
      "epReward so far: -10003.0\n",
      "final state: [6 1 1 2 0 0]\n",
      "Episode : 93\n",
      "state : [6 1 1 2 0 0]\n",
      "action : 3\n",
      "new state: [7 1 1 1 1 3]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [7 1 1 1 1 3]\n",
      "action : 3\n",
      "new state: [7 1 1 1 1 1]\n",
      "reward: -1.0\n",
      "epReward so far: -2.0\n",
      "state : [7 1 1 1 1 1]\n",
      "action : 2\n",
      "new state: [7 1 1 1 2 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10002.0\n",
      "state : [7 1 1 1 2 3]\n",
      "action : 2\n",
      "new state: [7 1 0 2 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: -10002.0\n",
      "state : [7 1 0 2 0 2]\n",
      "action : 1\n",
      "new state: [7 0 1 2 1 1]\n",
      "reward: -1.0\n",
      "epReward so far: -10003.0\n",
      "final state: [7 0 1 2 1 1]\n",
      "Episode : 94\n",
      "state : [7 0 1 2 1 1]\n",
      "action : 2\n",
      "new state: [7 0 1 2 3 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [7 0 1 2 3 1]\n",
      "action : 2\n",
      "new state: [7 0 1 2 3 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [7 0 1 2 3 0]\n",
      "action : 2\n",
      "new state: [7 0 1 2 0 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -30000.0\n",
      "state : [7 0 1 2 0 1]\n",
      "action : 2\n",
      "new state: [7 1 0 2 2 3]\n",
      "reward: -1.0\n",
      "epReward so far: -30001.0\n",
      "state : [7 1 0 2 2 3]\n",
      "action : 3\n",
      "new state: [7 1 0 2 0 0]\n",
      "reward: -1.0\n",
      "epReward so far: -30002.0\n",
      "final state: [7 1 0 2 0 0]\n",
      "Episode : 95\n",
      "state : [7 1 0 2 0 0]\n",
      "action : 0\n",
      "new state: [7 1 0 2 3 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [7 1 0 2 3 2]\n",
      "action : 1\n",
      "new state: [7 1 0 2 0 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [7 1 0 2 0 3]\n",
      "action : 1\n",
      "new state: [7 1 0 2 1 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [7 1 0 2 1 3]\n",
      "action : 2\n",
      "new state: [7 1 0 2 0 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -30000.0\n",
      "final state: [7 1 0 2 0 3]\n",
      "Episode : 96\n",
      "state : [7 1 0 2 0 3]\n",
      "action : 3\n",
      "new state: [7 1 0 2 1 1]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [7 1 0 2 1 1]\n",
      "action : 1\n",
      "new state: [7 1 0 2 1 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "state : [7 1 0 2 1 2]\n",
      "action : 0\n",
      "new state: [6 1 1 2 0 1]\n",
      "reward: -1.0\n",
      "epReward so far: -10002.0\n",
      "state : [6 1 1 2 0 1]\n",
      "action : 1\n",
      "new state: [6 1 1 2 3 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -20002.0\n",
      "state : [6 1 1 2 3 0]\n",
      "action : 3\n",
      "new state: [7 1 1 1 2 3]\n",
      "reward: -0.0\n",
      "epReward so far: -20002.0\n",
      "final state: [7 1 1 1 2 3]\n",
      "Episode : 97\n",
      "state : [7 1 1 1 2 3]\n",
      "action : 3\n",
      "new state: [7 1 1 1 2 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [7 1 1 1 2 2]\n",
      "action : 1\n",
      "new state: [7 0 2 1 0 0]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [7 0 2 1 0 0]\n",
      "action : 3\n",
      "new state: [8 0 2 0 1 3]\n",
      "reward: -1.0\n",
      "epReward so far: -10002.0\n",
      "state : [8 0 2 0 1 3]\n",
      "action : 0\n",
      "new state: [7 0 2 1 2 1]\n",
      "reward: -1.0\n",
      "epReward so far: -10003.0\n",
      "state : [7 0 2 1 2 1]\n",
      "action : 2\n",
      "new state: [7 0 2 1 3 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -20003.0\n",
      "final state: [7 0 2 1 3 0]\n",
      "Episode : 98\n",
      "state : [7 0 2 1 3 0]\n",
      "action : 0\n",
      "new state: [7 0 2 1 3 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [7 0 2 1 3 3]\n",
      "action : 3\n",
      "new state: [7 0 2 1 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [7 0 2 1 2 2]\n",
      "action : 3\n",
      "new state: [7 0 3 0 2 1]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [7 0 3 0 2 1]\n",
      "action : 3\n",
      "new state: [7 0 3 0 1 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "final state: [7 0 3 0 1 3]\n",
      "Episode : 99\n",
      "state : [7 0 3 0 1 3]\n",
      "action : 3\n",
      "new state: [7 0 3 0 1 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "final state: [7 0 3 0 1 1]\n",
      "**************************************************\n",
      "Experiment complete\n",
      "**************************************************\n",
      "**************************************************\n",
      "Saving data\n",
      "**************************************************\n",
      "[[ 0.00000000e+00  0.00000000e+00 -3.00000000e+04  2.23130000e+04\n",
      "  -4.26024026e+00]\n",
      " [ 1.00000000e+00  0.00000000e+00 -1.00010000e+04  6.97500000e+03\n",
      "  -6.37605007e+00]\n",
      " [ 2.00000000e+00  0.00000000e+00 -4.00010000e+04  1.71700000e+04\n",
      "  -4.87278244e+00]\n",
      " [ 3.00000000e+00  0.00000000e+00 -4.00000000e+04  1.71700000e+04\n",
      "  -4.86839885e+00]\n",
      " [ 4.00000000e+00  0.00000000e+00 -1.00000000e+04  3.83400000e+03\n",
      "  -6.00478607e+00]\n",
      " [ 5.00000000e+00  0.00000000e+00 -1.00030000e+04  1.71530000e+04\n",
      "  -4.72361601e+00]\n",
      " [ 6.00000000e+00  0.00000000e+00 -3.00010000e+04  1.37690000e+04\n",
      "  -4.94861995e+00]\n",
      " [ 7.00000000e+00  0.00000000e+00 -2.00000000e+04  1.03760000e+04\n",
      "  -5.38080781e+00]\n",
      " [ 8.00000000e+00  0.00000000e+00 -2.00010000e+04  1.37640000e+04\n",
      "  -4.61936191e+00]\n",
      " [ 9.00000000e+00  0.00000000e+00 -1.00000000e+04  6.29700000e+03\n",
      "  -5.69665632e+00]\n",
      " [ 1.00000000e+01  0.00000000e+00 -1.00020000e+04  1.26780000e+04\n",
      "  -4.94165291e+00]\n",
      " [ 1.10000000e+01  0.00000000e+00 -2.00030000e+04  1.26780000e+04\n",
      "  -4.72104269e+00]\n",
      " [ 1.20000000e+01  0.00000000e+00 -1.00040000e+04  1.26780000e+04\n",
      "  -4.93489984e+00]\n",
      " [ 1.30000000e+01  0.00000000e+00 -1.00000000e+04  1.48480000e+04\n",
      "  -4.46498324e+00]\n",
      " [ 1.40000000e+01  0.00000000e+00 -1.00030000e+04  1.26780000e+04\n",
      "  -4.74548637e+00]\n",
      " [ 1.50000000e+01  0.00000000e+00 -2.00030000e+04  1.26700000e+04\n",
      "  -4.75134681e+00]\n",
      " [ 1.60000000e+01  0.00000000e+00 -1.00000000e+04  5.18200000e+03\n",
      "  -5.91641452e+00]\n",
      " [ 1.70000000e+01  0.00000000e+00 -1.00040000e+04  1.26740000e+04\n",
      "  -4.87368641e+00]\n",
      " [ 1.80000000e+01  0.00000000e+00 -3.00020000e+04  1.26820000e+04\n",
      "  -4.52431997e+00]\n",
      " [ 1.90000000e+01  0.00000000e+00 -2.00000000e+04  1.26820000e+04\n",
      "  -4.65993001e+00]\n",
      " [ 2.00000000e+01  0.00000000e+00 -3.00020000e+04  1.83010000e+04\n",
      "  -4.32414517e+00]\n",
      " [ 2.10000000e+01  0.00000000e+00 -1.00020000e+04  7.77100000e+03\n",
      "  -4.47044836e+00]\n",
      " [ 2.20000000e+01  0.00000000e+00 -1.00000000e+04  1.19370000e+04\n",
      "  -6.22306085e+00]\n",
      " [ 2.30000000e+01  0.00000000e+00 -1.00000000e+04  2.11300000e+03\n",
      "  -5.87703389e+00]\n",
      " [ 2.40000000e+01  0.00000000e+00 -2.00000000e+04  1.59000000e+03\n",
      "  -4.85993496e+00]\n",
      " [ 2.50000000e+01  0.00000000e+00 -1.00000000e+04  2.11300000e+03\n",
      "  -5.95529202e+00]\n",
      " [ 2.60000000e+01  0.00000000e+00 -1.00010000e+04  2.66200000e+03\n",
      "  -4.39256861e+00]\n",
      " [ 2.70000000e+01  0.00000000e+00 -4.00000000e+04  1.36090000e+04\n",
      "  -4.25685182e+00]\n",
      " [ 2.80000000e+01  0.00000000e+00 -2.00030000e+04  7.78500000e+03\n",
      "  -4.52930212e+00]\n",
      " [ 2.90000000e+01  0.00000000e+00 -2.00020000e+04  5.84100000e+03\n",
      "  -4.84965272e+00]\n",
      " [ 3.00000000e+01  0.00000000e+00 -2.00010000e+04  7.68400000e+03\n",
      "  -5.55919120e+00]\n",
      " [ 3.10000000e+01  0.00000000e+00 -3.00000000e+04  7.69200000e+03\n",
      "  -5.39200380e+00]\n",
      " [ 3.20000000e+01  0.00000000e+00 -2.00020000e+04  1.76040000e+04\n",
      "  -4.95259409e+00]\n",
      " [ 3.30000000e+01  0.00000000e+00 -1.00010000e+04  3.25130000e+04\n",
      "  -4.74318462e+00]\n",
      " [ 3.40000000e+01  0.00000000e+00 -1.00010000e+04  6.93400000e+03\n",
      "  -5.47029375e+00]\n",
      " [ 3.50000000e+01  0.00000000e+00 -2.00020000e+04  1.01130000e+04\n",
      "  -5.03349769e+00]\n",
      " [ 3.60000000e+01  0.00000000e+00 -2.00020000e+04  1.26780000e+04\n",
      "  -4.92277289e+00]\n",
      " [ 3.70000000e+01  0.00000000e+00 -2.00030000e+04  5.48900000e+03\n",
      "  -3.99274142e+00]\n",
      " [ 3.80000000e+01  0.00000000e+00 -1.00000000e+04  3.32900000e+03\n",
      "  -6.06698848e+00]\n",
      " [ 3.90000000e+01  0.00000000e+00 -3.00010000e+04  5.92500000e+03\n",
      "  -4.36339109e+00]\n",
      " [ 4.00000000e+01  0.00000000e+00 -4.00010000e+04  7.16100000e+03\n",
      "  -4.03963628e+00]\n",
      " [ 4.10000000e+01  0.00000000e+00 -1.00010000e+04  5.18300000e+03\n",
      "  -5.82322186e+00]\n",
      " [ 4.20000000e+01  0.00000000e+00 -1.00000000e+04  5.29700000e+03\n",
      "  -5.59382724e+00]\n",
      " [ 4.30000000e+01  0.00000000e+00 -3.00000000e+04  9.22100000e+03\n",
      "  -4.57319150e+00]\n",
      " [ 4.40000000e+01  0.00000000e+00 -2.00020000e+04  9.54500000e+03\n",
      "  -4.98849564e+00]\n",
      " [ 4.50000000e+01  0.00000000e+00 -2.00010000e+04  5.01200000e+03\n",
      "  -5.31852443e+00]\n",
      " [ 4.60000000e+01  0.00000000e+00 -3.00000000e+00  6.38700000e+03\n",
      "  -5.35046150e+00]\n",
      " [ 4.70000000e+01  0.00000000e+00 -2.00030000e+04  6.89300000e+03\n",
      "  -4.89368803e+00]\n",
      " [ 4.80000000e+01  0.00000000e+00 -1.00010000e+04  6.49900000e+03\n",
      "  -5.72233657e+00]\n",
      " [ 4.90000000e+01  0.00000000e+00 -1.00010000e+04  9.64700000e+03\n",
      "  -5.42479363e+00]\n",
      " [ 5.00000000e+01  0.00000000e+00 -2.00000000e+04  6.18200000e+03\n",
      "  -6.27272300e+00]\n",
      " [ 5.10000000e+01  0.00000000e+00 -2.00030000e+04  1.04330000e+04\n",
      "  -5.14407568e+00]\n",
      " [ 5.20000000e+01  0.00000000e+00 -2.00020000e+04  6.78500000e+03\n",
      "  -5.42636412e+00]\n",
      " [ 5.30000000e+01  0.00000000e+00 -3.00010000e+04  8.97200000e+03\n",
      "  -5.57617038e+00]\n",
      " [ 5.40000000e+01  0.00000000e+00 -1.00000000e+04  2.69000000e+03\n",
      "  -7.37297909e+00]\n",
      " [ 5.50000000e+01  0.00000000e+00 -2.00020000e+04  1.04520000e+04\n",
      "  -5.69672734e+00]\n",
      " [ 5.60000000e+01  0.00000000e+00 -2.00020000e+04  1.11010000e+04\n",
      "  -5.53226074e+00]\n",
      " [ 5.70000000e+01  0.00000000e+00 -1.00000000e+04  2.69000000e+03\n",
      "  -7.49819285e+00]\n",
      " [ 5.80000000e+01  0.00000000e+00 -1.00000000e+04  9.11300000e+03\n",
      "  -6.86534763e+00]\n",
      " [ 5.90000000e+01  0.00000000e+00 -3.00020000e+04  4.17300000e+03\n",
      "  -5.67801154e+00]\n",
      " [ 6.00000000e+01  0.00000000e+00 -2.00010000e+04  1.36640000e+04\n",
      "  -5.90865912e+00]\n",
      " [ 6.10000000e+01  0.00000000e+00 -1.00000000e+04  2.81700000e+03\n",
      "  -6.99444905e+00]\n",
      " [ 6.20000000e+01  0.00000000e+00 -1.00010000e+04  3.31800000e+03\n",
      "  -5.90314368e+00]\n",
      " [ 6.30000000e+01  0.00000000e+00 -2.00020000e+04  1.29640000e+04\n",
      "  -5.53630578e+00]\n",
      " [ 6.40000000e+01  0.00000000e+00 -1.00020000e+04  1.38310000e+04\n",
      "  -5.70628996e+00]\n",
      " [ 6.50000000e+01  0.00000000e+00 -1.00010000e+04  5.09400000e+03\n",
      "  -6.36532050e+00]\n",
      " [ 6.60000000e+01  0.00000000e+00 -1.00000000e+04  1.03050000e+04\n",
      "  -6.96120640e+00]\n",
      " [ 6.70000000e+01  0.00000000e+00 -1.00000000e+04  2.69000000e+03\n",
      "  -7.51467913e+00]\n",
      " [ 6.80000000e+01  0.00000000e+00 -1.00000000e+04  2.69000000e+03\n",
      "  -7.56583429e+00]\n",
      " [ 6.90000000e+01  0.00000000e+00 -1.00000000e+04  3.68100000e+03\n",
      "  -6.88193787e+00]\n",
      " [ 7.00000000e+01  0.00000000e+00 -3.00010000e+04  6.32900000e+03\n",
      "  -5.41189022e+00]\n",
      " [ 7.10000000e+01  0.00000000e+00 -3.00000000e+04  8.33100000e+03\n",
      "  -6.12926004e+00]\n",
      " [ 7.20000000e+01  0.00000000e+00 -1.00000000e+04  2.69000000e+03\n",
      "  -7.52214349e+00]\n",
      " [ 7.30000000e+01  0.00000000e+00 -2.00000000e+04  4.02200000e+03\n",
      "  -6.35333441e+00]\n",
      " [ 7.40000000e+01  0.00000000e+00 -1.00010000e+04  5.64200000e+03\n",
      "  -6.24150350e+00]\n",
      " [ 7.50000000e+01  0.00000000e+00 -1.00010000e+04  3.15000000e+03\n",
      "  -6.60171852e+00]\n",
      " [ 7.60000000e+01  0.00000000e+00 -3.00010000e+04  1.61320000e+04\n",
      "  -5.99775566e+00]\n",
      " [ 7.70000000e+01  0.00000000e+00 -3.00000000e+04  3.49900000e+03\n",
      "  -5.68458545e+00]\n",
      " [ 7.80000000e+01  0.00000000e+00 -1.00000000e+04  2.06900000e+03\n",
      "  -6.48315551e+00]\n",
      " [ 7.90000000e+01  0.00000000e+00 -1.00020000e+04  8.47300000e+03\n",
      "  -5.66416665e+00]\n",
      " [ 8.00000000e+01  0.00000000e+00 -5.00000000e+04  8.50100000e+03\n",
      "  -5.79071028e+00]\n",
      " [ 8.10000000e+01  0.00000000e+00 -1.00040000e+04  4.14900000e+03\n",
      "  -5.16097355e+00]\n",
      " [ 8.20000000e+01  0.00000000e+00 -3.00010000e+04  6.32500000e+03\n",
      "  -5.49965946e+00]\n",
      " [ 8.30000000e+01  0.00000000e+00 -1.00030000e+04  1.93770000e+04\n",
      "  -5.62798071e+00]\n",
      " [ 8.40000000e+01  0.00000000e+00 -3.00010000e+04  1.03010000e+04\n",
      "  -5.85615946e+00]\n",
      " [ 8.50000000e+01  0.00000000e+00 -2.00020000e+04  7.95300000e+03\n",
      "  -5.73834866e+00]\n",
      " [ 8.60000000e+01  0.00000000e+00 -2.00020000e+04  4.90090000e+04\n",
      "  -5.27655090e+00]\n",
      " [ 8.70000000e+01  0.00000000e+00 -2.00020000e+04  1.38040000e+04\n",
      "  -5.66823100e+00]\n",
      " [ 8.80000000e+01  0.00000000e+00 -4.00010000e+04  1.26900000e+04\n",
      "  -5.96741450e+00]\n",
      " [ 8.90000000e+01  0.00000000e+00 -4.00010000e+04  1.26900000e+04\n",
      "  -5.88283446e+00]\n",
      " [ 9.00000000e+01  0.00000000e+00 -2.00000000e+04  5.19100000e+03\n",
      "  -6.74817416e+00]\n",
      " [ 9.10000000e+01  0.00000000e+00 -3.00010000e+04  1.26810000e+04\n",
      "  -5.61578906e+00]\n",
      " [ 9.20000000e+01  0.00000000e+00 -3.00000000e+04  9.61200000e+03\n",
      "  -5.05584582e+00]\n",
      " [ 9.30000000e+01  0.00000000e+00 -1.00000000e+04  2.75300000e+03\n",
      "  -6.12195318e+00]\n",
      " [ 9.40000000e+01  0.00000000e+00 -1.00010000e+04  5.18300000e+03\n",
      "  -6.76995435e+00]\n",
      " [ 9.50000000e+01  0.00000000e+00 -2.00010000e+04  8.21700000e+03\n",
      "  -2.08236596e+00]\n",
      " [ 9.60000000e+01  0.00000000e+00 -1.00000000e+04  2.69000000e+03\n",
      "  -7.49647316e+00]\n",
      " [ 9.70000000e+01  0.00000000e+00 -1.00000000e+04  2.73700000e+03\n",
      "  -6.86374927e+00]\n",
      " [ 9.80000000e+01  0.00000000e+00 -2.00000000e+04  7.07000000e+03\n",
      "  -6.40692243e+00]\n",
      " [ 9.90000000e+01  0.00000000e+00 -2.00010000e+04  6.33600000e+03\n",
      "  -5.92449806e+00]\n",
      " [ 0.00000000e+00  1.00000000e+00 -1.00040000e+04  1.14570000e+04\n",
      "  -5.57510069e+00]\n",
      " [ 1.00000000e+00  1.00000000e+00 -1.00010000e+04  4.36600000e+03\n",
      "  -6.39371712e+00]\n",
      " [ 2.00000000e+00  1.00000000e+00 -1.00030000e+04  1.42570000e+04\n",
      "  -5.22934611e+00]\n",
      " [ 3.00000000e+00  1.00000000e+00 -2.00010000e+04  1.25030000e+04\n",
      "  -5.88172293e+00]\n",
      " [ 4.00000000e+00  1.00000000e+00 -1.00010000e+04  6.85900000e+03\n",
      "  -6.21253696e+00]\n",
      " [ 5.00000000e+00  1.00000000e+00 -1.00000000e+04  2.95300000e+03\n",
      "  -6.92587228e+00]\n",
      " [ 6.00000000e+00  1.00000000e+00 -4.00010000e+04  1.62730000e+04\n",
      "  -5.79227192e+00]\n",
      " [ 7.00000000e+00  1.00000000e+00 -3.00010000e+04  7.01300000e+03\n",
      "  -5.76445275e+00]\n",
      " [ 8.00000000e+00  1.00000000e+00 -2.00000000e+04  3.51000000e+03\n",
      "  -6.41431859e+00]\n",
      " [ 9.00000000e+00  1.00000000e+00 -1.00000000e+04  2.69000000e+03\n",
      "  -7.34011579e+00]\n",
      " [ 1.00000000e+01  1.00000000e+00 -1.00000000e+04  1.10890000e+04\n",
      "  -6.79818458e+00]\n",
      " [ 1.10000000e+01  1.00000000e+00 -5.00000000e+04  4.53300000e+03\n",
      "  -5.58941561e+00]\n",
      " [ 1.20000000e+01  1.00000000e+00 -1.00000000e+04  2.69000000e+03\n",
      "  -7.33681485e+00]\n",
      " [ 1.30000000e+01  1.00000000e+00 -1.00000000e+04  3.16900000e+03\n",
      "  -6.82240722e+00]\n",
      " [ 1.40000000e+01  1.00000000e+00 -2.00020000e+04  1.82200000e+04\n",
      "  -5.75748514e+00]\n",
      " [ 1.50000000e+01  1.00000000e+00 -1.00010000e+04  5.18300000e+03\n",
      "  -6.85037797e+00]\n",
      " [ 1.60000000e+01  1.00000000e+00 -1.00000000e+04  3.16900000e+03\n",
      "  -6.64074262e+00]\n",
      " [ 1.70000000e+01  1.00000000e+00 -2.00010000e+04  8.16700000e+03\n",
      "  -5.89142632e+00]\n",
      " [ 1.80000000e+01  1.00000000e+00 -2.00020000e+04  3.37790000e+04\n",
      "  -5.54860127e+00]\n",
      " [ 1.90000000e+01  1.00000000e+00 -1.00000000e+04  2.69000000e+03\n",
      "  -7.36202939e+00]\n",
      " [ 2.00000000e+01  1.00000000e+00 -1.00000000e+04  2.78600000e+03\n",
      "  -7.21040321e+00]\n",
      " [ 2.10000000e+01  1.00000000e+00 -1.00000000e+04  2.69000000e+03\n",
      "  -7.48962382e+00]\n",
      " [ 2.20000000e+01  1.00000000e+00 -1.00000000e+04  2.69000000e+03\n",
      "  -7.51336765e+00]\n",
      " [ 2.30000000e+01  1.00000000e+00 -2.00000000e+04  6.91800000e+03\n",
      "  -6.39800407e+00]\n",
      " [ 2.40000000e+01  1.00000000e+00 -2.00010000e+04  5.78300000e+03\n",
      "  -5.11531353e+00]\n",
      " [ 2.50000000e+01  1.00000000e+00 -2.00020000e+04  8.20900000e+03\n",
      "  -5.46645021e+00]\n",
      " [ 2.60000000e+01  1.00000000e+00 -1.00000000e+04  3.09700000e+03\n",
      "  -6.76705039e+00]\n",
      " [ 2.70000000e+01  1.00000000e+00 -1.00000000e+04  2.69000000e+03\n",
      "  -7.39563316e+00]\n",
      " [ 2.80000000e+01  1.00000000e+00 -2.00000000e+04  7.21500000e+03\n",
      "  -2.46455116e+00]\n",
      " [ 2.90000000e+01  1.00000000e+00 -1.00000000e+04  2.69000000e+03\n",
      "  -7.52390793e+00]\n",
      " [ 3.00000000e+01  1.00000000e+00 -2.00000000e+04  9.35800000e+03\n",
      "  -6.41126648e+00]\n",
      " [ 3.10000000e+01  1.00000000e+00 -1.00000000e+04  2.79300000e+03\n",
      "  -7.26137388e+00]\n",
      " [ 3.20000000e+01  1.00000000e+00 -1.00000000e+04  2.69000000e+03\n",
      "  -7.49133176e+00]\n",
      " [ 3.30000000e+01  1.00000000e+00 -1.00000000e+04  2.69000000e+03\n",
      "  -7.32011149e+00]\n",
      " [ 3.40000000e+01  1.00000000e+00 -1.00000000e+04  6.90500000e+03\n",
      "  -6.23527751e+00]\n",
      " [ 3.50000000e+01  1.00000000e+00 -1.00030000e+04  1.00400000e+04\n",
      "  -5.42978442e+00]\n",
      " [ 3.60000000e+01  1.00000000e+00 -2.00000000e+04  5.19100000e+03\n",
      "  -6.74148924e+00]\n",
      " [ 3.70000000e+01  1.00000000e+00 -2.00020000e+04  9.31200000e+03\n",
      "  -5.72598647e+00]\n",
      " [ 3.80000000e+01  1.00000000e+00 -1.00000000e+04  2.69000000e+03\n",
      "  -7.41365873e+00]\n",
      " [ 3.90000000e+01  1.00000000e+00 -2.00010000e+04  8.61500000e+03\n",
      "  -5.85615946e+00]\n",
      " [ 4.00000000e+01  1.00000000e+00 -2.00000000e+04  5.19100000e+03\n",
      "  -6.73886801e+00]\n",
      " [ 4.10000000e+01  1.00000000e+00 -1.00000000e+04  3.52100000e+03\n",
      "  -6.30991923e+00]\n",
      " [ 4.20000000e+01  1.00000000e+00 -4.00010000e+04  1.07450000e+04\n",
      "  -5.19410287e+00]\n",
      " [ 4.30000000e+01  1.00000000e+00 -1.00000000e+04  2.69000000e+03\n",
      "  -7.51293088e+00]\n",
      " [ 4.40000000e+01  1.00000000e+00 -2.00000000e+04  5.46200000e+03\n",
      "  -6.59664014e+00]\n",
      " [ 4.50000000e+01  1.00000000e+00 -2.00020000e+04  1.17800000e+04\n",
      "  -5.69644329e+00]\n",
      " [ 4.60000000e+01  1.00000000e+00 -1.00000000e+04  2.69000000e+03\n",
      "  -7.52966398e+00]\n",
      " [ 4.70000000e+01  1.00000000e+00 -1.00000000e+04  4.16900000e+03\n",
      "  -6.81586127e+00]\n",
      " [ 4.80000000e+01  1.00000000e+00 -4.00000000e+04  4.97600000e+03\n",
      "  -5.42620154e+00]\n",
      " [ 4.90000000e+01  1.00000000e+00 -1.00000000e+04  9.40900000e+03\n",
      "  -6.91692962e+00]\n",
      " [ 5.00000000e+01  1.00000000e+00 -1.00020000e+04  9.33500000e+03\n",
      "  -5.77568754e+00]\n",
      " [ 5.10000000e+01  1.00000000e+00 -1.00010000e+04  4.71800000e+03\n",
      "  -6.10183740e+00]\n",
      " [ 5.20000000e+01  1.00000000e+00 -1.00000000e+04  1.17730000e+04\n",
      "  -6.45093425e+00]\n",
      " [ 5.30000000e+01  1.00000000e+00 -1.00020000e+04  6.13100000e+03\n",
      "  -6.05757087e+00]\n",
      " [ 5.40000000e+01  1.00000000e+00 -2.00000000e+04  4.47800000e+03\n",
      "  -5.55919120e+00]\n",
      " [ 5.50000000e+01  1.00000000e+00 -3.00000000e+04  7.11500000e+03\n",
      "  -6.17345809e+00]\n",
      " [ 5.60000000e+01  1.00000000e+00 -2.00010000e+04  1.74790000e+04\n",
      "  -6.12249662e+00]\n",
      " [ 5.70000000e+01  1.00000000e+00 -3.00000000e+04  4.20300000e+03\n",
      "  -5.92040371e+00]\n",
      " [ 5.80000000e+01  1.00000000e+00 -1.00000000e+04  2.69000000e+03\n",
      "  -7.49090450e+00]\n",
      " [ 5.90000000e+01  1.00000000e+00 -4.00000000e+04  1.46320000e+04\n",
      "  -5.81487444e+00]\n",
      " [ 6.00000000e+01  1.00000000e+00 -2.00010000e+04  3.55500000e+03\n",
      "  -6.19625341e+00]\n",
      " [ 6.10000000e+01  1.00000000e+00 -1.00010000e+04  5.18300000e+03\n",
      "  -6.84185965e+00]\n",
      " [ 6.20000000e+01  1.00000000e+00 -3.00000000e+00  5.01300000e+03\n",
      "  -5.38782320e+00]\n",
      " [ 6.30000000e+01  1.00000000e+00 -1.00010000e+04  1.26620000e+04\n",
      "  -5.36147338e+00]\n",
      " [ 6.40000000e+01  1.00000000e+00 -4.00010000e+04  3.57700000e+03\n",
      "  -5.21266288e+00]\n",
      " [ 6.50000000e+01  1.00000000e+00 -2.00000000e+04  9.33700000e+03\n",
      "  -5.37605500e+00]\n",
      " [ 6.60000000e+01  1.00000000e+00 -3.00000000e+04  1.42560000e+04\n",
      "  -5.31667717e+00]\n",
      " [ 6.70000000e+01  1.00000000e+00 -1.00020000e+04  1.09840000e+04\n",
      "  -5.24780735e+00]\n",
      " [ 6.80000000e+01  1.00000000e+00 -2.00020000e+04  7.83200000e+03\n",
      "  -5.71869994e+00]\n",
      " [ 6.90000000e+01  1.00000000e+00 -1.00010000e+04  1.06990000e+04\n",
      "  -5.84103083e+00]\n",
      " [ 7.00000000e+01  1.00000000e+00 -3.00000000e+04  1.42450000e+04\n",
      "  -5.26370971e+00]\n",
      " [ 7.10000000e+01  1.00000000e+00 -1.00040000e+04  7.98900000e+03\n",
      "  -5.33209409e+00]\n",
      " [ 7.20000000e+01  1.00000000e+00 -1.00030000e+04  1.02760000e+04\n",
      "  -5.43759162e+00]\n",
      " [ 7.30000000e+01  1.00000000e+00 -2.00010000e+04  7.57500000e+03\n",
      "  -6.11837380e+00]\n",
      " [ 7.40000000e+01  1.00000000e+00 -1.00010000e+04  1.05660000e+04\n",
      "  -6.32951732e+00]\n",
      " [ 7.50000000e+01  1.00000000e+00 -3.00000000e+04  7.46700000e+03\n",
      "  -5.90778162e+00]\n",
      " [ 7.60000000e+01  1.00000000e+00 -3.00010000e+04  5.45600000e+03\n",
      "  -6.07670332e+00]\n",
      " [ 7.70000000e+01  1.00000000e+00 -2.00020000e+04  9.20000000e+03\n",
      "  -5.82960960e+00]\n",
      " [ 7.80000000e+01  1.00000000e+00 -1.00000000e+04  2.69000000e+03\n",
      "  -7.39757679e+00]\n",
      " [ 7.90000000e+01  1.00000000e+00 -1.00000000e+04  2.69000000e+03\n",
      "  -7.53679414e+00]\n",
      " [ 8.00000000e+01  1.00000000e+00 -2.00000000e+04  5.46200000e+03\n",
      "  -6.74715842e+00]\n",
      " [ 8.10000000e+01  1.00000000e+00 -3.00010000e+04  7.05200000e+03\n",
      "  -5.67571388e+00]\n",
      " [ 8.20000000e+01  1.00000000e+00 -1.00000000e+04  2.69000000e+03\n",
      "  -6.74229716e+00]\n",
      " [ 8.30000000e+01  1.00000000e+00 -3.00000000e+04  7.96300000e+03\n",
      "  -6.00933880e+00]\n",
      " [ 8.40000000e+01  1.00000000e+00 -1.00020000e+04  1.66400000e+04\n",
      "  -5.48754443e+00]\n",
      " [ 8.50000000e+01  1.00000000e+00 -2.00010000e+04  1.58950000e+04\n",
      "  -6.30494865e+00]\n",
      " [ 8.60000000e+01  1.00000000e+00 -1.00000000e+04  4.73700000e+03\n",
      "  -6.75326842e+00]\n",
      " [ 8.70000000e+01  1.00000000e+00 -3.00020000e+04  9.71300000e+03\n",
      "  -5.34880508e+00]\n",
      " [ 8.80000000e+01  1.00000000e+00 -1.00020000e+04  1.52090000e+04\n",
      "  -2.44463382e+00]\n",
      " [ 8.90000000e+01  1.00000000e+00 -2.00010000e+04  5.41500000e+03\n",
      "  -6.00875644e+00]\n",
      " [ 9.00000000e+01  1.00000000e+00 -1.00010000e+04  5.71800000e+03\n",
      "  -6.30325385e+00]\n",
      " [ 9.10000000e+01  1.00000000e+00 -1.00000000e+04  3.44100000e+03\n",
      "  -6.75081994e+00]\n",
      " [ 9.20000000e+01  1.00000000e+00 -1.00030000e+04  2.15130000e+04\n",
      "  -5.66795493e+00]\n",
      " [ 9.30000000e+01  1.00000000e+00 -1.00030000e+04  5.16500000e+03\n",
      "  -5.50070976e+00]\n",
      " [ 9.40000000e+01  1.00000000e+00 -3.00020000e+04  1.55810000e+04\n",
      "  -5.24044491e+00]\n",
      " [ 9.50000000e+01  1.00000000e+00 -3.00000000e+04  8.85500000e+03\n",
      "  -5.80723048e+00]\n",
      " [ 9.60000000e+01  1.00000000e+00 -2.00020000e+04  9.54100000e+03\n",
      "  -5.50211187e+00]\n",
      " [ 9.70000000e+01  1.00000000e+00 -2.00030000e+04  8.60900000e+03\n",
      "  -5.76134162e+00]\n",
      " [ 9.80000000e+01  1.00000000e+00 -2.00010000e+04  2.10960000e+04\n",
      "  -5.54805022e+00]\n",
      " [ 9.90000000e+01  1.00000000e+00 -1.00000000e+04  3.80100000e+03\n",
      "  -6.73304528e+00]]\n",
      "Writing to file data.csv\n",
      "**************************************************\n",
      "Data save complete\n",
      "**************************************************\n",
      "max_weight_fixed\n",
      "**************************************************\n",
      "Running experiment\n",
      "**************************************************\n",
      "Episode : 0\n",
      "state : [7 0 3 0 1 1]\n",
      "action : 0\n",
      "new state: [7 0 3 0 0 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [7 0 3 0 0 0]\n",
      "action : 0\n",
      "new state: [7 0 3 0 3 1]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [7 0 3 0 3 1]\n",
      "action : 0\n",
      "new state: [7 0 3 0 0 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [7 0 3 0 0 1]\n",
      "action : 0\n",
      "new state: [6 1 3 0 1 0]\n",
      "reward: -0.0\n",
      "epReward so far: -20000.0\n",
      "state : [6 1 3 0 1 0]\n",
      "action : 0\n",
      "new state: [6 1 3 0 2 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -30000.0\n",
      "final state: [6 1 3 0 2 0]\n",
      "Episode : 1\n",
      "state : [6 1 3 0 2 0]\n",
      "action : 0\n",
      "new state: [6 1 3 0 2 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [6 1 3 0 2 0]\n",
      "action : 0\n",
      "new state: [6 1 3 0 2 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [6 1 3 0 2 0]\n",
      "action : 0\n",
      "new state: [6 1 3 0 2 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -30000.0\n",
      "state : [6 1 3 0 2 0]\n",
      "action : 0\n",
      "new state: [6 1 3 0 1 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -40000.0\n",
      "state : [6 1 3 0 1 1]\n",
      "action : 0\n",
      "new state: [6 1 3 0 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -50000.0\n",
      "final state: [6 1 3 0 0 2]\n",
      "Episode : 2\n",
      "state : [6 1 3 0 0 2]\n",
      "action : 0\n",
      "new state: [6 1 3 0 1 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [6 1 3 0 1 3]\n",
      "action : 0\n",
      "new state: [6 1 3 0 2 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [6 1 3 0 2 1]\n",
      "action : 0\n",
      "new state: [5 2 3 0 0 1]\n",
      "reward: -1.0\n",
      "epReward so far: -20001.0\n",
      "state : [5 2 3 0 0 1]\n",
      "action : 0\n",
      "new state: [4 3 3 0 1 3]\n",
      "reward: -0.0\n",
      "epReward so far: -20001.0\n",
      "state : [4 3 3 0 1 3]\n",
      "action : 0\n",
      "new state: [4 3 3 0 1 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -30001.0\n",
      "final state: [4 3 3 0 1 0]\n",
      "Episode : 3\n",
      "state : [4 3 3 0 1 0]\n",
      "action : 0\n",
      "new state: [4 3 3 0 3 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [4 3 3 0 3 2]\n",
      "action : 0\n",
      "new state: [3 3 4 0 1 3]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [3 3 4 0 1 3]\n",
      "action : 2\n",
      "new state: [3 3 3 1 1 3]\n",
      "reward: -1.0\n",
      "epReward so far: -10002.0\n",
      "state : [3 3 3 1 1 3]\n",
      "action : 0\n",
      "new state: [2 3 3 2 0 0]\n",
      "reward: -1.0\n",
      "epReward so far: -10003.0\n",
      "state : [2 3 3 2 0 0]\n",
      "action : 1\n",
      "new state: [2 3 3 2 1 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20003.0\n",
      "final state: [2 3 3 2 1 2]\n",
      "Episode : 4\n",
      "state : [2 3 3 2 1 2]\n",
      "action : 1\n",
      "new state: [2 3 3 2 0 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 3 3 2 0 1]\n",
      "action : 1\n",
      "new state: [2 3 3 2 3 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [2 3 3 2 3 3]\n",
      "action : 1\n",
      "new state: [2 3 3 2 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -30000.0\n",
      "state : [2 3 3 2 0 2]\n",
      "action : 1\n",
      "new state: [2 2 4 2 3 2]\n",
      "reward: -1.0\n",
      "epReward so far: -30001.0\n",
      "state : [2 2 4 2 3 2]\n",
      "action : 2\n",
      "new state: [2 2 4 2 0 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -40001.0\n",
      "final state: [2 2 4 2 0 3]\n",
      "Episode : 5\n",
      "state : [2 2 4 2 0 3]\n",
      "action : 2\n",
      "new state: [2 2 4 2 3 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 2 4 2 3 2]\n",
      "action : 2\n",
      "new state: [2 2 4 2 2 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [2 2 4 2 2 2]\n",
      "action : 2\n",
      "new state: [2 2 4 2 3 3]\n",
      "reward: -0.0\n",
      "epReward so far: -20000.0\n",
      "state : [2 2 4 2 3 3]\n",
      "action : 2\n",
      "new state: [2 2 3 3 3 0]\n",
      "reward: -1.0\n",
      "epReward so far: -20001.0\n",
      "state : [2 2 3 3 3 0]\n",
      "action : 2\n",
      "new state: [3 2 2 3 2 0]\n",
      "reward: -1.0\n",
      "epReward so far: -20002.0\n",
      "final state: [3 2 2 3 2 0]\n",
      "Episode : 6\n",
      "state : [3 2 2 3 2 0]\n",
      "action : 0\n",
      "new state: [3 2 2 3 3 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [3 2 2 3 3 1]\n",
      "action : 0\n",
      "new state: [2 3 2 3 3 0]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 3 2 3 3 0]\n",
      "action : 1\n",
      "new state: [2 3 2 3 1 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "state : [2 3 2 3 1 2]\n",
      "action : 1\n",
      "new state: [2 2 3 3 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: -20001.0\n",
      "state : [2 2 3 3 2 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 1 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -30001.0\n",
      "final state: [2 2 3 3 1 3]\n",
      "Episode : 7\n",
      "state : [2 2 3 3 1 3]\n",
      "action : 2\n",
      "new state: [2 2 2 4 0 3]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [2 2 2 4 0 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 2 2 4 0 2]\n",
      "action : 3\n",
      "new state: [2 2 2 4 3 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "state : [2 2 2 4 3 1]\n",
      "action : 3\n",
      "new state: [2 2 2 4 0 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -30001.0\n",
      "state : [2 2 2 4 0 0]\n",
      "action : 3\n",
      "new state: [3 2 2 3 1 2]\n",
      "reward: -1.0\n",
      "epReward so far: -30002.0\n",
      "final state: [3 2 2 3 1 2]\n",
      "Episode : 8\n",
      "state : [3 2 2 3 1 2]\n",
      "action : 0\n",
      "new state: [2 2 3 3 1 3]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [2 2 3 3 1 3]\n",
      "action : 2\n",
      "new state: [2 2 2 4 2 0]\n",
      "reward: -1.0\n",
      "epReward so far: -2.0\n",
      "state : [2 2 2 4 2 0]\n",
      "action : 3\n",
      "new state: [2 2 2 4 0 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10002.0\n",
      "state : [2 2 2 4 0 1]\n",
      "action : 3\n",
      "new state: [2 2 2 4 1 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -20002.0\n",
      "state : [2 2 2 4 1 0]\n",
      "action : 3\n",
      "new state: [3 2 2 3 1 2]\n",
      "reward: -1.0\n",
      "epReward so far: -20003.0\n",
      "final state: [3 2 2 3 1 2]\n",
      "Episode : 9\n",
      "state : [3 2 2 3 1 2]\n",
      "action : 0\n",
      "new state: [3 2 2 3 3 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [3 2 2 3 3 2]\n",
      "action : 0\n",
      "new state: [3 2 2 3 0 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [3 2 2 3 0 1]\n",
      "action : 0\n",
      "new state: [2 3 2 3 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: -20000.0\n",
      "state : [2 3 2 3 2 2]\n",
      "action : 1\n",
      "new state: [2 2 3 3 0 2]\n",
      "reward: -1.0\n",
      "epReward so far: -20001.0\n",
      "state : [2 2 3 3 0 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 1 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -30001.0\n",
      "final state: [2 2 3 3 1 2]\n",
      "Episode : 10\n",
      "state : [2 2 3 3 1 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 2 3 3 0 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 1 2]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 2 3 3 1 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 0 2]\n",
      "reward: -1.0\n",
      "epReward so far: -10002.0\n",
      "state : [2 2 3 3 0 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 1 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -20002.0\n",
      "state : [2 2 3 3 1 0]\n",
      "action : 2\n",
      "new state: [3 2 2 3 3 0]\n",
      "reward: -1.0\n",
      "epReward so far: -20003.0\n",
      "final state: [3 2 2 3 3 0]\n",
      "Episode : 11\n",
      "state : [3 2 2 3 3 0]\n",
      "action : 0\n",
      "new state: [3 2 2 3 1 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [3 2 2 3 1 1]\n",
      "action : 0\n",
      "new state: [2 3 2 3 0 0]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 3 2 3 0 0]\n",
      "action : 1\n",
      "new state: [2 3 2 3 1 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "state : [2 3 2 3 1 0]\n",
      "action : 1\n",
      "new state: [2 3 2 3 0 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -30001.0\n",
      "state : [2 3 2 3 0 3]\n",
      "action : 1\n",
      "new state: [2 2 2 4 3 1]\n",
      "reward: -1.0\n",
      "epReward so far: -30002.0\n",
      "final state: [2 2 2 4 3 1]\n",
      "Episode : 12\n",
      "state : [2 2 2 4 3 1]\n",
      "action : 3\n",
      "new state: [2 3 2 3 1 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 3 2 3 1 0]\n",
      "action : 1\n",
      "new state: [3 2 2 3 1 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 2 2 3 1 3]\n",
      "action : 0\n",
      "new state: [2 2 2 4 3 1]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [2 2 2 4 3 1]\n",
      "action : 3\n",
      "new state: [2 3 2 3 1 3]\n",
      "reward: -0.0\n",
      "epReward so far: -1.0\n",
      "state : [2 3 2 3 1 3]\n",
      "action : 1\n",
      "new state: [2 2 2 4 2 1]\n",
      "reward: -0.0\n",
      "epReward so far: -1.0\n",
      "final state: [2 2 2 4 2 1]\n",
      "Episode : 13\n",
      "state : [2 2 2 4 2 1]\n",
      "action : 3\n",
      "new state: [2 3 2 3 2 2]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [2 3 2 3 2 2]\n",
      "action : 1\n",
      "new state: [2 2 3 3 1 3]\n",
      "reward: -1.0\n",
      "epReward so far: -2.0\n",
      "state : [2 2 3 3 1 3]\n",
      "action : 2\n",
      "new state: [2 2 2 4 1 1]\n",
      "reward: -1.0\n",
      "epReward so far: -3.0\n",
      "state : [2 2 2 4 1 1]\n",
      "action : 3\n",
      "new state: [2 3 2 3 0 3]\n",
      "reward: -1.0\n",
      "epReward so far: -4.0\n",
      "state : [2 3 2 3 0 3]\n",
      "action : 1\n",
      "new state: [2 2 2 4 0 1]\n",
      "reward: -1.0\n",
      "epReward so far: -5.0\n",
      "final state: [2 2 2 4 0 1]\n",
      "Episode : 14\n",
      "state : [2 2 2 4 0 1]\n",
      "action : 3\n",
      "new state: [2 3 2 3 2 3]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [2 3 2 3 2 3]\n",
      "action : 1\n",
      "new state: [2 3 2 3 2 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 3 2 3 2 0]\n",
      "action : 1\n",
      "new state: [2 3 2 3 0 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "state : [2 3 2 3 0 0]\n",
      "action : 1\n",
      "new state: [2 3 2 3 2 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -30001.0\n",
      "state : [2 3 2 3 2 3]\n",
      "action : 1\n",
      "new state: [2 2 2 4 0 3]\n",
      "reward: -1.0\n",
      "epReward so far: -30002.0\n",
      "final state: [2 2 2 4 0 3]\n",
      "Episode : 15\n",
      "state : [2 2 2 4 0 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 2 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 2 2 4 2 0]\n",
      "action : 3\n",
      "new state: [3 2 2 3 2 3]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [3 2 2 3 2 3]\n",
      "action : 0\n",
      "new state: [3 2 2 3 3 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "state : [3 2 2 3 3 3]\n",
      "action : 0\n",
      "new state: [3 2 2 3 2 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -30001.0\n",
      "state : [3 2 2 3 2 3]\n",
      "action : 0\n",
      "new state: [2 2 2 4 3 2]\n",
      "reward: -1.0\n",
      "epReward so far: -30002.0\n",
      "final state: [2 2 2 4 3 2]\n",
      "Episode : 16\n",
      "state : [2 2 2 4 3 2]\n",
      "action : 3\n",
      "new state: [2 2 3 3 2 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 2 3 3 2 0]\n",
      "action : 2\n",
      "new state: [3 2 2 3 3 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 2 2 3 3 3]\n",
      "action : 0\n",
      "new state: [3 2 2 3 1 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [3 2 2 3 1 2]\n",
      "action : 0\n",
      "new state: [3 2 2 3 3 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [3 2 2 3 3 1]\n",
      "action : 0\n",
      "new state: [2 3 2 3 1 3]\n",
      "reward: -1.0\n",
      "epReward so far: -20001.0\n",
      "final state: [2 3 2 3 1 3]\n",
      "Episode : 17\n",
      "state : [2 3 2 3 1 3]\n",
      "action : 1\n",
      "new state: [2 2 2 4 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 2 2 4 2 2]\n",
      "action : 3\n",
      "new state: [2 2 3 3 2 1]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [2 2 3 3 2 1]\n",
      "action : 2\n",
      "new state: [2 3 2 3 0 0]\n",
      "reward: -0.0\n",
      "epReward so far: -1.0\n",
      "state : [2 3 2 3 0 0]\n",
      "action : 1\n",
      "new state: [3 2 2 3 3 3]\n",
      "reward: -1.0\n",
      "epReward so far: -2.0\n",
      "state : [3 2 2 3 3 3]\n",
      "action : 0\n",
      "new state: [3 2 2 3 2 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10002.0\n",
      "final state: [3 2 2 3 2 2]\n",
      "Episode : 18\n",
      "state : [3 2 2 3 2 2]\n",
      "action : 0\n",
      "new state: [2 2 3 3 1 2]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [2 2 3 3 1 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 0 0]\n",
      "reward: -1.0\n",
      "epReward so far: -2.0\n",
      "state : [2 2 3 3 0 0]\n",
      "action : 2\n",
      "new state: [2 2 3 3 0 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10002.0\n",
      "state : [2 2 3 3 0 1]\n",
      "action : 2\n",
      "new state: [2 2 3 3 2 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20002.0\n",
      "state : [2 2 3 3 2 3]\n",
      "action : 2\n",
      "new state: [2 2 2 4 1 0]\n",
      "reward: -0.0\n",
      "epReward so far: -20002.0\n",
      "final state: [2 2 2 4 1 0]\n",
      "Episode : 19\n",
      "state : [2 2 2 4 1 0]\n",
      "action : 3\n",
      "new state: [2 2 2 4 3 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 2 2 4 3 2]\n",
      "action : 3\n",
      "new state: [2 2 2 4 3 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [2 2 2 4 3 2]\n",
      "action : 3\n",
      "new state: [2 2 2 4 2 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -30000.0\n",
      "state : [2 2 2 4 2 0]\n",
      "action : 3\n",
      "new state: [2 2 2 4 0 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -40000.0\n",
      "state : [2 2 2 4 0 0]\n",
      "action : 3\n",
      "new state: [3 2 2 3 3 3]\n",
      "reward: -1.0\n",
      "epReward so far: -40001.0\n",
      "final state: [3 2 2 3 3 3]\n",
      "Episode : 20\n",
      "state : [3 2 2 3 3 3]\n",
      "action : 0\n",
      "new state: [2 2 2 4 1 3]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [2 2 2 4 1 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 0 1]\n",
      "reward: -1.0\n",
      "epReward so far: -2.0\n",
      "state : [2 2 2 4 0 1]\n",
      "action : 3\n",
      "new state: [2 2 2 4 2 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10002.0\n",
      "state : [2 2 2 4 2 2]\n",
      "action : 3\n",
      "new state: [2 2 3 3 1 2]\n",
      "reward: -1.0\n",
      "epReward so far: -10003.0\n",
      "state : [2 2 3 3 1 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 0 0]\n",
      "reward: -1.0\n",
      "epReward so far: -10004.0\n",
      "final state: [2 2 3 3 0 0]\n",
      "Episode : 21\n",
      "state : [2 2 3 3 0 0]\n",
      "action : 2\n",
      "new state: [2 2 3 3 2 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 2 3 3 2 0]\n",
      "action : 2\n",
      "new state: [3 2 2 3 3 1]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [3 2 2 3 3 1]\n",
      "action : 0\n",
      "new state: [3 2 2 3 2 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [3 2 2 3 2 2]\n",
      "action : 0\n",
      "new state: [3 2 2 3 1 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -30000.0\n",
      "state : [3 2 2 3 1 1]\n",
      "action : 0\n",
      "new state: [2 3 2 3 3 2]\n",
      "reward: -1.0\n",
      "epReward so far: -30001.0\n",
      "final state: [2 3 2 3 3 2]\n",
      "Episode : 22\n",
      "state : [2 3 2 3 3 2]\n",
      "action : 1\n",
      "new state: [2 3 2 3 1 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 3 2 3 1 0]\n",
      "action : 1\n",
      "new state: [3 2 2 3 3 1]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [3 2 2 3 3 1]\n",
      "action : 0\n",
      "new state: [3 2 2 3 2 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [3 2 2 3 2 0]\n",
      "action : 0\n",
      "new state: [3 2 2 3 2 3]\n",
      "reward: -1.0\n",
      "epReward so far: -20001.0\n",
      "state : [3 2 2 3 2 3]\n",
      "action : 0\n",
      "new state: [2 2 2 4 2 0]\n",
      "reward: -1.0\n",
      "epReward so far: -20002.0\n",
      "final state: [2 2 2 4 2 0]\n",
      "Episode : 23\n",
      "state : [2 2 2 4 2 0]\n",
      "action : 3\n",
      "new state: [2 2 2 4 0 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 2 2 4 0 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 3 0]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 2 2 4 3 0]\n",
      "action : 3\n",
      "new state: [3 2 2 3 1 0]\n",
      "reward: -0.0\n",
      "epReward so far: -10001.0\n",
      "state : [3 2 2 3 1 0]\n",
      "action : 0\n",
      "new state: [3 2 2 3 2 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "state : [3 2 2 3 2 2]\n",
      "action : 0\n",
      "new state: [3 2 2 3 2 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -30001.0\n",
      "final state: [3 2 2 3 2 3]\n",
      "Episode : 24\n",
      "state : [3 2 2 3 2 3]\n",
      "action : 0\n",
      "new state: [3 2 2 3 1 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [3 2 2 3 1 1]\n",
      "action : 0\n",
      "new state: [2 3 2 3 2 1]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 3 2 3 2 1]\n",
      "action : 1\n",
      "new state: [2 3 2 3 2 2]\n",
      "reward: -1.0\n",
      "epReward so far: -10002.0\n",
      "state : [2 3 2 3 2 2]\n",
      "action : 1\n",
      "new state: [2 3 2 3 0 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -20002.0\n",
      "state : [2 3 2 3 0 0]\n",
      "action : 1\n",
      "new state: [3 2 2 3 3 2]\n",
      "reward: -1.0\n",
      "epReward so far: -20003.0\n",
      "final state: [3 2 2 3 3 2]\n",
      "Episode : 25\n",
      "state : [3 2 2 3 3 2]\n",
      "action : 0\n",
      "new state: [3 2 2 3 3 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [3 2 2 3 3 1]\n",
      "action : 0\n",
      "new state: [3 2 2 3 1 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [3 2 2 3 1 1]\n",
      "action : 0\n",
      "new state: [2 3 2 3 2 0]\n",
      "reward: -1.0\n",
      "epReward so far: -20001.0\n",
      "state : [2 3 2 3 2 0]\n",
      "action : 1\n",
      "new state: [3 2 2 3 0 2]\n",
      "reward: -1.0\n",
      "epReward so far: -20002.0\n",
      "state : [3 2 2 3 0 2]\n",
      "action : 0\n",
      "new state: [2 2 3 3 1 1]\n",
      "reward: -0.0\n",
      "epReward so far: -20002.0\n",
      "final state: [2 2 3 3 1 1]\n",
      "Episode : 26\n",
      "state : [2 2 3 3 1 1]\n",
      "action : 2\n",
      "new state: [2 3 2 3 0 2]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [2 3 2 3 0 2]\n",
      "action : 1\n",
      "new state: [2 2 3 3 3 3]\n",
      "reward: -1.0\n",
      "epReward so far: -2.0\n",
      "state : [2 2 3 3 3 3]\n",
      "action : 2\n",
      "new state: [2 2 2 4 1 3]\n",
      "reward: -1.0\n",
      "epReward so far: -3.0\n",
      "state : [2 2 2 4 1 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 1 2]\n",
      "reward: -1.0\n",
      "epReward so far: -4.0\n",
      "state : [2 2 2 4 1 2]\n",
      "action : 3\n",
      "new state: [2 2 2 4 2 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10004.0\n",
      "final state: [2 2 2 4 2 3]\n",
      "Episode : 27\n",
      "state : [2 2 2 4 2 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 2 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 2 2 4 2 0]\n",
      "action : 3\n",
      "new state: [3 2 2 3 3 0]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [3 2 2 3 3 0]\n",
      "action : 0\n",
      "new state: [3 2 2 3 1 1]\n",
      "reward: -1.0\n",
      "epReward so far: -10002.0\n",
      "state : [3 2 2 3 1 1]\n",
      "action : 0\n",
      "new state: [3 2 2 3 3 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20002.0\n",
      "state : [3 2 2 3 3 2]\n",
      "action : 0\n",
      "new state: [3 2 2 3 2 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -30002.0\n",
      "final state: [3 2 2 3 2 0]\n",
      "Episode : 28\n",
      "state : [3 2 2 3 2 0]\n",
      "action : 0\n",
      "new state: [3 2 2 3 3 3]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [3 2 2 3 3 3]\n",
      "action : 0\n",
      "new state: [2 2 2 4 2 3]\n",
      "reward: -1.0\n",
      "epReward so far: -2.0\n",
      "state : [2 2 2 4 2 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 2 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10002.0\n",
      "state : [2 2 2 4 2 0]\n",
      "action : 3\n",
      "new state: [3 2 2 3 2 0]\n",
      "reward: -1.0\n",
      "epReward so far: -10003.0\n",
      "state : [3 2 2 3 2 0]\n",
      "action : 0\n",
      "new state: [3 2 2 3 0 1]\n",
      "reward: -1.0\n",
      "epReward so far: -10004.0\n",
      "final state: [3 2 2 3 0 1]\n",
      "Episode : 29\n",
      "state : [3 2 2 3 0 1]\n",
      "action : 0\n",
      "new state: [2 3 2 3 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 3 2 3 2 2]\n",
      "action : 1\n",
      "new state: [2 2 3 3 2 3]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [2 2 3 3 2 3]\n",
      "action : 2\n",
      "new state: [2 2 2 4 3 1]\n",
      "reward: -0.0\n",
      "epReward so far: -1.0\n",
      "state : [2 2 2 4 3 1]\n",
      "action : 3\n",
      "new state: [2 3 2 3 3 2]\n",
      "reward: -0.0\n",
      "epReward so far: -1.0\n",
      "state : [2 3 2 3 3 2]\n",
      "action : 1\n",
      "new state: [2 2 3 3 2 0]\n",
      "reward: -1.0\n",
      "epReward so far: -2.0\n",
      "final state: [2 2 3 3 2 0]\n",
      "Episode : 30\n",
      "state : [2 2 3 3 2 0]\n",
      "action : 2\n",
      "new state: [2 2 3 3 2 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 2 3 3 2 3]\n",
      "action : 2\n",
      "new state: [2 2 2 4 0 3]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 2 2 4 0 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 2 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [2 2 2 4 2 1]\n",
      "action : 3\n",
      "new state: [2 2 2 4 1 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -30000.0\n",
      "state : [2 2 2 4 1 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 1 1]\n",
      "reward: -1.0\n",
      "epReward so far: -30001.0\n",
      "final state: [2 2 2 4 1 1]\n",
      "Episode : 31\n",
      "state : [2 2 2 4 1 1]\n",
      "action : 3\n",
      "new state: [2 3 2 3 0 1]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [2 3 2 3 0 1]\n",
      "action : 1\n",
      "new state: [2 3 2 3 3 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 3 2 3 3 2]\n",
      "action : 1\n",
      "new state: [2 3 2 3 2 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "state : [2 3 2 3 2 0]\n",
      "action : 1\n",
      "new state: [3 2 2 3 3 0]\n",
      "reward: -1.0\n",
      "epReward so far: -20002.0\n",
      "state : [3 2 2 3 3 0]\n",
      "action : 0\n",
      "new state: [3 2 2 3 0 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -30002.0\n",
      "final state: [3 2 2 3 0 0]\n",
      "Episode : 32\n",
      "state : [3 2 2 3 0 0]\n",
      "action : 0\n",
      "new state: [3 2 2 3 3 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 2 2 3 3 2]\n",
      "action : 0\n",
      "new state: [3 2 2 3 2 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [3 2 2 3 2 0]\n",
      "action : 0\n",
      "new state: [3 2 2 3 0 0]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [3 2 2 3 0 0]\n",
      "action : 0\n",
      "new state: [3 2 2 3 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: -10001.0\n",
      "state : [3 2 2 3 0 2]\n",
      "action : 0\n",
      "new state: [2 2 3 3 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: -10001.0\n",
      "final state: [2 2 3 3 2 2]\n",
      "Episode : 33\n",
      "state : [2 2 3 3 2 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 0 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 2 3 3 0 3]\n",
      "action : 2\n",
      "new state: [2 2 3 3 3 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 2 3 3 3 0]\n",
      "action : 2\n",
      "new state: [3 2 2 3 3 0]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [3 2 2 3 3 0]\n",
      "action : 0\n",
      "new state: [3 2 2 3 3 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "state : [3 2 2 3 3 1]\n",
      "action : 0\n",
      "new state: [2 3 2 3 1 0]\n",
      "reward: -1.0\n",
      "epReward so far: -20002.0\n",
      "final state: [2 3 2 3 1 0]\n",
      "Episode : 34\n",
      "state : [2 3 2 3 1 0]\n",
      "action : 1\n",
      "new state: [2 3 2 3 3 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 3 2 3 3 2]\n",
      "action : 1\n",
      "new state: [2 2 3 3 2 2]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 2 3 3 2 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 2 0]\n",
      "reward: -0.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 2 3 3 2 0]\n",
      "action : 2\n",
      "new state: [3 2 2 3 2 1]\n",
      "reward: -0.0\n",
      "epReward so far: -10001.0\n",
      "state : [3 2 2 3 2 1]\n",
      "action : 0\n",
      "new state: [2 3 2 3 3 1]\n",
      "reward: -1.0\n",
      "epReward so far: -10002.0\n",
      "final state: [2 3 2 3 3 1]\n",
      "Episode : 35\n",
      "state : [2 3 2 3 3 1]\n",
      "action : 1\n",
      "new state: [2 3 2 3 1 1]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [2 3 2 3 1 1]\n",
      "action : 1\n",
      "new state: [2 3 2 3 1 1]\n",
      "reward: -0.0\n",
      "epReward so far: -1.0\n",
      "state : [2 3 2 3 1 1]\n",
      "action : 1\n",
      "new state: [2 3 2 3 3 3]\n",
      "reward: -0.0\n",
      "epReward so far: -1.0\n",
      "state : [2 3 2 3 3 3]\n",
      "action : 1\n",
      "new state: [2 2 2 4 3 2]\n",
      "reward: -1.0\n",
      "epReward so far: -2.0\n",
      "state : [2 2 2 4 3 2]\n",
      "action : 3\n",
      "new state: [2 2 2 4 3 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10002.0\n",
      "final state: [2 2 2 4 3 3]\n",
      "Episode : 36\n",
      "state : [2 2 2 4 3 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 3 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 2 2 4 3 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 1 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 2 2 4 1 0]\n",
      "action : 3\n",
      "new state: [3 2 2 3 2 2]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [3 2 2 3 2 2]\n",
      "action : 0\n",
      "new state: [2 2 3 3 0 0]\n",
      "reward: -1.0\n",
      "epReward so far: -10002.0\n",
      "state : [2 2 3 3 0 0]\n",
      "action : 2\n",
      "new state: [3 2 2 3 2 3]\n",
      "reward: -1.0\n",
      "epReward so far: -10003.0\n",
      "final state: [3 2 2 3 2 3]\n",
      "Episode : 37\n",
      "state : [3 2 2 3 2 3]\n",
      "action : 0\n",
      "new state: [3 2 2 3 2 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [3 2 2 3 2 0]\n",
      "action : 0\n",
      "new state: [3 2 2 3 3 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [3 2 2 3 3 0]\n",
      "action : 0\n",
      "new state: [3 2 2 3 3 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -30000.0\n",
      "state : [3 2 2 3 3 3]\n",
      "action : 0\n",
      "new state: [2 2 2 4 0 2]\n",
      "reward: -1.0\n",
      "epReward so far: -30001.0\n",
      "state : [2 2 2 4 0 2]\n",
      "action : 3\n",
      "new state: [2 2 3 3 3 3]\n",
      "reward: -1.0\n",
      "epReward so far: -30002.0\n",
      "final state: [2 2 3 3 3 3]\n",
      "Episode : 38\n",
      "state : [2 2 3 3 3 3]\n",
      "action : 2\n",
      "new state: [2 2 3 3 3 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 2 3 3 3 0]\n",
      "action : 2\n",
      "new state: [3 2 2 3 0 2]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [3 2 2 3 0 2]\n",
      "action : 0\n",
      "new state: [2 2 3 3 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 2 3 3 0 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 3 3]\n",
      "reward: -1.0\n",
      "epReward so far: -10002.0\n",
      "state : [2 2 3 3 3 3]\n",
      "action : 2\n",
      "new state: [2 2 3 3 0 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -20002.0\n",
      "final state: [2 2 3 3 0 0]\n",
      "Episode : 39\n",
      "state : [2 2 3 3 0 0]\n",
      "action : 2\n",
      "new state: [2 2 3 3 2 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 2 3 3 2 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 2 0]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 2 3 3 2 0]\n",
      "action : 2\n",
      "new state: [3 2 2 3 1 2]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [3 2 2 3 1 2]\n",
      "action : 0\n",
      "new state: [3 2 2 3 1 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [3 2 2 3 1 1]\n",
      "action : 0\n",
      "new state: [2 3 2 3 2 3]\n",
      "reward: -1.0\n",
      "epReward so far: -20001.0\n",
      "final state: [2 3 2 3 2 3]\n",
      "Episode : 40\n",
      "state : [2 3 2 3 2 3]\n",
      "action : 1\n",
      "new state: [2 2 2 4 0 3]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [2 2 2 4 0 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 3 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 2 2 4 3 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 2 2 4 0 2]\n",
      "action : 3\n",
      "new state: [2 2 2 4 1 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "state : [2 2 2 4 1 1]\n",
      "action : 3\n",
      "new state: [2 2 2 4 3 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -30001.0\n",
      "final state: [2 2 2 4 3 2]\n",
      "Episode : 41\n",
      "state : [2 2 2 4 3 2]\n",
      "action : 3\n",
      "new state: [2 2 3 3 3 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 2 3 3 3 3]\n",
      "action : 2\n",
      "new state: [2 2 2 4 1 3]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [2 2 2 4 1 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 0 0]\n",
      "reward: -1.0\n",
      "epReward so far: -2.0\n",
      "state : [2 2 2 4 0 0]\n",
      "action : 3\n",
      "new state: [3 2 2 3 0 2]\n",
      "reward: -1.0\n",
      "epReward so far: -3.0\n",
      "state : [3 2 2 3 0 2]\n",
      "action : 0\n",
      "new state: [3 2 2 3 1 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10003.0\n",
      "final state: [3 2 2 3 1 2]\n",
      "Episode : 42\n",
      "state : [3 2 2 3 1 2]\n",
      "action : 0\n",
      "new state: [3 2 2 3 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [3 2 2 3 0 2]\n",
      "action : 0\n",
      "new state: [2 2 3 3 0 1]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 2 3 3 0 1]\n",
      "action : 2\n",
      "new state: [2 2 3 3 1 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [2 2 3 3 1 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 1 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -30000.0\n",
      "state : [2 2 3 3 1 0]\n",
      "action : 2\n",
      "new state: [2 2 3 3 2 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -40000.0\n",
      "final state: [2 2 3 3 2 2]\n",
      "Episode : 43\n",
      "state : [2 2 3 3 2 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 3 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 2 3 3 3 1]\n",
      "action : 2\n",
      "new state: [2 2 3 3 0 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 2 3 3 0 1]\n",
      "action : 2\n",
      "new state: [2 3 2 3 0 3]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 3 2 3 0 3]\n",
      "action : 1\n",
      "new state: [2 2 2 4 1 2]\n",
      "reward: -1.0\n",
      "epReward so far: -10002.0\n",
      "state : [2 2 2 4 1 2]\n",
      "action : 3\n",
      "new state: [2 2 3 3 0 1]\n",
      "reward: -1.0\n",
      "epReward so far: -10003.0\n",
      "final state: [2 2 3 3 0 1]\n",
      "Episode : 44\n",
      "state : [2 2 3 3 0 1]\n",
      "action : 2\n",
      "new state: [2 3 2 3 3 2]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [2 3 2 3 3 2]\n",
      "action : 1\n",
      "new state: [2 2 3 3 2 0]\n",
      "reward: -1.0\n",
      "epReward so far: -2.0\n",
      "state : [2 2 3 3 2 0]\n",
      "action : 2\n",
      "new state: [2 2 3 3 1 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10002.0\n",
      "state : [2 2 3 3 1 0]\n",
      "action : 2\n",
      "new state: [3 2 2 3 3 1]\n",
      "reward: -1.0\n",
      "epReward so far: -10003.0\n",
      "state : [3 2 2 3 3 1]\n",
      "action : 0\n",
      "new state: [3 2 2 3 1 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20003.0\n",
      "final state: [3 2 2 3 1 1]\n",
      "Episode : 45\n",
      "state : [3 2 2 3 1 1]\n",
      "action : 0\n",
      "new state: [3 2 2 3 2 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [3 2 2 3 2 1]\n",
      "action : 0\n",
      "new state: [2 3 2 3 0 1]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 3 2 3 0 1]\n",
      "action : 1\n",
      "new state: [2 3 2 3 3 1]\n",
      "reward: -1.0\n",
      "epReward so far: -10002.0\n",
      "state : [2 3 2 3 3 1]\n",
      "action : 1\n",
      "new state: [2 3 2 3 2 0]\n",
      "reward: -1.0\n",
      "epReward so far: -10003.0\n",
      "state : [2 3 2 3 2 0]\n",
      "action : 1\n",
      "new state: [2 3 2 3 3 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20003.0\n",
      "final state: [2 3 2 3 3 3]\n",
      "Episode : 46\n",
      "state : [2 3 2 3 3 3]\n",
      "action : 1\n",
      "new state: [2 3 2 3 3 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 3 2 3 3 0]\n",
      "action : 1\n",
      "new state: [2 3 2 3 1 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [2 3 2 3 1 1]\n",
      "action : 1\n",
      "new state: [2 3 2 3 2 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -30000.0\n",
      "state : [2 3 2 3 2 0]\n",
      "action : 1\n",
      "new state: [3 2 2 3 0 0]\n",
      "reward: -1.0\n",
      "epReward so far: -30001.0\n",
      "state : [3 2 2 3 0 0]\n",
      "action : 0\n",
      "new state: [3 2 2 3 0 1]\n",
      "reward: -0.0\n",
      "epReward so far: -30001.0\n",
      "final state: [3 2 2 3 0 1]\n",
      "Episode : 47\n",
      "state : [3 2 2 3 0 1]\n",
      "action : 0\n",
      "new state: [3 2 2 3 1 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [3 2 2 3 1 3]\n",
      "action : 0\n",
      "new state: [3 2 2 3 2 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [3 2 2 3 2 0]\n",
      "action : 0\n",
      "new state: [3 2 2 3 0 1]\n",
      "reward: -1.0\n",
      "epReward so far: -20001.0\n",
      "state : [3 2 2 3 0 1]\n",
      "action : 0\n",
      "new state: [2 3 2 3 2 3]\n",
      "reward: -0.0\n",
      "epReward so far: -20001.0\n",
      "state : [2 3 2 3 2 3]\n",
      "action : 1\n",
      "new state: [2 3 2 3 1 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -30001.0\n",
      "final state: [2 3 2 3 1 0]\n",
      "Episode : 48\n",
      "state : [2 3 2 3 1 0]\n",
      "action : 1\n",
      "new state: [3 2 2 3 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 2 2 3 2 2]\n",
      "action : 0\n",
      "new state: [2 2 3 3 3 3]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [2 2 3 3 3 3]\n",
      "action : 2\n",
      "new state: [2 2 3 3 3 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 2 3 3 3 3]\n",
      "action : 2\n",
      "new state: [2 2 3 3 1 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "state : [2 2 3 3 1 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 3 0]\n",
      "reward: -1.0\n",
      "epReward so far: -20002.0\n",
      "final state: [2 2 3 3 3 0]\n",
      "Episode : 49\n",
      "state : [2 2 3 3 3 0]\n",
      "action : 2\n",
      "new state: [3 2 2 3 0 2]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [3 2 2 3 0 2]\n",
      "action : 0\n",
      "new state: [2 2 3 3 1 0]\n",
      "reward: -0.0\n",
      "epReward so far: -1.0\n",
      "state : [2 2 3 3 1 0]\n",
      "action : 2\n",
      "new state: [2 2 3 3 2 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 2 3 3 2 3]\n",
      "action : 2\n",
      "new state: [2 2 3 3 0 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "state : [2 2 3 3 0 0]\n",
      "action : 2\n",
      "new state: [3 2 2 3 2 2]\n",
      "reward: -1.0\n",
      "epReward so far: -20002.0\n",
      "final state: [3 2 2 3 2 2]\n",
      "Episode : 50\n",
      "state : [3 2 2 3 2 2]\n",
      "action : 0\n",
      "new state: [3 2 2 3 0 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [3 2 2 3 0 1]\n",
      "action : 0\n",
      "new state: [2 3 2 3 2 1]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 3 2 3 2 1]\n",
      "action : 1\n",
      "new state: [2 3 2 3 1 2]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 3 2 3 1 2]\n",
      "action : 1\n",
      "new state: [2 3 2 3 2 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "state : [2 3 2 3 2 1]\n",
      "action : 1\n",
      "new state: [2 3 2 3 1 3]\n",
      "reward: -1.0\n",
      "epReward so far: -20002.0\n",
      "final state: [2 3 2 3 1 3]\n",
      "Episode : 51\n",
      "state : [2 3 2 3 1 3]\n",
      "action : 1\n",
      "new state: [2 2 2 4 1 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 2 2 4 1 2]\n",
      "action : 3\n",
      "new state: [2 2 2 4 2 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 2 2 4 2 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 2 0]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 2 2 4 2 0]\n",
      "action : 3\n",
      "new state: [3 2 2 3 0 2]\n",
      "reward: -1.0\n",
      "epReward so far: -10002.0\n",
      "state : [3 2 2 3 0 2]\n",
      "action : 0\n",
      "new state: [2 2 3 3 0 0]\n",
      "reward: -0.0\n",
      "epReward so far: -10002.0\n",
      "final state: [2 2 3 3 0 0]\n",
      "Episode : 52\n",
      "state : [2 2 3 3 0 0]\n",
      "action : 2\n",
      "new state: [3 2 2 3 0 2]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [3 2 2 3 0 2]\n",
      "action : 0\n",
      "new state: [3 2 2 3 1 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "state : [3 2 2 3 1 1]\n",
      "action : 0\n",
      "new state: [2 3 2 3 2 1]\n",
      "reward: -1.0\n",
      "epReward so far: -10002.0\n",
      "state : [2 3 2 3 2 1]\n",
      "action : 1\n",
      "new state: [2 3 2 3 3 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20002.0\n",
      "state : [2 3 2 3 3 2]\n",
      "action : 1\n",
      "new state: [2 3 2 3 0 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -30002.0\n",
      "final state: [2 3 2 3 0 0]\n",
      "Episode : 53\n",
      "state : [2 3 2 3 0 0]\n",
      "action : 1\n",
      "new state: [3 2 2 3 3 1]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [3 2 2 3 3 1]\n",
      "action : 0\n",
      "new state: [2 3 2 3 2 2]\n",
      "reward: -1.0\n",
      "epReward so far: -2.0\n",
      "state : [2 3 2 3 2 2]\n",
      "action : 1\n",
      "new state: [2 2 3 3 1 1]\n",
      "reward: -1.0\n",
      "epReward so far: -3.0\n",
      "state : [2 2 3 3 1 1]\n",
      "action : 2\n",
      "new state: [2 3 2 3 2 1]\n",
      "reward: -1.0\n",
      "epReward so far: -4.0\n",
      "state : [2 3 2 3 2 1]\n",
      "action : 1\n",
      "new state: [2 3 2 3 2 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10004.0\n",
      "final state: [2 3 2 3 2 1]\n",
      "Episode : 54\n",
      "state : [2 3 2 3 2 1]\n",
      "action : 1\n",
      "new state: [2 3 2 3 3 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 3 2 3 3 0]\n",
      "action : 1\n",
      "new state: [2 3 2 3 1 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [2 3 2 3 1 1]\n",
      "action : 1\n",
      "new state: [2 3 2 3 1 3]\n",
      "reward: -0.0\n",
      "epReward so far: -20000.0\n",
      "state : [2 3 2 3 1 3]\n",
      "action : 1\n",
      "new state: [2 3 2 3 0 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -30000.0\n",
      "state : [2 3 2 3 0 3]\n",
      "action : 1\n",
      "new state: [2 2 2 4 0 2]\n",
      "reward: -1.0\n",
      "epReward so far: -30001.0\n",
      "final state: [2 2 2 4 0 2]\n",
      "Episode : 55\n",
      "state : [2 2 2 4 0 2]\n",
      "action : 3\n",
      "new state: [2 2 2 4 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 2 2 4 0 2]\n",
      "action : 3\n",
      "new state: [2 2 3 3 2 1]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 2 3 3 2 1]\n",
      "action : 2\n",
      "new state: [2 3 2 3 0 1]\n",
      "reward: -0.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 3 2 3 0 1]\n",
      "action : 1\n",
      "new state: [2 3 2 3 0 3]\n",
      "reward: -1.0\n",
      "epReward so far: -10002.0\n",
      "state : [2 3 2 3 0 3]\n",
      "action : 1\n",
      "new state: [2 2 2 4 1 1]\n",
      "reward: -1.0\n",
      "epReward so far: -10003.0\n",
      "final state: [2 2 2 4 1 1]\n",
      "Episode : 56\n",
      "state : [2 2 2 4 1 1]\n",
      "action : 3\n",
      "new state: [2 2 2 4 2 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 2 2 4 2 2]\n",
      "action : 3\n",
      "new state: [2 2 3 3 2 1]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 2 3 3 2 1]\n",
      "action : 2\n",
      "new state: [2 3 2 3 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 3 2 3 0 2]\n",
      "action : 1\n",
      "new state: [2 3 2 3 2 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "state : [2 3 2 3 2 3]\n",
      "action : 1\n",
      "new state: [2 2 2 4 2 0]\n",
      "reward: -1.0\n",
      "epReward so far: -20002.0\n",
      "final state: [2 2 2 4 2 0]\n",
      "Episode : 57\n",
      "state : [2 2 2 4 2 0]\n",
      "action : 3\n",
      "new state: [2 2 2 4 3 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 2 2 4 3 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [2 2 2 4 0 2]\n",
      "action : 3\n",
      "new state: [2 2 3 3 0 2]\n",
      "reward: -1.0\n",
      "epReward so far: -20001.0\n",
      "state : [2 2 3 3 0 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 0 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -30001.0\n",
      "state : [2 2 3 3 0 3]\n",
      "action : 2\n",
      "new state: [2 2 2 4 1 1]\n",
      "reward: -1.0\n",
      "epReward so far: -30002.0\n",
      "final state: [2 2 2 4 1 1]\n",
      "Episode : 58\n",
      "state : [2 2 2 4 1 1]\n",
      "action : 3\n",
      "new state: [2 3 2 3 2 2]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [2 3 2 3 2 2]\n",
      "action : 1\n",
      "new state: [2 2 3 3 2 3]\n",
      "reward: -1.0\n",
      "epReward so far: -2.0\n",
      "state : [2 2 3 3 2 3]\n",
      "action : 2\n",
      "new state: [2 2 3 3 1 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10002.0\n",
      "state : [2 2 3 3 1 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 3 1]\n",
      "reward: -1.0\n",
      "epReward so far: -10003.0\n",
      "state : [2 2 3 3 3 1]\n",
      "action : 2\n",
      "new state: [2 2 3 3 3 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20003.0\n",
      "final state: [2 2 3 3 3 3]\n",
      "Episode : 59\n",
      "state : [2 2 3 3 3 3]\n",
      "action : 2\n",
      "new state: [2 2 3 3 3 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 2 3 3 3 3]\n",
      "action : 2\n",
      "new state: [2 2 2 4 0 2]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 2 2 4 0 2]\n",
      "action : 3\n",
      "new state: [2 2 2 4 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "state : [2 2 2 4 0 2]\n",
      "action : 3\n",
      "new state: [2 2 2 4 2 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -30001.0\n",
      "state : [2 2 2 4 2 1]\n",
      "action : 3\n",
      "new state: [2 2 2 4 2 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -40001.0\n",
      "final state: [2 2 2 4 2 0]\n",
      "Episode : 60\n",
      "state : [2 2 2 4 2 0]\n",
      "action : 3\n",
      "new state: [3 2 2 3 3 1]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [3 2 2 3 3 1]\n",
      "action : 0\n",
      "new state: [3 2 2 3 1 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "state : [3 2 2 3 1 3]\n",
      "action : 0\n",
      "new state: [3 2 2 3 2 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "state : [3 2 2 3 2 3]\n",
      "action : 0\n",
      "new state: [2 2 2 4 2 2]\n",
      "reward: -1.0\n",
      "epReward so far: -20002.0\n",
      "state : [2 2 2 4 2 2]\n",
      "action : 3\n",
      "new state: [2 2 2 4 2 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -30002.0\n",
      "final state: [2 2 2 4 2 2]\n",
      "Episode : 61\n",
      "state : [2 2 2 4 2 2]\n",
      "action : 3\n",
      "new state: [2 2 2 4 1 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 2 2 4 1 1]\n",
      "action : 3\n",
      "new state: [2 3 2 3 2 3]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 3 2 3 2 3]\n",
      "action : 1\n",
      "new state: [2 3 2 3 1 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "state : [2 3 2 3 1 0]\n",
      "action : 1\n",
      "new state: [3 2 2 3 2 1]\n",
      "reward: -0.0\n",
      "epReward so far: -20001.0\n",
      "state : [3 2 2 3 2 1]\n",
      "action : 0\n",
      "new state: [3 2 2 3 0 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -30001.0\n",
      "final state: [3 2 2 3 0 1]\n",
      "Episode : 62\n",
      "state : [3 2 2 3 0 1]\n",
      "action : 0\n",
      "new state: [2 3 2 3 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 3 2 3 2 2]\n",
      "action : 1\n",
      "new state: [2 3 2 3 0 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 3 2 3 0 3]\n",
      "action : 1\n",
      "new state: [2 2 2 4 3 1]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 2 2 4 3 1]\n",
      "action : 3\n",
      "new state: [2 3 2 3 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 3 2 3 0 2]\n",
      "action : 1\n",
      "new state: [2 2 3 3 1 3]\n",
      "reward: -1.0\n",
      "epReward so far: -10002.0\n",
      "final state: [2 2 3 3 1 3]\n",
      "Episode : 63\n",
      "state : [2 2 3 3 1 3]\n",
      "action : 2\n",
      "new state: [2 2 3 3 2 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 2 3 3 2 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 3 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [2 2 3 3 3 3]\n",
      "action : 2\n",
      "new state: [2 2 2 4 3 1]\n",
      "reward: -1.0\n",
      "epReward so far: -20001.0\n",
      "state : [2 2 2 4 3 1]\n",
      "action : 3\n",
      "new state: [2 2 2 4 3 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -30001.0\n",
      "state : [2 2 2 4 3 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 1 1]\n",
      "reward: -0.0\n",
      "epReward so far: -30001.0\n",
      "final state: [2 2 2 4 1 1]\n",
      "Episode : 64\n",
      "state : [2 2 2 4 1 1]\n",
      "action : 3\n",
      "new state: [2 2 2 4 0 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 2 2 4 0 1]\n",
      "action : 3\n",
      "new state: [2 2 2 4 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [2 2 2 4 0 2]\n",
      "action : 3\n",
      "new state: [2 2 2 4 1 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -30000.0\n",
      "state : [2 2 2 4 1 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 3 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -40000.0\n",
      "state : [2 2 2 4 3 2]\n",
      "action : 3\n",
      "new state: [2 2 3 3 2 0]\n",
      "reward: -0.0\n",
      "epReward so far: -40000.0\n",
      "final state: [2 2 3 3 2 0]\n",
      "Episode : 65\n",
      "state : [2 2 3 3 2 0]\n",
      "action : 2\n",
      "new state: [3 2 2 3 2 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 2 2 3 2 0]\n",
      "action : 0\n",
      "new state: [3 2 2 3 3 1]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [3 2 2 3 3 1]\n",
      "action : 0\n",
      "new state: [2 3 2 3 3 1]\n",
      "reward: -1.0\n",
      "epReward so far: -2.0\n",
      "state : [2 3 2 3 3 1]\n",
      "action : 1\n",
      "new state: [2 3 2 3 2 1]\n",
      "reward: -1.0\n",
      "epReward so far: -3.0\n",
      "state : [2 3 2 3 2 1]\n",
      "action : 1\n",
      "new state: [2 3 2 3 3 1]\n",
      "reward: -1.0\n",
      "epReward so far: -4.0\n",
      "final state: [2 3 2 3 3 1]\n",
      "Episode : 66\n",
      "state : [2 3 2 3 3 1]\n",
      "action : 1\n",
      "new state: [2 3 2 3 3 2]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [2 3 2 3 3 2]\n",
      "action : 1\n",
      "new state: [2 3 2 3 1 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 3 2 3 1 1]\n",
      "action : 1\n",
      "new state: [2 3 2 3 1 2]\n",
      "reward: -0.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 3 2 3 1 2]\n",
      "action : 1\n",
      "new state: [2 2 3 3 0 3]\n",
      "reward: -0.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 2 3 3 0 3]\n",
      "action : 2\n",
      "new state: [2 2 2 4 0 0]\n",
      "reward: -1.0\n",
      "epReward so far: -10002.0\n",
      "final state: [2 2 2 4 0 0]\n",
      "Episode : 67\n",
      "state : [2 2 2 4 0 0]\n",
      "action : 3\n",
      "new state: [3 2 2 3 2 3]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [3 2 2 3 2 3]\n",
      "action : 0\n",
      "new state: [3 2 2 3 2 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "state : [3 2 2 3 2 0]\n",
      "action : 0\n",
      "new state: [3 2 2 3 1 2]\n",
      "reward: -1.0\n",
      "epReward so far: -10002.0\n",
      "state : [3 2 2 3 1 2]\n",
      "action : 0\n",
      "new state: [3 2 2 3 1 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20002.0\n",
      "state : [3 2 2 3 1 1]\n",
      "action : 0\n",
      "new state: [2 3 2 3 0 3]\n",
      "reward: -1.0\n",
      "epReward so far: -20003.0\n",
      "final state: [2 3 2 3 0 3]\n",
      "Episode : 68\n",
      "state : [2 3 2 3 0 3]\n",
      "action : 1\n",
      "new state: [2 2 2 4 0 1]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [2 2 2 4 0 1]\n",
      "action : 3\n",
      "new state: [2 3 2 3 0 3]\n",
      "reward: -1.0\n",
      "epReward so far: -2.0\n",
      "state : [2 3 2 3 0 3]\n",
      "action : 1\n",
      "new state: [2 3 2 3 3 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10002.0\n",
      "state : [2 3 2 3 3 1]\n",
      "action : 1\n",
      "new state: [2 3 2 3 2 2]\n",
      "reward: -1.0\n",
      "epReward so far: -10003.0\n",
      "state : [2 3 2 3 2 2]\n",
      "action : 1\n",
      "new state: [2 3 2 3 0 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20003.0\n",
      "final state: [2 3 2 3 0 3]\n",
      "Episode : 69\n",
      "state : [2 3 2 3 0 3]\n",
      "action : 1\n",
      "new state: [2 3 2 3 2 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 3 2 3 2 2]\n",
      "action : 1\n",
      "new state: [2 2 3 3 0 3]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 2 3 3 0 3]\n",
      "action : 2\n",
      "new state: [2 2 3 3 2 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "state : [2 2 3 3 2 3]\n",
      "action : 2\n",
      "new state: [2 2 2 4 1 3]\n",
      "reward: -0.0\n",
      "epReward so far: -20001.0\n",
      "state : [2 2 2 4 1 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 3 2]\n",
      "reward: -1.0\n",
      "epReward so far: -20002.0\n",
      "final state: [2 2 2 4 3 2]\n",
      "Episode : 70\n",
      "state : [2 2 2 4 3 2]\n",
      "action : 3\n",
      "new state: [2 2 3 3 2 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 2 3 3 2 3]\n",
      "action : 2\n",
      "new state: [2 2 2 4 1 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 2 2 4 1 2]\n",
      "action : 3\n",
      "new state: [2 2 2 4 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 2 2 4 0 2]\n",
      "action : 3\n",
      "new state: [2 2 3 3 0 1]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 2 3 3 0 1]\n",
      "action : 2\n",
      "new state: [2 2 3 3 2 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "final state: [2 2 3 3 2 1]\n",
      "Episode : 71\n",
      "state : [2 2 3 3 2 1]\n",
      "action : 2\n",
      "new state: [2 3 2 3 2 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 3 2 3 2 0]\n",
      "action : 1\n",
      "new state: [2 3 2 3 1 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 3 2 3 1 1]\n",
      "action : 1\n",
      "new state: [2 3 2 3 0 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [2 3 2 3 0 3]\n",
      "action : 1\n",
      "new state: [2 2 2 4 1 2]\n",
      "reward: -1.0\n",
      "epReward so far: -20001.0\n",
      "state : [2 2 2 4 1 2]\n",
      "action : 3\n",
      "new state: [2 2 3 3 0 2]\n",
      "reward: -1.0\n",
      "epReward so far: -20002.0\n",
      "final state: [2 2 3 3 0 2]\n",
      "Episode : 72\n",
      "state : [2 2 3 3 0 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 2 3 3 0 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 2 0]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 2 3 3 2 0]\n",
      "action : 2\n",
      "new state: [3 2 2 3 1 2]\n",
      "reward: -0.0\n",
      "epReward so far: -10001.0\n",
      "state : [3 2 2 3 1 2]\n",
      "action : 0\n",
      "new state: [2 2 3 3 3 0]\n",
      "reward: -1.0\n",
      "epReward so far: -10002.0\n",
      "state : [2 2 3 3 3 0]\n",
      "action : 2\n",
      "new state: [3 2 2 3 0 0]\n",
      "reward: -1.0\n",
      "epReward so far: -10003.0\n",
      "final state: [3 2 2 3 0 0]\n",
      "Episode : 73\n",
      "state : [3 2 2 3 0 0]\n",
      "action : 0\n",
      "new state: [3 2 2 3 0 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 2 2 3 0 3]\n",
      "action : 0\n",
      "new state: [3 2 2 3 3 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [3 2 2 3 3 2]\n",
      "action : 0\n",
      "new state: [2 2 3 3 3 0]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 2 3 3 3 0]\n",
      "action : 2\n",
      "new state: [3 2 2 3 0 0]\n",
      "reward: -1.0\n",
      "epReward so far: -10002.0\n",
      "state : [3 2 2 3 0 0]\n",
      "action : 0\n",
      "new state: [3 2 2 3 3 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20002.0\n",
      "final state: [3 2 2 3 3 1]\n",
      "Episode : 74\n",
      "state : [3 2 2 3 3 1]\n",
      "action : 0\n",
      "new state: [2 3 2 3 2 0]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [2 3 2 3 2 0]\n",
      "action : 1\n",
      "new state: [2 3 2 3 1 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 3 2 3 1 3]\n",
      "action : 1\n",
      "new state: [2 2 2 4 0 3]\n",
      "reward: -0.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 2 2 4 0 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 0 1]\n",
      "reward: -1.0\n",
      "epReward so far: -10002.0\n",
      "state : [2 2 2 4 0 1]\n",
      "action : 3\n",
      "new state: [2 2 2 4 2 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20002.0\n",
      "final state: [2 2 2 4 2 3]\n",
      "Episode : 75\n",
      "state : [2 2 2 4 2 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 3 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 2 2 4 3 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [2 2 2 4 0 2]\n",
      "action : 3\n",
      "new state: [2 2 2 4 0 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -30000.0\n",
      "state : [2 2 2 4 0 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 2 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -40000.0\n",
      "state : [2 2 2 4 2 2]\n",
      "action : 3\n",
      "new state: [2 2 2 4 2 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -50000.0\n",
      "final state: [2 2 2 4 2 3]\n",
      "Episode : 76\n",
      "state : [2 2 2 4 2 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 2 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 2 2 4 2 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 3 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [2 2 2 4 3 1]\n",
      "action : 3\n",
      "new state: [2 3 2 3 1 2]\n",
      "reward: -0.0\n",
      "epReward so far: -20000.0\n",
      "state : [2 3 2 3 1 2]\n",
      "action : 1\n",
      "new state: [2 2 3 3 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: -20000.0\n",
      "state : [2 2 3 3 0 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 1 0]\n",
      "reward: -1.0\n",
      "epReward so far: -20001.0\n",
      "final state: [2 2 3 3 1 0]\n",
      "Episode : 77\n",
      "state : [2 2 3 3 1 0]\n",
      "action : 2\n",
      "new state: [3 2 2 3 2 2]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [3 2 2 3 2 2]\n",
      "action : 0\n",
      "new state: [2 2 3 3 2 2]\n",
      "reward: -1.0\n",
      "epReward so far: -2.0\n",
      "state : [2 2 3 3 2 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 1 0]\n",
      "reward: -0.0\n",
      "epReward so far: -2.0\n",
      "state : [2 2 3 3 1 0]\n",
      "action : 2\n",
      "new state: [2 2 3 3 1 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10002.0\n",
      "state : [2 2 3 3 1 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 1 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20002.0\n",
      "final state: [2 2 3 3 1 1]\n",
      "Episode : 78\n",
      "state : [2 2 3 3 1 1]\n",
      "action : 2\n",
      "new state: [2 2 3 3 3 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 2 3 3 3 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 1 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [2 2 3 3 1 1]\n",
      "action : 2\n",
      "new state: [2 3 2 3 3 3]\n",
      "reward: -1.0\n",
      "epReward so far: -20001.0\n",
      "state : [2 3 2 3 3 3]\n",
      "action : 1\n",
      "new state: [2 2 2 4 1 1]\n",
      "reward: -1.0\n",
      "epReward so far: -20002.0\n",
      "state : [2 2 2 4 1 1]\n",
      "action : 3\n",
      "new state: [2 2 2 4 3 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -30002.0\n",
      "final state: [2 2 2 4 3 2]\n",
      "Episode : 79\n",
      "state : [2 2 2 4 3 2]\n",
      "action : 3\n",
      "new state: [2 2 3 3 0 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 2 3 3 0 0]\n",
      "action : 2\n",
      "new state: [2 2 3 3 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 2 3 3 0 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 1 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [2 2 3 3 1 3]\n",
      "action : 2\n",
      "new state: [2 2 3 3 3 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -30000.0\n",
      "state : [2 2 3 3 3 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 2 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -40000.0\n",
      "final state: [2 2 3 3 2 2]\n",
      "Episode : 80\n",
      "state : [2 2 3 3 2 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 3 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 2 3 3 3 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 0 3]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 2 3 3 0 3]\n",
      "action : 2\n",
      "new state: [2 2 3 3 3 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "state : [2 2 3 3 3 3]\n",
      "action : 2\n",
      "new state: [2 2 3 3 2 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -30001.0\n",
      "state : [2 2 3 3 2 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 2 0]\n",
      "reward: -0.0\n",
      "epReward so far: -30001.0\n",
      "final state: [2 2 3 3 2 0]\n",
      "Episode : 81\n",
      "state : [2 2 3 3 2 0]\n",
      "action : 2\n",
      "new state: [3 2 2 3 1 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 2 2 3 1 2]\n",
      "action : 0\n",
      "new state: [2 2 3 3 2 2]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [2 2 3 3 2 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 3 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 2 3 3 3 0]\n",
      "action : 2\n",
      "new state: [2 2 3 3 3 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "state : [2 2 3 3 3 3]\n",
      "action : 2\n",
      "new state: [2 2 2 4 1 0]\n",
      "reward: -1.0\n",
      "epReward so far: -20002.0\n",
      "final state: [2 2 2 4 1 0]\n",
      "Episode : 82\n",
      "state : [2 2 2 4 1 0]\n",
      "action : 3\n",
      "new state: [3 2 2 3 1 1]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [3 2 2 3 1 1]\n",
      "action : 0\n",
      "new state: [2 3 2 3 2 1]\n",
      "reward: -1.0\n",
      "epReward so far: -2.0\n",
      "state : [2 3 2 3 2 1]\n",
      "action : 1\n",
      "new state: [2 3 2 3 3 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10002.0\n",
      "state : [2 3 2 3 3 1]\n",
      "action : 1\n",
      "new state: [2 3 2 3 3 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20002.0\n",
      "state : [2 3 2 3 3 1]\n",
      "action : 1\n",
      "new state: [2 3 2 3 3 1]\n",
      "reward: -1.0\n",
      "epReward so far: -20003.0\n",
      "final state: [2 3 2 3 3 1]\n",
      "Episode : 83\n",
      "state : [2 3 2 3 3 1]\n",
      "action : 1\n",
      "new state: [2 3 2 3 3 1]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [2 3 2 3 3 1]\n",
      "action : 1\n",
      "new state: [2 3 2 3 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 3 2 3 0 2]\n",
      "action : 1\n",
      "new state: [2 2 3 3 1 2]\n",
      "reward: -1.0\n",
      "epReward so far: -10002.0\n",
      "state : [2 2 3 3 1 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20002.0\n",
      "state : [2 2 3 3 0 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 1 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -30002.0\n",
      "final state: [2 2 3 3 1 2]\n",
      "Episode : 84\n",
      "state : [2 2 3 3 1 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 3 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 2 3 3 3 1]\n",
      "action : 2\n",
      "new state: [2 2 3 3 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [2 2 3 3 0 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 2 3]\n",
      "reward: -1.0\n",
      "epReward so far: -20001.0\n",
      "state : [2 2 3 3 2 3]\n",
      "action : 2\n",
      "new state: [2 2 2 4 2 3]\n",
      "reward: -0.0\n",
      "epReward so far: -20001.0\n",
      "state : [2 2 2 4 2 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 3 2]\n",
      "reward: -1.0\n",
      "epReward so far: -20002.0\n",
      "final state: [2 2 2 4 3 2]\n",
      "Episode : 85\n",
      "state : [2 2 2 4 3 2]\n",
      "action : 3\n",
      "new state: [2 2 2 4 2 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 2 2 4 2 2]\n",
      "action : 3\n",
      "new state: [2 2 3 3 0 2]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 2 3 3 0 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 0 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "state : [2 2 3 3 0 1]\n",
      "action : 2\n",
      "new state: [2 3 2 3 2 2]\n",
      "reward: -1.0\n",
      "epReward so far: -20002.0\n",
      "state : [2 3 2 3 2 2]\n",
      "action : 1\n",
      "new state: [2 3 2 3 3 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -30002.0\n",
      "final state: [2 3 2 3 3 0]\n",
      "Episode : 86\n",
      "state : [2 3 2 3 3 0]\n",
      "action : 1\n",
      "new state: [3 2 2 3 0 3]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [3 2 2 3 0 3]\n",
      "action : 0\n",
      "new state: [2 2 2 4 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: -1.0\n",
      "state : [2 2 2 4 2 2]\n",
      "action : 3\n",
      "new state: [2 2 3 3 0 2]\n",
      "reward: -1.0\n",
      "epReward so far: -2.0\n",
      "state : [2 2 3 3 0 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 3 0]\n",
      "reward: -1.0\n",
      "epReward so far: -3.0\n",
      "state : [2 2 3 3 3 0]\n",
      "action : 2\n",
      "new state: [2 2 3 3 0 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10003.0\n",
      "final state: [2 2 3 3 0 3]\n",
      "Episode : 87\n",
      "state : [2 2 3 3 0 3]\n",
      "action : 2\n",
      "new state: [2 2 3 3 2 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 2 3 3 2 3]\n",
      "action : 2\n",
      "new state: [2 2 2 4 3 1]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 2 2 4 3 1]\n",
      "action : 3\n",
      "new state: [2 3 2 3 3 3]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 3 2 3 3 3]\n",
      "action : 1\n",
      "new state: [2 3 2 3 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [2 3 2 3 0 2]\n",
      "action : 1\n",
      "new state: [2 2 3 3 1 0]\n",
      "reward: -1.0\n",
      "epReward so far: -20001.0\n",
      "final state: [2 2 3 3 1 0]\n",
      "Episode : 88\n",
      "state : [2 2 3 3 1 0]\n",
      "action : 2\n",
      "new state: [2 2 3 3 1 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 2 3 3 1 3]\n",
      "action : 2\n",
      "new state: [2 2 2 4 3 2]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 2 2 4 3 2]\n",
      "action : 3\n",
      "new state: [2 2 2 4 1 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "state : [2 2 2 4 1 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 0 3]\n",
      "reward: -1.0\n",
      "epReward so far: -20002.0\n",
      "state : [2 2 2 4 0 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 1 3]\n",
      "reward: -1.0\n",
      "epReward so far: -20003.0\n",
      "final state: [2 2 2 4 1 3]\n",
      "Episode : 89\n",
      "state : [2 2 2 4 1 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 3 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 2 2 4 3 2]\n",
      "action : 3\n",
      "new state: [2 2 3 3 1 2]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 2 3 3 1 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 0 2]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 2 3 3 0 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 3 1]\n",
      "reward: -1.0\n",
      "epReward so far: -10002.0\n",
      "state : [2 2 3 3 3 1]\n",
      "action : 2\n",
      "new state: [2 3 2 3 3 2]\n",
      "reward: -1.0\n",
      "epReward so far: -10003.0\n",
      "final state: [2 3 2 3 3 2]\n",
      "Episode : 90\n",
      "state : [2 3 2 3 3 2]\n",
      "action : 1\n",
      "new state: [2 3 2 3 1 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 3 2 3 1 0]\n",
      "action : 1\n",
      "new state: [3 2 2 3 2 1]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [3 2 2 3 2 1]\n",
      "action : 0\n",
      "new state: [2 3 2 3 2 3]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 3 2 3 2 3]\n",
      "action : 1\n",
      "new state: [2 2 2 4 2 3]\n",
      "reward: -1.0\n",
      "epReward so far: -10002.0\n",
      "state : [2 2 2 4 2 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 3 0]\n",
      "reward: -1.0\n",
      "epReward so far: -10003.0\n",
      "final state: [2 2 2 4 3 0]\n",
      "Episode : 91\n",
      "state : [2 2 2 4 3 0]\n",
      "action : 3\n",
      "new state: [3 2 2 3 3 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 2 2 3 3 0]\n",
      "action : 0\n",
      "new state: [3 2 2 3 0 3]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [3 2 2 3 0 3]\n",
      "action : 0\n",
      "new state: [3 2 2 3 1 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "state : [3 2 2 3 1 0]\n",
      "action : 0\n",
      "new state: [3 2 2 3 3 2]\n",
      "reward: -1.0\n",
      "epReward so far: -10002.0\n",
      "state : [3 2 2 3 3 2]\n",
      "action : 0\n",
      "new state: [3 2 2 3 2 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -20002.0\n",
      "final state: [3 2 2 3 2 0]\n",
      "Episode : 92\n",
      "state : [3 2 2 3 2 0]\n",
      "action : 0\n",
      "new state: [3 2 2 3 3 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [3 2 2 3 3 1]\n",
      "action : 0\n",
      "new state: [3 2 2 3 1 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [3 2 2 3 1 3]\n",
      "action : 0\n",
      "new state: [3 2 2 3 1 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -30000.0\n",
      "state : [3 2 2 3 1 3]\n",
      "action : 0\n",
      "new state: [2 2 2 4 2 0]\n",
      "reward: -1.0\n",
      "epReward so far: -30001.0\n",
      "state : [2 2 2 4 2 0]\n",
      "action : 3\n",
      "new state: [2 2 2 4 2 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -40001.0\n",
      "final state: [2 2 2 4 2 2]\n",
      "Episode : 93\n",
      "state : [2 2 2 4 2 2]\n",
      "action : 3\n",
      "new state: [2 2 2 4 3 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 2 2 4 3 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 3 3]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 2 2 4 3 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 2 3]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 2 2 4 2 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 3 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [2 2 2 4 3 0]\n",
      "action : 3\n",
      "new state: [3 2 2 3 1 2]\n",
      "reward: -0.0\n",
      "epReward so far: -20000.0\n",
      "final state: [3 2 2 3 1 2]\n",
      "Episode : 94\n",
      "state : [3 2 2 3 1 2]\n",
      "action : 0\n",
      "new state: [3 2 2 3 2 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [3 2 2 3 2 0]\n",
      "action : 0\n",
      "new state: [3 2 2 3 0 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [3 2 2 3 0 3]\n",
      "action : 0\n",
      "new state: [2 2 2 4 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: -20000.0\n",
      "state : [2 2 2 4 2 2]\n",
      "action : 3\n",
      "new state: [2 2 2 4 1 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -30000.0\n",
      "state : [2 2 2 4 1 0]\n",
      "action : 3\n",
      "new state: [2 2 2 4 2 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -40000.0\n",
      "final state: [2 2 2 4 2 0]\n",
      "Episode : 95\n",
      "state : [2 2 2 4 2 0]\n",
      "action : 3\n",
      "new state: [2 2 2 4 2 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 2 2 4 2 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 2 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [2 2 2 4 2 1]\n",
      "action : 3\n",
      "new state: [2 2 2 4 0 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -30000.0\n",
      "state : [2 2 2 4 0 0]\n",
      "action : 3\n",
      "new state: [3 2 2 3 3 2]\n",
      "reward: -1.0\n",
      "epReward so far: -30001.0\n",
      "state : [3 2 2 3 3 2]\n",
      "action : 0\n",
      "new state: [3 2 2 3 2 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -40001.0\n",
      "final state: [3 2 2 3 2 1]\n",
      "Episode : 96\n",
      "state : [3 2 2 3 2 1]\n",
      "action : 0\n",
      "new state: [2 3 2 3 3 3]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [2 3 2 3 3 3]\n",
      "action : 1\n",
      "new state: [2 2 2 4 2 0]\n",
      "reward: -1.0\n",
      "epReward so far: -2.0\n",
      "state : [2 2 2 4 2 0]\n",
      "action : 3\n",
      "new state: [2 2 2 4 0 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10002.0\n",
      "state : [2 2 2 4 0 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20002.0\n",
      "state : [2 2 2 4 0 2]\n",
      "action : 3\n",
      "new state: [2 2 3 3 2 0]\n",
      "reward: -1.0\n",
      "epReward so far: -20003.0\n",
      "final state: [2 2 3 3 2 0]\n",
      "Episode : 97\n",
      "state : [2 2 3 3 2 0]\n",
      "action : 2\n",
      "new state: [3 2 2 3 3 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 2 2 3 3 1]\n",
      "action : 0\n",
      "new state: [3 2 2 3 0 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [3 2 2 3 0 3]\n",
      "action : 0\n",
      "new state: [2 2 2 4 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 2 2 4 2 2]\n",
      "action : 3\n",
      "new state: [2 2 2 4 0 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [2 2 2 4 0 1]\n",
      "action : 3\n",
      "new state: [2 2 2 4 0 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -30000.0\n",
      "final state: [2 2 2 4 0 0]\n",
      "Episode : 98\n",
      "state : [2 2 2 4 0 0]\n",
      "action : 3\n",
      "new state: [2 2 2 4 3 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 2 2 4 3 2]\n",
      "action : 3\n",
      "new state: [2 2 2 4 0 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [2 2 2 4 0 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 3 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -30000.0\n",
      "state : [2 2 2 4 3 1]\n",
      "action : 3\n",
      "new state: [2 3 2 3 1 3]\n",
      "reward: -0.0\n",
      "epReward so far: -30000.0\n",
      "state : [2 3 2 3 1 3]\n",
      "action : 1\n",
      "new state: [2 3 2 3 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -40000.0\n",
      "final state: [2 3 2 3 0 2]\n",
      "Episode : 99\n",
      "state : [2 3 2 3 0 2]\n",
      "action : 1\n",
      "new state: [2 2 3 3 3 3]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [2 2 3 3 3 3]\n",
      "action : 2\n",
      "new state: [2 2 2 4 0 0]\n",
      "reward: -1.0\n",
      "epReward so far: -2.0\n",
      "state : [2 2 2 4 0 0]\n",
      "action : 3\n",
      "new state: [3 2 2 3 0 0]\n",
      "reward: -1.0\n",
      "epReward so far: -3.0\n",
      "state : [3 2 2 3 0 0]\n",
      "action : 0\n",
      "new state: [3 2 2 3 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: -3.0\n",
      "state : [3 2 2 3 2 2]\n",
      "action : 0\n",
      "new state: [3 2 2 3 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10003.0\n",
      "final state: [3 2 2 3 0 2]\n",
      "Episode : 0\n",
      "state : [3 2 2 3 0 2]\n",
      "action : 0\n",
      "new state: [2 2 3 3 2 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 2 3 3 2 3]\n",
      "action : 2\n",
      "new state: [2 2 3 3 3 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 2 3 3 3 0]\n",
      "action : 2\n",
      "new state: [2 2 3 3 3 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [2 2 3 3 3 3]\n",
      "action : 2\n",
      "new state: [2 2 3 3 2 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -30000.0\n",
      "state : [2 2 3 3 2 3]\n",
      "action : 2\n",
      "new state: [2 2 2 4 3 0]\n",
      "reward: -0.0\n",
      "epReward so far: -30000.0\n",
      "final state: [2 2 2 4 3 0]\n",
      "Episode : 1\n",
      "state : [2 2 2 4 3 0]\n",
      "action : 3\n",
      "new state: [3 2 2 3 1 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 2 2 3 1 0]\n",
      "action : 0\n",
      "new state: [3 2 2 3 2 0]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [3 2 2 3 2 0]\n",
      "action : 0\n",
      "new state: [3 2 2 3 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "state : [3 2 2 3 0 2]\n",
      "action : 0\n",
      "new state: [2 2 3 3 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 2 3 3 2 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 0 3]\n",
      "reward: -0.0\n",
      "epReward so far: -10001.0\n",
      "final state: [2 2 3 3 0 3]\n",
      "Episode : 2\n",
      "state : [2 2 3 3 0 3]\n",
      "action : 2\n",
      "new state: [2 2 2 4 0 2]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [2 2 2 4 0 2]\n",
      "action : 3\n",
      "new state: [2 2 2 4 1 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 2 2 4 1 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 1 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "state : [2 2 2 4 1 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 2 1]\n",
      "reward: -1.0\n",
      "epReward so far: -20002.0\n",
      "state : [2 2 2 4 2 1]\n",
      "action : 3\n",
      "new state: [2 3 2 3 2 1]\n",
      "reward: -1.0\n",
      "epReward so far: -20003.0\n",
      "final state: [2 3 2 3 2 1]\n",
      "Episode : 3\n",
      "state : [2 3 2 3 2 1]\n",
      "action : 1\n",
      "new state: [2 3 2 3 1 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 3 2 3 1 1]\n",
      "action : 1\n",
      "new state: [2 3 2 3 1 3]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 3 2 3 1 3]\n",
      "action : 1\n",
      "new state: [2 3 2 3 1 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [2 3 2 3 1 3]\n",
      "action : 1\n",
      "new state: [2 2 2 4 2 3]\n",
      "reward: -0.0\n",
      "epReward so far: -20000.0\n",
      "state : [2 2 2 4 2 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 3 3]\n",
      "reward: -1.0\n",
      "epReward so far: -20001.0\n",
      "final state: [2 2 2 4 3 3]\n",
      "Episode : 4\n",
      "state : [2 2 2 4 3 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 2 2 4 0 2]\n",
      "action : 3\n",
      "new state: [2 2 2 4 0 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 2 2 4 0 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 0 2]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 2 2 4 0 2]\n",
      "action : 3\n",
      "new state: [2 2 3 3 2 2]\n",
      "reward: -1.0\n",
      "epReward so far: -10002.0\n",
      "state : [2 2 3 3 2 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 2 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20002.0\n",
      "final state: [2 2 3 3 2 3]\n",
      "Episode : 5\n",
      "state : [2 2 3 3 2 3]\n",
      "action : 2\n",
      "new state: [2 2 2 4 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 2 2 4 0 2]\n",
      "action : 3\n",
      "new state: [2 2 3 3 3 2]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [2 2 3 3 3 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 3 1]\n",
      "reward: -1.0\n",
      "epReward so far: -2.0\n",
      "state : [2 2 3 3 3 1]\n",
      "action : 2\n",
      "new state: [2 3 2 3 1 1]\n",
      "reward: -1.0\n",
      "epReward so far: -3.0\n",
      "state : [2 3 2 3 1 1]\n",
      "action : 1\n",
      "new state: [2 3 2 3 1 0]\n",
      "reward: -0.0\n",
      "epReward so far: -3.0\n",
      "final state: [2 3 2 3 1 0]\n",
      "Episode : 6\n",
      "state : [2 3 2 3 1 0]\n",
      "action : 1\n",
      "new state: [2 3 2 3 1 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 3 2 3 1 3]\n",
      "action : 1\n",
      "new state: [2 2 2 4 0 1]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 2 2 4 0 1]\n",
      "action : 3\n",
      "new state: [2 3 2 3 2 0]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 3 2 3 2 0]\n",
      "action : 1\n",
      "new state: [2 3 2 3 0 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "state : [2 3 2 3 0 0]\n",
      "action : 1\n",
      "new state: [3 2 2 3 1 0]\n",
      "reward: -1.0\n",
      "epReward so far: -20002.0\n",
      "final state: [3 2 2 3 1 0]\n",
      "Episode : 7\n",
      "state : [3 2 2 3 1 0]\n",
      "action : 0\n",
      "new state: [3 2 2 3 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [3 2 2 3 0 2]\n",
      "action : 0\n",
      "new state: [2 2 3 3 0 3]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 2 3 3 0 3]\n",
      "action : 2\n",
      "new state: [2 2 2 4 1 3]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 2 2 4 1 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 3 3]\n",
      "reward: -1.0\n",
      "epReward so far: -10002.0\n",
      "state : [2 2 2 4 3 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 2 3]\n",
      "reward: -0.0\n",
      "epReward so far: -10002.0\n",
      "final state: [2 2 2 4 2 3]\n",
      "Episode : 8\n",
      "state : [2 2 2 4 2 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 2 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 2 2 4 2 2]\n",
      "action : 3\n",
      "new state: [2 2 3 3 2 1]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 2 3 3 2 1]\n",
      "action : 2\n",
      "new state: [2 3 2 3 1 0]\n",
      "reward: -0.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 3 2 3 1 0]\n",
      "action : 1\n",
      "new state: [3 2 2 3 2 3]\n",
      "reward: -0.0\n",
      "epReward so far: -10001.0\n",
      "state : [3 2 2 3 2 3]\n",
      "action : 0\n",
      "new state: [3 2 2 3 3 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "final state: [3 2 2 3 3 0]\n",
      "Episode : 9\n",
      "state : [3 2 2 3 3 0]\n",
      "action : 0\n",
      "new state: [3 2 2 3 0 2]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [3 2 2 3 0 2]\n",
      "action : 0\n",
      "new state: [2 2 3 3 2 0]\n",
      "reward: -0.0\n",
      "epReward so far: -1.0\n",
      "state : [2 2 3 3 2 0]\n",
      "action : 2\n",
      "new state: [3 2 2 3 0 3]\n",
      "reward: -0.0\n",
      "epReward so far: -1.0\n",
      "state : [3 2 2 3 0 3]\n",
      "action : 0\n",
      "new state: [2 2 2 4 1 2]\n",
      "reward: -0.0\n",
      "epReward so far: -1.0\n",
      "state : [2 2 2 4 1 2]\n",
      "action : 3\n",
      "new state: [2 2 3 3 0 0]\n",
      "reward: -1.0\n",
      "epReward so far: -2.0\n",
      "final state: [2 2 3 3 0 0]\n",
      "Episode : 10\n",
      "state : [2 2 3 3 0 0]\n",
      "action : 2\n",
      "new state: [2 2 3 3 0 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 2 3 3 0 1]\n",
      "action : 2\n",
      "new state: [2 2 3 3 0 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [2 2 3 3 0 0]\n",
      "action : 2\n",
      "new state: [3 2 2 3 1 1]\n",
      "reward: -1.0\n",
      "epReward so far: -20001.0\n",
      "state : [3 2 2 3 1 1]\n",
      "action : 0\n",
      "new state: [2 3 2 3 0 0]\n",
      "reward: -1.0\n",
      "epReward so far: -20002.0\n",
      "state : [2 3 2 3 0 0]\n",
      "action : 1\n",
      "new state: [3 2 2 3 2 3]\n",
      "reward: -1.0\n",
      "epReward so far: -20003.0\n",
      "final state: [3 2 2 3 2 3]\n",
      "Episode : 11\n",
      "state : [3 2 2 3 2 3]\n",
      "action : 0\n",
      "new state: [3 2 2 3 2 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [3 2 2 3 2 2]\n",
      "action : 0\n",
      "new state: [2 2 3 3 2 1]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 2 3 3 2 1]\n",
      "action : 2\n",
      "new state: [2 3 2 3 1 2]\n",
      "reward: -0.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 3 2 3 1 2]\n",
      "action : 1\n",
      "new state: [2 2 3 3 0 1]\n",
      "reward: -0.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 2 3 3 0 1]\n",
      "action : 2\n",
      "new state: [2 3 2 3 2 2]\n",
      "reward: -1.0\n",
      "epReward so far: -10002.0\n",
      "final state: [2 3 2 3 2 2]\n",
      "Episode : 12\n",
      "state : [2 3 2 3 2 2]\n",
      "action : 1\n",
      "new state: [2 3 2 3 3 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 3 2 3 3 0]\n",
      "action : 1\n",
      "new state: [3 2 2 3 0 1]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [3 2 2 3 0 1]\n",
      "action : 0\n",
      "new state: [2 3 2 3 3 2]\n",
      "reward: -0.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 3 2 3 3 2]\n",
      "action : 1\n",
      "new state: [2 2 3 3 1 0]\n",
      "reward: -1.0\n",
      "epReward so far: -10002.0\n",
      "state : [2 2 3 3 1 0]\n",
      "action : 2\n",
      "new state: [3 2 2 3 1 0]\n",
      "reward: -1.0\n",
      "epReward so far: -10003.0\n",
      "final state: [3 2 2 3 1 0]\n",
      "Episode : 13\n",
      "state : [3 2 2 3 1 0]\n",
      "action : 0\n",
      "new state: [3 2 2 3 1 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [3 2 2 3 1 1]\n",
      "action : 0\n",
      "new state: [3 2 2 3 0 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [3 2 2 3 0 3]\n",
      "action : 0\n",
      "new state: [2 2 2 4 1 1]\n",
      "reward: -0.0\n",
      "epReward so far: -20000.0\n",
      "state : [2 2 2 4 1 1]\n",
      "action : 3\n",
      "new state: [2 3 2 3 1 2]\n",
      "reward: -1.0\n",
      "epReward so far: -20001.0\n",
      "state : [2 3 2 3 1 2]\n",
      "action : 1\n",
      "new state: [2 3 2 3 0 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -30001.0\n",
      "final state: [2 3 2 3 0 1]\n",
      "Episode : 14\n",
      "state : [2 3 2 3 0 1]\n",
      "action : 1\n",
      "new state: [2 3 2 3 3 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 3 2 3 3 0]\n",
      "action : 1\n",
      "new state: [2 3 2 3 2 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [2 3 2 3 2 3]\n",
      "action : 1\n",
      "new state: [2 3 2 3 2 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -30000.0\n",
      "state : [2 3 2 3 2 2]\n",
      "action : 1\n",
      "new state: [2 2 3 3 0 0]\n",
      "reward: -1.0\n",
      "epReward so far: -30001.0\n",
      "state : [2 2 3 3 0 0]\n",
      "action : 2\n",
      "new state: [3 2 2 3 1 3]\n",
      "reward: -1.0\n",
      "epReward so far: -30002.0\n",
      "final state: [3 2 2 3 1 3]\n",
      "Episode : 15\n",
      "state : [3 2 2 3 1 3]\n",
      "action : 0\n",
      "new state: [2 2 2 4 2 1]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [2 2 2 4 2 1]\n",
      "action : 3\n",
      "new state: [2 3 2 3 3 0]\n",
      "reward: -1.0\n",
      "epReward so far: -2.0\n",
      "state : [2 3 2 3 3 0]\n",
      "action : 1\n",
      "new state: [2 3 2 3 3 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10002.0\n",
      "state : [2 3 2 3 3 3]\n",
      "action : 1\n",
      "new state: [2 3 2 3 2 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20002.0\n",
      "state : [2 3 2 3 2 2]\n",
      "action : 1\n",
      "new state: [2 2 3 3 2 1]\n",
      "reward: -1.0\n",
      "epReward so far: -20003.0\n",
      "final state: [2 2 3 3 2 1]\n",
      "Episode : 16\n",
      "state : [2 2 3 3 2 1]\n",
      "action : 2\n",
      "new state: [2 3 2 3 1 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 3 2 3 1 1]\n",
      "action : 1\n",
      "new state: [2 3 2 3 3 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 3 2 3 3 3]\n",
      "action : 1\n",
      "new state: [2 3 2 3 3 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 3 2 3 3 0]\n",
      "action : 1\n",
      "new state: [3 2 2 3 3 3]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [3 2 2 3 3 3]\n",
      "action : 0\n",
      "new state: [3 2 2 3 2 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "final state: [3 2 2 3 2 2]\n",
      "Episode : 17\n",
      "state : [3 2 2 3 2 2]\n",
      "action : 0\n",
      "new state: [3 2 2 3 1 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [3 2 2 3 1 2]\n",
      "action : 0\n",
      "new state: [3 2 2 3 1 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [3 2 2 3 1 1]\n",
      "action : 0\n",
      "new state: [2 3 2 3 1 0]\n",
      "reward: -1.0\n",
      "epReward so far: -20001.0\n",
      "state : [2 3 2 3 1 0]\n",
      "action : 1\n",
      "new state: [3 2 2 3 3 3]\n",
      "reward: -0.0\n",
      "epReward so far: -20001.0\n",
      "state : [3 2 2 3 3 3]\n",
      "action : 0\n",
      "new state: [3 2 2 3 3 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -30001.0\n",
      "final state: [3 2 2 3 3 3]\n",
      "Episode : 18\n",
      "state : [3 2 2 3 3 3]\n",
      "action : 0\n",
      "new state: [2 2 2 4 0 1]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [2 2 2 4 0 1]\n",
      "action : 3\n",
      "new state: [2 2 2 4 3 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 2 2 4 3 0]\n",
      "action : 3\n",
      "new state: [2 2 2 4 2 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "state : [2 2 2 4 2 0]\n",
      "action : 3\n",
      "new state: [3 2 2 3 1 2]\n",
      "reward: -1.0\n",
      "epReward so far: -20002.0\n",
      "state : [3 2 2 3 1 2]\n",
      "action : 0\n",
      "new state: [3 2 2 3 2 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -30002.0\n",
      "final state: [3 2 2 3 2 1]\n",
      "Episode : 19\n",
      "state : [3 2 2 3 2 1]\n",
      "action : 0\n",
      "new state: [2 3 2 3 2 1]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [2 3 2 3 2 1]\n",
      "action : 1\n",
      "new state: [2 3 2 3 1 3]\n",
      "reward: -1.0\n",
      "epReward so far: -2.0\n",
      "state : [2 3 2 3 1 3]\n",
      "action : 1\n",
      "new state: [2 3 2 3 2 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10002.0\n",
      "state : [2 3 2 3 2 1]\n",
      "action : 1\n",
      "new state: [2 3 2 3 2 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20002.0\n",
      "state : [2 3 2 3 2 3]\n",
      "action : 1\n",
      "new state: [2 2 2 4 1 3]\n",
      "reward: -1.0\n",
      "epReward so far: -20003.0\n",
      "final state: [2 2 2 4 1 3]\n",
      "Episode : 20\n",
      "state : [2 2 2 4 1 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 2 1]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [2 2 2 4 2 1]\n",
      "action : 3\n",
      "new state: [2 3 2 3 0 1]\n",
      "reward: -1.0\n",
      "epReward so far: -2.0\n",
      "state : [2 3 2 3 0 1]\n",
      "action : 1\n",
      "new state: [2 3 2 3 1 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10002.0\n",
      "state : [2 3 2 3 1 2]\n",
      "action : 1\n",
      "new state: [2 2 3 3 2 1]\n",
      "reward: -0.0\n",
      "epReward so far: -10002.0\n",
      "state : [2 2 3 3 2 1]\n",
      "action : 2\n",
      "new state: [2 3 2 3 3 0]\n",
      "reward: -0.0\n",
      "epReward so far: -10002.0\n",
      "final state: [2 3 2 3 3 0]\n",
      "Episode : 21\n",
      "state : [2 3 2 3 3 0]\n",
      "action : 1\n",
      "new state: [3 2 2 3 0 2]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [3 2 2 3 0 2]\n",
      "action : 0\n",
      "new state: [2 2 3 3 0 3]\n",
      "reward: -0.0\n",
      "epReward so far: -1.0\n",
      "state : [2 2 3 3 0 3]\n",
      "action : 2\n",
      "new state: [2 2 3 3 1 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 2 3 3 1 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 3 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "state : [2 2 3 3 3 0]\n",
      "action : 2\n",
      "new state: [3 2 2 3 3 3]\n",
      "reward: -1.0\n",
      "epReward so far: -20002.0\n",
      "final state: [3 2 2 3 3 3]\n",
      "Episode : 22\n",
      "state : [3 2 2 3 3 3]\n",
      "action : 0\n",
      "new state: [3 2 2 3 1 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [3 2 2 3 1 3]\n",
      "action : 0\n",
      "new state: [2 2 2 4 0 0]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 2 2 4 0 0]\n",
      "action : 3\n",
      "new state: [2 2 2 4 3 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "state : [2 2 2 4 3 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 0 3]\n",
      "reward: -0.0\n",
      "epReward so far: -20001.0\n",
      "state : [2 2 2 4 0 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 2 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -30001.0\n",
      "final state: [2 2 2 4 2 3]\n",
      "Episode : 23\n",
      "state : [2 2 2 4 2 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 0 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 2 2 4 0 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [2 2 2 4 0 2]\n",
      "action : 3\n",
      "new state: [2 2 2 4 0 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -30000.0\n",
      "state : [2 2 2 4 0 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 0 0]\n",
      "reward: -1.0\n",
      "epReward so far: -30001.0\n",
      "state : [2 2 2 4 0 0]\n",
      "action : 3\n",
      "new state: [2 2 2 4 0 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -40001.0\n",
      "final state: [2 2 2 4 0 0]\n",
      "Episode : 24\n",
      "state : [2 2 2 4 0 0]\n",
      "action : 3\n",
      "new state: [2 2 2 4 3 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 2 2 4 3 0]\n",
      "action : 3\n",
      "new state: [3 2 2 3 3 2]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [3 2 2 3 3 2]\n",
      "action : 0\n",
      "new state: [2 2 3 3 0 3]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 2 3 3 0 3]\n",
      "action : 2\n",
      "new state: [2 2 3 3 3 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "state : [2 2 3 3 3 1]\n",
      "action : 2\n",
      "new state: [2 2 3 3 1 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -30001.0\n",
      "final state: [2 2 3 3 1 2]\n",
      "Episode : 25\n",
      "state : [2 2 3 3 1 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 3 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 2 3 3 3 1]\n",
      "action : 2\n",
      "new state: [2 2 3 3 3 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [2 2 3 3 3 1]\n",
      "action : 2\n",
      "new state: [2 2 3 3 1 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -30000.0\n",
      "state : [2 2 3 3 1 1]\n",
      "action : 2\n",
      "new state: [2 2 3 3 1 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -40000.0\n",
      "state : [2 2 3 3 1 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 3 2]\n",
      "reward: -1.0\n",
      "epReward so far: -40001.0\n",
      "final state: [2 2 3 3 3 2]\n",
      "Episode : 26\n",
      "state : [2 2 3 3 3 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 2 3 3 0 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 0 2]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 2 3 3 0 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 3 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "state : [2 2 3 3 3 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 3 2]\n",
      "reward: -1.0\n",
      "epReward so far: -20002.0\n",
      "state : [2 2 3 3 3 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 0 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -30002.0\n",
      "final state: [2 2 3 3 0 1]\n",
      "Episode : 27\n",
      "state : [2 2 3 3 0 1]\n",
      "action : 2\n",
      "new state: [2 2 3 3 0 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 2 3 3 0 3]\n",
      "action : 2\n",
      "new state: [2 2 3 3 1 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [2 2 3 3 1 1]\n",
      "action : 2\n",
      "new state: [2 2 3 3 3 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -30000.0\n",
      "state : [2 2 3 3 3 3]\n",
      "action : 2\n",
      "new state: [2 2 3 3 0 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -40000.0\n",
      "state : [2 2 3 3 0 1]\n",
      "action : 2\n",
      "new state: [2 2 3 3 2 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -50000.0\n",
      "final state: [2 2 3 3 2 3]\n",
      "Episode : 28\n",
      "state : [2 2 3 3 2 3]\n",
      "action : 2\n",
      "new state: [2 2 3 3 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 2 3 3 0 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 0 2]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 2 3 3 0 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 1 0]\n",
      "reward: -1.0\n",
      "epReward so far: -10002.0\n",
      "state : [2 2 3 3 1 0]\n",
      "action : 2\n",
      "new state: [2 2 3 3 2 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20002.0\n",
      "state : [2 2 3 3 2 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 3 3]\n",
      "reward: -0.0\n",
      "epReward so far: -20002.0\n",
      "final state: [2 2 3 3 3 3]\n",
      "Episode : 29\n",
      "state : [2 2 3 3 3 3]\n",
      "action : 2\n",
      "new state: [2 2 2 4 3 0]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [2 2 2 4 3 0]\n",
      "action : 3\n",
      "new state: [3 2 2 3 1 2]\n",
      "reward: -0.0\n",
      "epReward so far: -1.0\n",
      "state : [3 2 2 3 1 2]\n",
      "action : 0\n",
      "new state: [2 2 3 3 3 3]\n",
      "reward: -1.0\n",
      "epReward so far: -2.0\n",
      "state : [2 2 3 3 3 3]\n",
      "action : 2\n",
      "new state: [2 2 3 3 1 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10002.0\n",
      "state : [2 2 3 3 1 0]\n",
      "action : 2\n",
      "new state: [3 2 2 3 3 3]\n",
      "reward: -1.0\n",
      "epReward so far: -10003.0\n",
      "final state: [3 2 2 3 3 3]\n",
      "Episode : 30\n",
      "state : [3 2 2 3 3 3]\n",
      "action : 0\n",
      "new state: [2 2 2 4 2 2]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [2 2 2 4 2 2]\n",
      "action : 3\n",
      "new state: [2 2 3 3 2 1]\n",
      "reward: -1.0\n",
      "epReward so far: -2.0\n",
      "state : [2 2 3 3 2 1]\n",
      "action : 2\n",
      "new state: [2 3 2 3 2 0]\n",
      "reward: -0.0\n",
      "epReward so far: -2.0\n",
      "state : [2 3 2 3 2 0]\n",
      "action : 1\n",
      "new state: [3 2 2 3 2 2]\n",
      "reward: -1.0\n",
      "epReward so far: -3.0\n",
      "state : [3 2 2 3 2 2]\n",
      "action : 0\n",
      "new state: [3 2 2 3 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10003.0\n",
      "final state: [3 2 2 3 0 2]\n",
      "Episode : 31\n",
      "state : [3 2 2 3 0 2]\n",
      "action : 0\n",
      "new state: [2 2 3 3 2 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 2 3 3 2 0]\n",
      "action : 2\n",
      "new state: [3 2 2 3 3 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 2 2 3 3 0]\n",
      "action : 0\n",
      "new state: [3 2 2 3 3 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [3 2 2 3 3 1]\n",
      "action : 0\n",
      "new state: [2 3 2 3 2 3]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 3 2 3 2 3]\n",
      "action : 1\n",
      "new state: [2 2 2 4 2 3]\n",
      "reward: -1.0\n",
      "epReward so far: -10002.0\n",
      "final state: [2 2 2 4 2 3]\n",
      "Episode : 32\n",
      "state : [2 2 2 4 2 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 1 3]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [2 2 2 4 1 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 0 3]\n",
      "reward: -1.0\n",
      "epReward so far: -2.0\n",
      "state : [2 2 2 4 0 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 2 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10002.0\n",
      "state : [2 2 2 4 2 1]\n",
      "action : 3\n",
      "new state: [2 3 2 3 3 0]\n",
      "reward: -1.0\n",
      "epReward so far: -10003.0\n",
      "state : [2 3 2 3 3 0]\n",
      "action : 1\n",
      "new state: [3 2 2 3 1 0]\n",
      "reward: -1.0\n",
      "epReward so far: -10004.0\n",
      "final state: [3 2 2 3 1 0]\n",
      "Episode : 33\n",
      "state : [3 2 2 3 1 0]\n",
      "action : 0\n",
      "new state: [3 2 2 3 1 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [3 2 2 3 1 0]\n",
      "action : 0\n",
      "new state: [3 2 2 3 3 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [3 2 2 3 3 0]\n",
      "action : 0\n",
      "new state: [3 2 2 3 3 1]\n",
      "reward: -1.0\n",
      "epReward so far: -20001.0\n",
      "state : [3 2 2 3 3 1]\n",
      "action : 0\n",
      "new state: [2 3 2 3 3 2]\n",
      "reward: -1.0\n",
      "epReward so far: -20002.0\n",
      "state : [2 3 2 3 3 2]\n",
      "action : 1\n",
      "new state: [2 3 2 3 0 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -30002.0\n",
      "final state: [2 3 2 3 0 1]\n",
      "Episode : 34\n",
      "state : [2 3 2 3 0 1]\n",
      "action : 1\n",
      "new state: [2 3 2 3 3 2]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [2 3 2 3 3 2]\n",
      "action : 1\n",
      "new state: [2 2 3 3 0 3]\n",
      "reward: -1.0\n",
      "epReward so far: -2.0\n",
      "state : [2 2 3 3 0 3]\n",
      "action : 2\n",
      "new state: [2 2 2 4 2 1]\n",
      "reward: -1.0\n",
      "epReward so far: -3.0\n",
      "state : [2 2 2 4 2 1]\n",
      "action : 3\n",
      "new state: [2 3 2 3 0 0]\n",
      "reward: -1.0\n",
      "epReward so far: -4.0\n",
      "state : [2 3 2 3 0 0]\n",
      "action : 1\n",
      "new state: [2 3 2 3 3 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10004.0\n",
      "final state: [2 3 2 3 3 0]\n",
      "Episode : 35\n",
      "state : [2 3 2 3 3 0]\n",
      "action : 1\n",
      "new state: [2 3 2 3 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 3 2 3 0 2]\n",
      "action : 1\n",
      "new state: [2 2 3 3 0 1]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 2 3 3 0 1]\n",
      "action : 2\n",
      "new state: [2 2 3 3 3 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "state : [2 2 3 3 3 1]\n",
      "action : 2\n",
      "new state: [2 3 2 3 0 0]\n",
      "reward: -1.0\n",
      "epReward so far: -20002.0\n",
      "state : [2 3 2 3 0 0]\n",
      "action : 1\n",
      "new state: [3 2 2 3 1 0]\n",
      "reward: -1.0\n",
      "epReward so far: -20003.0\n",
      "final state: [3 2 2 3 1 0]\n",
      "Episode : 36\n",
      "state : [3 2 2 3 1 0]\n",
      "action : 0\n",
      "new state: [3 2 2 3 1 3]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [3 2 2 3 1 3]\n",
      "action : 0\n",
      "new state: [3 2 2 3 0 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "state : [3 2 2 3 0 1]\n",
      "action : 0\n",
      "new state: [3 2 2 3 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "state : [3 2 2 3 0 2]\n",
      "action : 0\n",
      "new state: [2 2 3 3 0 1]\n",
      "reward: -0.0\n",
      "epReward so far: -20001.0\n",
      "state : [2 2 3 3 0 1]\n",
      "action : 2\n",
      "new state: [2 3 2 3 2 0]\n",
      "reward: -1.0\n",
      "epReward so far: -20002.0\n",
      "final state: [2 3 2 3 2 0]\n",
      "Episode : 37\n",
      "state : [2 3 2 3 2 0]\n",
      "action : 1\n",
      "new state: [3 2 2 3 0 1]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [3 2 2 3 0 1]\n",
      "action : 0\n",
      "new state: [2 3 2 3 3 2]\n",
      "reward: -0.0\n",
      "epReward so far: -1.0\n",
      "state : [2 3 2 3 3 2]\n",
      "action : 1\n",
      "new state: [2 3 2 3 0 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 3 2 3 0 1]\n",
      "action : 1\n",
      "new state: [2 3 2 3 3 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "state : [2 3 2 3 3 2]\n",
      "action : 1\n",
      "new state: [2 2 3 3 2 0]\n",
      "reward: -1.0\n",
      "epReward so far: -20002.0\n",
      "final state: [2 2 3 3 2 0]\n",
      "Episode : 38\n",
      "state : [2 2 3 3 2 0]\n",
      "action : 2\n",
      "new state: [3 2 2 3 1 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 2 2 3 1 0]\n",
      "action : 0\n",
      "new state: [3 2 2 3 0 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [3 2 2 3 0 1]\n",
      "action : 0\n",
      "new state: [2 3 2 3 3 2]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 3 2 3 3 2]\n",
      "action : 1\n",
      "new state: [2 3 2 3 1 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [2 3 2 3 1 0]\n",
      "action : 1\n",
      "new state: [3 2 2 3 1 2]\n",
      "reward: -0.0\n",
      "epReward so far: -20000.0\n",
      "final state: [3 2 2 3 1 2]\n",
      "Episode : 39\n",
      "state : [3 2 2 3 1 2]\n",
      "action : 0\n",
      "new state: [2 2 3 3 1 2]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [2 2 3 3 1 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 3 3]\n",
      "reward: -1.0\n",
      "epReward so far: -2.0\n",
      "state : [2 2 3 3 3 3]\n",
      "action : 2\n",
      "new state: [2 2 3 3 2 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10002.0\n",
      "state : [2 2 3 3 2 0]\n",
      "action : 2\n",
      "new state: [2 2 3 3 3 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20002.0\n",
      "state : [2 2 3 3 3 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 2 3]\n",
      "reward: -1.0\n",
      "epReward so far: -20003.0\n",
      "final state: [2 2 3 3 2 3]\n",
      "Episode : 40\n",
      "state : [2 2 3 3 2 3]\n",
      "action : 2\n",
      "new state: [2 2 2 4 3 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 2 2 4 3 2]\n",
      "action : 3\n",
      "new state: [2 2 3 3 2 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 2 3 3 2 0]\n",
      "action : 2\n",
      "new state: [3 2 2 3 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 2 2 3 2 2]\n",
      "action : 0\n",
      "new state: [2 2 3 3 1 0]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [2 2 3 3 1 0]\n",
      "action : 2\n",
      "new state: [2 2 3 3 1 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "final state: [2 2 3 3 1 0]\n",
      "Episode : 41\n",
      "state : [2 2 3 3 1 0]\n",
      "action : 2\n",
      "new state: [3 2 2 3 2 1]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [3 2 2 3 2 1]\n",
      "action : 0\n",
      "new state: [2 3 2 3 0 3]\n",
      "reward: -1.0\n",
      "epReward so far: -2.0\n",
      "state : [2 3 2 3 0 3]\n",
      "action : 1\n",
      "new state: [2 3 2 3 3 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10002.0\n",
      "state : [2 3 2 3 3 1]\n",
      "action : 1\n",
      "new state: [2 3 2 3 2 2]\n",
      "reward: -1.0\n",
      "epReward so far: -10003.0\n",
      "state : [2 3 2 3 2 2]\n",
      "action : 1\n",
      "new state: [2 3 2 3 2 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20003.0\n",
      "final state: [2 3 2 3 2 3]\n",
      "Episode : 42\n",
      "state : [2 3 2 3 2 3]\n",
      "action : 1\n",
      "new state: [2 3 2 3 3 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 3 2 3 3 2]\n",
      "action : 1\n",
      "new state: [2 3 2 3 3 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [2 3 2 3 3 1]\n",
      "action : 1\n",
      "new state: [2 3 2 3 3 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -30000.0\n",
      "state : [2 3 2 3 3 1]\n",
      "action : 1\n",
      "new state: [2 3 2 3 1 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -40000.0\n",
      "state : [2 3 2 3 1 0]\n",
      "action : 1\n",
      "new state: [3 2 2 3 2 3]\n",
      "reward: -0.0\n",
      "epReward so far: -40000.0\n",
      "final state: [3 2 2 3 2 3]\n",
      "Episode : 43\n",
      "state : [3 2 2 3 2 3]\n",
      "action : 0\n",
      "new state: [3 2 2 3 0 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [3 2 2 3 0 0]\n",
      "action : 0\n",
      "new state: [3 2 2 3 1 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [3 2 2 3 1 1]\n",
      "action : 0\n",
      "new state: [2 3 2 3 3 0]\n",
      "reward: -1.0\n",
      "epReward so far: -20001.0\n",
      "state : [2 3 2 3 3 0]\n",
      "action : 1\n",
      "new state: [2 3 2 3 0 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -30001.0\n",
      "state : [2 3 2 3 0 1]\n",
      "action : 1\n",
      "new state: [2 3 2 3 0 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -40001.0\n",
      "final state: [2 3 2 3 0 0]\n",
      "Episode : 44\n",
      "state : [2 3 2 3 0 0]\n",
      "action : 1\n",
      "new state: [2 3 2 3 3 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 3 2 3 3 0]\n",
      "action : 1\n",
      "new state: [3 2 2 3 3 2]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [3 2 2 3 3 2]\n",
      "action : 0\n",
      "new state: [3 2 2 3 1 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "state : [3 2 2 3 1 1]\n",
      "action : 0\n",
      "new state: [2 3 2 3 3 0]\n",
      "reward: -1.0\n",
      "epReward so far: -20002.0\n",
      "state : [2 3 2 3 3 0]\n",
      "action : 1\n",
      "new state: [2 3 2 3 0 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -30002.0\n",
      "final state: [2 3 2 3 0 1]\n",
      "Episode : 45\n",
      "state : [2 3 2 3 0 1]\n",
      "action : 1\n",
      "new state: [2 3 2 3 3 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 3 2 3 3 2]\n",
      "action : 1\n",
      "new state: [2 3 2 3 2 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [2 3 2 3 2 0]\n",
      "action : 1\n",
      "new state: [3 2 2 3 1 1]\n",
      "reward: -1.0\n",
      "epReward so far: -20001.0\n",
      "state : [3 2 2 3 1 1]\n",
      "action : 0\n",
      "new state: [2 3 2 3 3 1]\n",
      "reward: -1.0\n",
      "epReward so far: -20002.0\n",
      "state : [2 3 2 3 3 1]\n",
      "action : 1\n",
      "new state: [2 3 2 3 1 0]\n",
      "reward: -1.0\n",
      "epReward so far: -20003.0\n",
      "final state: [2 3 2 3 1 0]\n",
      "Episode : 46\n",
      "state : [2 3 2 3 1 0]\n",
      "action : 1\n",
      "new state: [3 2 2 3 1 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 2 2 3 1 2]\n",
      "action : 0\n",
      "new state: [2 2 3 3 1 1]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [2 2 3 3 1 1]\n",
      "action : 2\n",
      "new state: [2 2 3 3 3 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 2 3 3 3 1]\n",
      "action : 2\n",
      "new state: [2 2 3 3 2 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "state : [2 2 3 3 2 3]\n",
      "action : 2\n",
      "new state: [2 2 2 4 2 1]\n",
      "reward: -0.0\n",
      "epReward so far: -20001.0\n",
      "final state: [2 2 2 4 2 1]\n",
      "Episode : 47\n",
      "state : [2 2 2 4 2 1]\n",
      "action : 3\n",
      "new state: [2 2 2 4 2 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 2 2 4 2 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 0 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [2 2 2 4 0 1]\n",
      "action : 3\n",
      "new state: [2 3 2 3 0 3]\n",
      "reward: -1.0\n",
      "epReward so far: -20001.0\n",
      "state : [2 3 2 3 0 3]\n",
      "action : 1\n",
      "new state: [2 2 2 4 1 0]\n",
      "reward: -1.0\n",
      "epReward so far: -20002.0\n",
      "state : [2 2 2 4 1 0]\n",
      "action : 3\n",
      "new state: [3 2 2 3 1 1]\n",
      "reward: -1.0\n",
      "epReward so far: -20003.0\n",
      "final state: [3 2 2 3 1 1]\n",
      "Episode : 48\n",
      "state : [3 2 2 3 1 1]\n",
      "action : 0\n",
      "new state: [3 2 2 3 2 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [3 2 2 3 2 2]\n",
      "action : 0\n",
      "new state: [2 2 3 3 2 3]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 2 3 3 2 3]\n",
      "action : 2\n",
      "new state: [2 2 3 3 2 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "state : [2 2 3 3 2 3]\n",
      "action : 2\n",
      "new state: [2 2 3 3 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -30001.0\n",
      "state : [2 2 3 3 0 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -40001.0\n",
      "final state: [2 2 3 3 0 2]\n",
      "Episode : 49\n",
      "state : [2 2 3 3 0 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 2 0]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [2 2 3 3 2 0]\n",
      "action : 2\n",
      "new state: [3 2 2 3 1 2]\n",
      "reward: -0.0\n",
      "epReward so far: -1.0\n",
      "state : [3 2 2 3 1 2]\n",
      "action : 0\n",
      "new state: [2 2 3 3 2 1]\n",
      "reward: -1.0\n",
      "epReward so far: -2.0\n",
      "state : [2 2 3 3 2 1]\n",
      "action : 2\n",
      "new state: [2 2 3 3 3 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10002.0\n",
      "state : [2 2 3 3 3 0]\n",
      "action : 2\n",
      "new state: [2 2 3 3 0 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -20002.0\n",
      "final state: [2 2 3 3 0 0]\n",
      "Episode : 50\n",
      "state : [2 2 3 3 0 0]\n",
      "action : 2\n",
      "new state: [2 2 3 3 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 2 3 3 0 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 2 0]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 2 3 3 2 0]\n",
      "action : 2\n",
      "new state: [3 2 2 3 0 0]\n",
      "reward: -0.0\n",
      "epReward so far: -10001.0\n",
      "state : [3 2 2 3 0 0]\n",
      "action : 0\n",
      "new state: [3 2 2 3 0 0]\n",
      "reward: -0.0\n",
      "epReward so far: -10001.0\n",
      "state : [3 2 2 3 0 0]\n",
      "action : 0\n",
      "new state: [3 2 2 3 1 1]\n",
      "reward: -0.0\n",
      "epReward so far: -10001.0\n",
      "final state: [3 2 2 3 1 1]\n",
      "Episode : 51\n",
      "state : [3 2 2 3 1 1]\n",
      "action : 0\n",
      "new state: [3 2 2 3 1 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [3 2 2 3 1 0]\n",
      "action : 0\n",
      "new state: [3 2 2 3 0 1]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [3 2 2 3 0 1]\n",
      "action : 0\n",
      "new state: [3 2 2 3 2 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "state : [3 2 2 3 2 3]\n",
      "action : 0\n",
      "new state: [3 2 2 3 3 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -30001.0\n",
      "state : [3 2 2 3 3 3]\n",
      "action : 0\n",
      "new state: [2 2 2 4 1 3]\n",
      "reward: -1.0\n",
      "epReward so far: -30002.0\n",
      "final state: [2 2 2 4 1 3]\n",
      "Episode : 52\n",
      "state : [2 2 2 4 1 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 0 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 2 2 4 0 0]\n",
      "action : 3\n",
      "new state: [3 2 2 3 0 0]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [3 2 2 3 0 0]\n",
      "action : 0\n",
      "new state: [3 2 2 3 1 2]\n",
      "reward: -0.0\n",
      "epReward so far: -10001.0\n",
      "state : [3 2 2 3 1 2]\n",
      "action : 0\n",
      "new state: [3 2 2 3 2 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "state : [3 2 2 3 2 1]\n",
      "action : 0\n",
      "new state: [3 2 2 3 1 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -30001.0\n",
      "final state: [3 2 2 3 1 1]\n",
      "Episode : 53\n",
      "state : [3 2 2 3 1 1]\n",
      "action : 0\n",
      "new state: [2 3 2 3 3 3]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [2 3 2 3 3 3]\n",
      "action : 1\n",
      "new state: [2 3 2 3 1 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 3 2 3 1 0]\n",
      "action : 1\n",
      "new state: [3 2 2 3 0 1]\n",
      "reward: -0.0\n",
      "epReward so far: -10001.0\n",
      "state : [3 2 2 3 0 1]\n",
      "action : 0\n",
      "new state: [2 3 2 3 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 3 2 3 2 2]\n",
      "action : 1\n",
      "new state: [2 3 2 3 2 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "final state: [2 3 2 3 2 1]\n",
      "Episode : 54\n",
      "state : [2 3 2 3 2 1]\n",
      "action : 1\n",
      "new state: [2 3 2 3 3 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 3 2 3 3 1]\n",
      "action : 1\n",
      "new state: [2 3 2 3 3 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [2 3 2 3 3 0]\n",
      "action : 1\n",
      "new state: [3 2 2 3 0 0]\n",
      "reward: -1.0\n",
      "epReward so far: -20001.0\n",
      "state : [3 2 2 3 0 0]\n",
      "action : 0\n",
      "new state: [3 2 2 3 3 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -30001.0\n",
      "state : [3 2 2 3 3 0]\n",
      "action : 0\n",
      "new state: [3 2 2 3 3 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -40001.0\n",
      "final state: [3 2 2 3 3 3]\n",
      "Episode : 55\n",
      "state : [3 2 2 3 3 3]\n",
      "action : 0\n",
      "new state: [2 2 2 4 2 0]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [2 2 2 4 2 0]\n",
      "action : 3\n",
      "new state: [3 2 2 3 2 2]\n",
      "reward: -1.0\n",
      "epReward so far: -2.0\n",
      "state : [3 2 2 3 2 2]\n",
      "action : 0\n",
      "new state: [2 2 3 3 0 0]\n",
      "reward: -1.0\n",
      "epReward so far: -3.0\n",
      "state : [2 2 3 3 0 0]\n",
      "action : 2\n",
      "new state: [3 2 2 3 2 3]\n",
      "reward: -1.0\n",
      "epReward so far: -4.0\n",
      "state : [3 2 2 3 2 3]\n",
      "action : 0\n",
      "new state: [2 2 2 4 1 3]\n",
      "reward: -1.0\n",
      "epReward so far: -5.0\n",
      "final state: [2 2 2 4 1 3]\n",
      "Episode : 56\n",
      "state : [2 2 2 4 1 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 2 3]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [2 2 2 4 2 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 0 0]\n",
      "reward: -1.0\n",
      "epReward so far: -2.0\n",
      "state : [2 2 2 4 0 0]\n",
      "action : 3\n",
      "new state: [2 2 2 4 3 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10002.0\n",
      "state : [2 2 2 4 3 2]\n",
      "action : 3\n",
      "new state: [2 2 2 4 1 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20002.0\n",
      "state : [2 2 2 4 1 2]\n",
      "action : 3\n",
      "new state: [2 2 2 4 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -30002.0\n",
      "final state: [2 2 2 4 0 2]\n",
      "Episode : 57\n",
      "state : [2 2 2 4 0 2]\n",
      "action : 3\n",
      "new state: [2 2 3 3 0 1]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [2 2 3 3 0 1]\n",
      "action : 2\n",
      "new state: [2 2 3 3 3 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 2 3 3 3 0]\n",
      "action : 2\n",
      "new state: [2 2 3 3 3 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "state : [2 2 3 3 3 1]\n",
      "action : 2\n",
      "new state: [2 3 2 3 1 2]\n",
      "reward: -1.0\n",
      "epReward so far: -20002.0\n",
      "state : [2 3 2 3 1 2]\n",
      "action : 1\n",
      "new state: [2 2 3 3 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: -20002.0\n",
      "final state: [2 2 3 3 0 2]\n",
      "Episode : 58\n",
      "state : [2 2 3 3 0 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 1 1]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [2 2 3 3 1 1]\n",
      "action : 2\n",
      "new state: [2 2 3 3 2 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 2 3 3 2 0]\n",
      "action : 2\n",
      "new state: [3 2 2 3 2 1]\n",
      "reward: -0.0\n",
      "epReward so far: -10001.0\n",
      "state : [3 2 2 3 2 1]\n",
      "action : 0\n",
      "new state: [2 3 2 3 2 1]\n",
      "reward: -1.0\n",
      "epReward so far: -10002.0\n",
      "state : [2 3 2 3 2 1]\n",
      "action : 1\n",
      "new state: [2 3 2 3 1 1]\n",
      "reward: -1.0\n",
      "epReward so far: -10003.0\n",
      "final state: [2 3 2 3 1 1]\n",
      "Episode : 59\n",
      "state : [2 3 2 3 1 1]\n",
      "action : 1\n",
      "new state: [2 3 2 3 1 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 3 2 3 1 1]\n",
      "action : 1\n",
      "new state: [2 3 2 3 2 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 3 2 3 2 3]\n",
      "action : 1\n",
      "new state: [2 2 2 4 1 0]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [2 2 2 4 1 0]\n",
      "action : 3\n",
      "new state: [3 2 2 3 1 3]\n",
      "reward: -1.0\n",
      "epReward so far: -2.0\n",
      "state : [3 2 2 3 1 3]\n",
      "action : 0\n",
      "new state: [2 2 2 4 1 1]\n",
      "reward: -1.0\n",
      "epReward so far: -3.0\n",
      "final state: [2 2 2 4 1 1]\n",
      "Episode : 60\n",
      "state : [2 2 2 4 1 1]\n",
      "action : 3\n",
      "new state: [2 2 2 4 3 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 2 2 4 3 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 1 2]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 2 2 4 1 2]\n",
      "action : 3\n",
      "new state: [2 2 2 4 0 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [2 2 2 4 0 0]\n",
      "action : 3\n",
      "new state: [2 2 2 4 1 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -30000.0\n",
      "state : [2 2 2 4 1 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 2 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -40000.0\n",
      "final state: [2 2 2 4 2 3]\n",
      "Episode : 61\n",
      "state : [2 2 2 4 2 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 1 2]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [2 2 2 4 1 2]\n",
      "action : 3\n",
      "new state: [2 2 2 4 3 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 2 2 4 3 2]\n",
      "action : 3\n",
      "new state: [2 2 3 3 0 3]\n",
      "reward: -0.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 2 3 3 0 3]\n",
      "action : 2\n",
      "new state: [2 2 3 3 1 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "state : [2 2 3 3 1 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 3 3]\n",
      "reward: -1.0\n",
      "epReward so far: -20002.0\n",
      "final state: [2 2 3 3 3 3]\n",
      "Episode : 62\n",
      "state : [2 2 3 3 3 3]\n",
      "action : 2\n",
      "new state: [2 2 2 4 0 3]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [2 2 2 4 0 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 1 3]\n",
      "reward: -1.0\n",
      "epReward so far: -2.0\n",
      "state : [2 2 2 4 1 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 0 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10002.0\n",
      "state : [2 2 2 4 0 1]\n",
      "action : 3\n",
      "new state: [2 2 2 4 3 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20002.0\n",
      "state : [2 2 2 4 3 2]\n",
      "action : 3\n",
      "new state: [2 2 3 3 3 1]\n",
      "reward: -0.0\n",
      "epReward so far: -20002.0\n",
      "final state: [2 2 3 3 3 1]\n",
      "Episode : 63\n",
      "state : [2 2 3 3 3 1]\n",
      "action : 2\n",
      "new state: [2 3 2 3 3 0]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [2 3 2 3 3 0]\n",
      "action : 1\n",
      "new state: [2 3 2 3 3 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 3 2 3 3 2]\n",
      "action : 1\n",
      "new state: [2 2 3 3 3 1]\n",
      "reward: -1.0\n",
      "epReward so far: -10002.0\n",
      "state : [2 2 3 3 3 1]\n",
      "action : 2\n",
      "new state: [2 2 3 3 1 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20002.0\n",
      "state : [2 2 3 3 1 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 1 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -30002.0\n",
      "final state: [2 2 3 3 1 2]\n",
      "Episode : 64\n",
      "state : [2 2 3 3 1 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 2 3]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [2 2 3 3 2 3]\n",
      "action : 2\n",
      "new state: [2 2 2 4 1 0]\n",
      "reward: -0.0\n",
      "epReward so far: -1.0\n",
      "state : [2 2 2 4 1 0]\n",
      "action : 3\n",
      "new state: [3 2 2 3 1 3]\n",
      "reward: -1.0\n",
      "epReward so far: -2.0\n",
      "state : [3 2 2 3 1 3]\n",
      "action : 0\n",
      "new state: [3 2 2 3 0 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10002.0\n",
      "state : [3 2 2 3 0 3]\n",
      "action : 0\n",
      "new state: [2 2 2 4 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: -10002.0\n",
      "final state: [2 2 2 4 2 2]\n",
      "Episode : 65\n",
      "state : [2 2 2 4 2 2]\n",
      "action : 3\n",
      "new state: [2 2 2 4 2 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 2 2 4 2 2]\n",
      "action : 3\n",
      "new state: [2 2 2 4 0 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [2 2 2 4 0 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 2 1]\n",
      "reward: -1.0\n",
      "epReward so far: -20001.0\n",
      "state : [2 2 2 4 2 1]\n",
      "action : 3\n",
      "new state: [2 3 2 3 3 3]\n",
      "reward: -1.0\n",
      "epReward so far: -20002.0\n",
      "state : [2 3 2 3 3 3]\n",
      "action : 1\n",
      "new state: [2 2 2 4 3 1]\n",
      "reward: -1.0\n",
      "epReward so far: -20003.0\n",
      "final state: [2 2 2 4 3 1]\n",
      "Episode : 66\n",
      "state : [2 2 2 4 3 1]\n",
      "action : 3\n",
      "new state: [2 3 2 3 1 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 3 2 3 1 1]\n",
      "action : 1\n",
      "new state: [2 3 2 3 0 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 3 2 3 0 1]\n",
      "action : 1\n",
      "new state: [2 3 2 3 0 3]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [2 3 2 3 0 3]\n",
      "action : 1\n",
      "new state: [2 3 2 3 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 3 2 3 0 2]\n",
      "action : 1\n",
      "new state: [2 2 3 3 0 2]\n",
      "reward: -1.0\n",
      "epReward so far: -10002.0\n",
      "final state: [2 2 3 3 0 2]\n",
      "Episode : 67\n",
      "state : [2 2 3 3 0 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 1 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 2 3 3 1 1]\n",
      "action : 2\n",
      "new state: [2 2 3 3 3 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [2 2 3 3 3 1]\n",
      "action : 2\n",
      "new state: [2 3 2 3 2 2]\n",
      "reward: -1.0\n",
      "epReward so far: -20001.0\n",
      "state : [2 3 2 3 2 2]\n",
      "action : 1\n",
      "new state: [2 3 2 3 2 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -30001.0\n",
      "state : [2 3 2 3 2 2]\n",
      "action : 1\n",
      "new state: [2 2 3 3 0 3]\n",
      "reward: -1.0\n",
      "epReward so far: -30002.0\n",
      "final state: [2 2 3 3 0 3]\n",
      "Episode : 68\n",
      "state : [2 2 3 3 0 3]\n",
      "action : 2\n",
      "new state: [2 2 3 3 0 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 2 3 3 0 1]\n",
      "action : 2\n",
      "new state: [2 2 3 3 3 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [2 2 3 3 3 1]\n",
      "action : 2\n",
      "new state: [2 2 3 3 0 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -30000.0\n",
      "state : [2 2 3 3 0 1]\n",
      "action : 2\n",
      "new state: [2 2 3 3 0 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -40000.0\n",
      "state : [2 2 3 3 0 3]\n",
      "action : 2\n",
      "new state: [2 2 2 4 3 0]\n",
      "reward: -1.0\n",
      "epReward so far: -40001.0\n",
      "final state: [2 2 2 4 3 0]\n",
      "Episode : 69\n",
      "state : [2 2 2 4 3 0]\n",
      "action : 3\n",
      "new state: [3 2 2 3 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 2 2 3 0 2]\n",
      "action : 0\n",
      "new state: [2 2 3 3 1 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 2 3 3 1 0]\n",
      "action : 2\n",
      "new state: [2 2 3 3 3 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 2 3 3 3 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 3 1]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 2 3 3 3 1]\n",
      "action : 2\n",
      "new state: [2 2 3 3 1 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "final state: [2 2 3 3 1 1]\n",
      "Episode : 70\n",
      "state : [2 2 3 3 1 1]\n",
      "action : 2\n",
      "new state: [2 2 3 3 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 2 3 3 0 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 2 1]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 2 3 3 2 1]\n",
      "action : 2\n",
      "new state: [2 3 2 3 1 2]\n",
      "reward: -0.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 3 2 3 1 2]\n",
      "action : 1\n",
      "new state: [2 2 3 3 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 2 3 3 0 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 2 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "final state: [2 2 3 3 2 1]\n",
      "Episode : 71\n",
      "state : [2 2 3 3 2 1]\n",
      "action : 2\n",
      "new state: [2 3 2 3 3 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 3 2 3 3 0]\n",
      "action : 1\n",
      "new state: [3 2 2 3 1 2]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [3 2 2 3 1 2]\n",
      "action : 0\n",
      "new state: [3 2 2 3 3 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "state : [3 2 2 3 3 0]\n",
      "action : 0\n",
      "new state: [3 2 2 3 0 0]\n",
      "reward: -1.0\n",
      "epReward so far: -10002.0\n",
      "state : [3 2 2 3 0 0]\n",
      "action : 0\n",
      "new state: [3 2 2 3 0 3]\n",
      "reward: -0.0\n",
      "epReward so far: -10002.0\n",
      "final state: [3 2 2 3 0 3]\n",
      "Episode : 72\n",
      "state : [3 2 2 3 0 3]\n",
      "action : 0\n",
      "new state: [3 2 2 3 3 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [3 2 2 3 3 1]\n",
      "action : 0\n",
      "new state: [2 3 2 3 2 2]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 3 2 3 2 2]\n",
      "action : 1\n",
      "new state: [2 3 2 3 0 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "state : [2 3 2 3 0 0]\n",
      "action : 1\n",
      "new state: [3 2 2 3 2 0]\n",
      "reward: -1.0\n",
      "epReward so far: -20002.0\n",
      "state : [3 2 2 3 2 0]\n",
      "action : 0\n",
      "new state: [3 2 2 3 3 1]\n",
      "reward: -1.0\n",
      "epReward so far: -20003.0\n",
      "final state: [3 2 2 3 3 1]\n",
      "Episode : 73\n",
      "state : [3 2 2 3 3 1]\n",
      "action : 0\n",
      "new state: [3 2 2 3 1 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [3 2 2 3 1 1]\n",
      "action : 0\n",
      "new state: [2 3 2 3 2 0]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 3 2 3 2 0]\n",
      "action : 1\n",
      "new state: [3 2 2 3 1 1]\n",
      "reward: -1.0\n",
      "epReward so far: -10002.0\n",
      "state : [3 2 2 3 1 1]\n",
      "action : 0\n",
      "new state: [2 3 2 3 2 2]\n",
      "reward: -1.0\n",
      "epReward so far: -10003.0\n",
      "state : [2 3 2 3 2 2]\n",
      "action : 1\n",
      "new state: [2 3 2 3 3 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -20003.0\n",
      "final state: [2 3 2 3 3 0]\n",
      "Episode : 74\n",
      "state : [2 3 2 3 3 0]\n",
      "action : 1\n",
      "new state: [3 2 2 3 2 1]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [3 2 2 3 2 1]\n",
      "action : 0\n",
      "new state: [2 3 2 3 0 1]\n",
      "reward: -1.0\n",
      "epReward so far: -2.0\n",
      "state : [2 3 2 3 0 1]\n",
      "action : 1\n",
      "new state: [2 3 2 3 1 0]\n",
      "reward: -1.0\n",
      "epReward so far: -3.0\n",
      "state : [2 3 2 3 1 0]\n",
      "action : 1\n",
      "new state: [2 3 2 3 1 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10003.0\n",
      "state : [2 3 2 3 1 0]\n",
      "action : 1\n",
      "new state: [3 2 2 3 3 1]\n",
      "reward: -0.0\n",
      "epReward so far: -10003.0\n",
      "final state: [3 2 2 3 3 1]\n",
      "Episode : 75\n",
      "state : [3 2 2 3 3 1]\n",
      "action : 0\n",
      "new state: [3 2 2 3 3 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [3 2 2 3 3 1]\n",
      "action : 0\n",
      "new state: [3 2 2 3 1 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [3 2 2 3 1 1]\n",
      "action : 0\n",
      "new state: [3 2 2 3 2 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -30000.0\n",
      "state : [3 2 2 3 2 2]\n",
      "action : 0\n",
      "new state: [3 2 2 3 0 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -40000.0\n",
      "state : [3 2 2 3 0 3]\n",
      "action : 0\n",
      "new state: [2 2 2 4 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: -40000.0\n",
      "final state: [2 2 2 4 2 2]\n",
      "Episode : 76\n",
      "state : [2 2 2 4 2 2]\n",
      "action : 3\n",
      "new state: [2 2 2 4 3 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 2 2 4 3 2]\n",
      "action : 3\n",
      "new state: [2 2 3 3 1 0]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 2 3 3 1 0]\n",
      "action : 2\n",
      "new state: [3 2 2 3 0 0]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [3 2 2 3 0 0]\n",
      "action : 0\n",
      "new state: [3 2 2 3 3 2]\n",
      "reward: -0.0\n",
      "epReward so far: -10001.0\n",
      "state : [3 2 2 3 3 2]\n",
      "action : 0\n",
      "new state: [3 2 2 3 0 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "final state: [3 2 2 3 0 3]\n",
      "Episode : 77\n",
      "state : [3 2 2 3 0 3]\n",
      "action : 0\n",
      "new state: [2 2 2 4 3 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 2 2 4 3 1]\n",
      "action : 3\n",
      "new state: [2 3 2 3 3 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 3 2 3 3 0]\n",
      "action : 1\n",
      "new state: [3 2 2 3 3 1]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [3 2 2 3 3 1]\n",
      "action : 0\n",
      "new state: [2 3 2 3 3 3]\n",
      "reward: -1.0\n",
      "epReward so far: -2.0\n",
      "state : [2 3 2 3 3 3]\n",
      "action : 1\n",
      "new state: [2 3 2 3 3 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10002.0\n",
      "final state: [2 3 2 3 3 3]\n",
      "Episode : 78\n",
      "state : [2 3 2 3 3 3]\n",
      "action : 1\n",
      "new state: [2 2 2 4 3 2]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [2 2 2 4 3 2]\n",
      "action : 3\n",
      "new state: [2 2 3 3 1 2]\n",
      "reward: -0.0\n",
      "epReward so far: -1.0\n",
      "state : [2 2 3 3 1 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 0 1]\n",
      "reward: -1.0\n",
      "epReward so far: -2.0\n",
      "state : [2 2 3 3 0 1]\n",
      "action : 2\n",
      "new state: [2 3 2 3 1 2]\n",
      "reward: -1.0\n",
      "epReward so far: -3.0\n",
      "state : [2 3 2 3 1 2]\n",
      "action : 1\n",
      "new state: [2 2 3 3 3 1]\n",
      "reward: -0.0\n",
      "epReward so far: -3.0\n",
      "final state: [2 2 3 3 3 1]\n",
      "Episode : 79\n",
      "state : [2 2 3 3 3 1]\n",
      "action : 2\n",
      "new state: [2 3 2 3 1 0]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [2 3 2 3 1 0]\n",
      "action : 1\n",
      "new state: [2 3 2 3 0 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 3 2 3 0 1]\n",
      "action : 1\n",
      "new state: [2 3 2 3 0 3]\n",
      "reward: -1.0\n",
      "epReward so far: -10002.0\n",
      "state : [2 3 2 3 0 3]\n",
      "action : 1\n",
      "new state: [2 2 2 4 0 1]\n",
      "reward: -1.0\n",
      "epReward so far: -10003.0\n",
      "state : [2 2 2 4 0 1]\n",
      "action : 3\n",
      "new state: [2 3 2 3 1 3]\n",
      "reward: -1.0\n",
      "epReward so far: -10004.0\n",
      "final state: [2 3 2 3 1 3]\n",
      "Episode : 80\n",
      "state : [2 3 2 3 1 3]\n",
      "action : 1\n",
      "new state: [2 3 2 3 3 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 3 2 3 3 2]\n",
      "action : 1\n",
      "new state: [2 3 2 3 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [2 3 2 3 0 2]\n",
      "action : 1\n",
      "new state: [2 2 3 3 3 3]\n",
      "reward: -1.0\n",
      "epReward so far: -20001.0\n",
      "state : [2 2 3 3 3 3]\n",
      "action : 2\n",
      "new state: [2 2 3 3 3 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -30001.0\n",
      "state : [2 2 3 3 3 3]\n",
      "action : 2\n",
      "new state: [2 2 3 3 2 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -40001.0\n",
      "final state: [2 2 3 3 2 3]\n",
      "Episode : 81\n",
      "state : [2 2 3 3 2 3]\n",
      "action : 2\n",
      "new state: [2 2 3 3 3 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 2 3 3 3 1]\n",
      "action : 2\n",
      "new state: [2 2 3 3 2 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [2 2 3 3 2 1]\n",
      "action : 2\n",
      "new state: [2 3 2 3 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: -20000.0\n",
      "state : [2 3 2 3 2 2]\n",
      "action : 1\n",
      "new state: [2 2 3 3 1 3]\n",
      "reward: -1.0\n",
      "epReward so far: -20001.0\n",
      "state : [2 2 3 3 1 3]\n",
      "action : 2\n",
      "new state: [2 2 3 3 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -30001.0\n",
      "final state: [2 2 3 3 0 2]\n",
      "Episode : 82\n",
      "state : [2 2 3 3 0 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 3 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 2 3 3 3 1]\n",
      "action : 2\n",
      "new state: [2 3 2 3 0 2]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 3 2 3 0 2]\n",
      "action : 1\n",
      "new state: [2 2 3 3 3 1]\n",
      "reward: -1.0\n",
      "epReward so far: -10002.0\n",
      "state : [2 2 3 3 3 1]\n",
      "action : 2\n",
      "new state: [2 3 2 3 3 1]\n",
      "reward: -1.0\n",
      "epReward so far: -10003.0\n",
      "state : [2 3 2 3 3 1]\n",
      "action : 1\n",
      "new state: [2 3 2 3 1 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20003.0\n",
      "final state: [2 3 2 3 1 1]\n",
      "Episode : 83\n",
      "state : [2 3 2 3 1 1]\n",
      "action : 1\n",
      "new state: [2 3 2 3 1 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 3 2 3 1 0]\n",
      "action : 1\n",
      "new state: [2 3 2 3 3 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 3 2 3 3 1]\n",
      "action : 1\n",
      "new state: [2 3 2 3 3 1]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 3 2 3 3 1]\n",
      "action : 1\n",
      "new state: [2 3 2 3 3 1]\n",
      "reward: -1.0\n",
      "epReward so far: -10002.0\n",
      "state : [2 3 2 3 3 1]\n",
      "action : 1\n",
      "new state: [2 3 2 3 0 0]\n",
      "reward: -1.0\n",
      "epReward so far: -10003.0\n",
      "final state: [2 3 2 3 0 0]\n",
      "Episode : 84\n",
      "state : [2 3 2 3 0 0]\n",
      "action : 1\n",
      "new state: [3 2 2 3 0 3]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [3 2 2 3 0 3]\n",
      "action : 0\n",
      "new state: [3 2 2 3 3 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "state : [3 2 2 3 3 2]\n",
      "action : 0\n",
      "new state: [2 2 3 3 3 0]\n",
      "reward: -1.0\n",
      "epReward so far: -10002.0\n",
      "state : [2 2 3 3 3 0]\n",
      "action : 2\n",
      "new state: [2 2 3 3 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20002.0\n",
      "state : [2 2 3 3 0 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 2 3]\n",
      "reward: -1.0\n",
      "epReward so far: -20003.0\n",
      "final state: [2 2 3 3 2 3]\n",
      "Episode : 85\n",
      "state : [2 2 3 3 2 3]\n",
      "action : 2\n",
      "new state: [2 2 2 4 3 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 2 2 4 3 0]\n",
      "action : 3\n",
      "new state: [3 2 2 3 1 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 2 2 3 1 1]\n",
      "action : 0\n",
      "new state: [2 3 2 3 1 0]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [2 3 2 3 1 0]\n",
      "action : 1\n",
      "new state: [3 2 2 3 2 3]\n",
      "reward: -0.0\n",
      "epReward so far: -1.0\n",
      "state : [3 2 2 3 2 3]\n",
      "action : 0\n",
      "new state: [2 2 2 4 3 1]\n",
      "reward: -1.0\n",
      "epReward so far: -2.0\n",
      "final state: [2 2 2 4 3 1]\n",
      "Episode : 86\n",
      "state : [2 2 2 4 3 1]\n",
      "action : 3\n",
      "new state: [2 3 2 3 2 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 3 2 3 2 1]\n",
      "action : 1\n",
      "new state: [2 3 2 3 0 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 3 2 3 0 0]\n",
      "action : 1\n",
      "new state: [2 3 2 3 3 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [2 3 2 3 3 3]\n",
      "action : 1\n",
      "new state: [2 2 2 4 0 0]\n",
      "reward: -1.0\n",
      "epReward so far: -20001.0\n",
      "state : [2 2 2 4 0 0]\n",
      "action : 3\n",
      "new state: [2 2 2 4 2 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -30001.0\n",
      "final state: [2 2 2 4 2 1]\n",
      "Episode : 87\n",
      "state : [2 2 2 4 2 1]\n",
      "action : 3\n",
      "new state: [2 3 2 3 0 1]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [2 3 2 3 0 1]\n",
      "action : 1\n",
      "new state: [2 3 2 3 3 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 3 2 3 3 2]\n",
      "action : 1\n",
      "new state: [2 3 2 3 1 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "state : [2 3 2 3 1 2]\n",
      "action : 1\n",
      "new state: [2 2 3 3 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: -20001.0\n",
      "state : [2 2 3 3 0 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 1 2]\n",
      "reward: -1.0\n",
      "epReward so far: -20002.0\n",
      "final state: [2 2 3 3 1 2]\n",
      "Episode : 88\n",
      "state : [2 2 3 3 1 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 3 2]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [2 2 3 3 3 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 2 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 2 3 3 2 0]\n",
      "action : 2\n",
      "new state: [3 2 2 3 2 3]\n",
      "reward: -0.0\n",
      "epReward so far: -10001.0\n",
      "state : [3 2 2 3 2 3]\n",
      "action : 0\n",
      "new state: [3 2 2 3 3 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "state : [3 2 2 3 3 2]\n",
      "action : 0\n",
      "new state: [2 2 3 3 0 0]\n",
      "reward: -1.0\n",
      "epReward so far: -20002.0\n",
      "final state: [2 2 3 3 0 0]\n",
      "Episode : 89\n",
      "state : [2 2 3 3 0 0]\n",
      "action : 2\n",
      "new state: [3 2 2 3 1 3]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [3 2 2 3 1 3]\n",
      "action : 0\n",
      "new state: [3 2 2 3 2 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "state : [3 2 2 3 2 3]\n",
      "action : 0\n",
      "new state: [3 2 2 3 1 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "state : [3 2 2 3 1 1]\n",
      "action : 0\n",
      "new state: [3 2 2 3 3 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -30001.0\n",
      "state : [3 2 2 3 3 1]\n",
      "action : 0\n",
      "new state: [3 2 2 3 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -40001.0\n",
      "final state: [3 2 2 3 0 2]\n",
      "Episode : 90\n",
      "state : [3 2 2 3 0 2]\n",
      "action : 0\n",
      "new state: [2 2 3 3 2 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 2 3 3 2 3]\n",
      "action : 2\n",
      "new state: [2 2 3 3 1 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 2 3 3 1 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 3 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [2 2 3 3 3 3]\n",
      "action : 2\n",
      "new state: [2 2 2 4 1 3]\n",
      "reward: -1.0\n",
      "epReward so far: -20001.0\n",
      "state : [2 2 2 4 1 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 1 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -30001.0\n",
      "final state: [2 2 2 4 1 1]\n",
      "Episode : 91\n",
      "state : [2 2 2 4 1 1]\n",
      "action : 3\n",
      "new state: [2 2 2 4 2 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 2 2 4 2 1]\n",
      "action : 3\n",
      "new state: [2 3 2 3 0 3]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 3 2 3 0 3]\n",
      "action : 1\n",
      "new state: [2 3 2 3 3 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "state : [2 3 2 3 3 2]\n",
      "action : 1\n",
      "new state: [2 3 2 3 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -30001.0\n",
      "state : [2 3 2 3 0 2]\n",
      "action : 1\n",
      "new state: [2 3 2 3 3 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -40001.0\n",
      "final state: [2 3 2 3 3 3]\n",
      "Episode : 92\n",
      "state : [2 3 2 3 3 3]\n",
      "action : 1\n",
      "new state: [2 2 2 4 2 1]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [2 2 2 4 2 1]\n",
      "action : 3\n",
      "new state: [2 3 2 3 1 2]\n",
      "reward: -1.0\n",
      "epReward so far: -2.0\n",
      "state : [2 3 2 3 1 2]\n",
      "action : 1\n",
      "new state: [2 2 3 3 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: -2.0\n",
      "state : [2 2 3 3 0 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 2 1]\n",
      "reward: -1.0\n",
      "epReward so far: -3.0\n",
      "state : [2 2 3 3 2 1]\n",
      "action : 2\n",
      "new state: [2 2 3 3 3 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10003.0\n",
      "final state: [2 2 3 3 3 2]\n",
      "Episode : 93\n",
      "state : [2 2 3 3 3 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 1 2]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [2 2 3 3 1 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 0 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 2 3 3 0 0]\n",
      "action : 2\n",
      "new state: [3 2 2 3 3 1]\n",
      "reward: -1.0\n",
      "epReward so far: -10002.0\n",
      "state : [3 2 2 3 3 1]\n",
      "action : 0\n",
      "new state: [2 3 2 3 1 1]\n",
      "reward: -1.0\n",
      "epReward so far: -10003.0\n",
      "state : [2 3 2 3 1 1]\n",
      "action : 1\n",
      "new state: [2 3 2 3 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: -10003.0\n",
      "final state: [2 3 2 3 2 2]\n",
      "Episode : 94\n",
      "state : [2 3 2 3 2 2]\n",
      "action : 1\n",
      "new state: [2 2 3 3 0 3]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [2 2 3 3 0 3]\n",
      "action : 2\n",
      "new state: [2 2 3 3 0 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 2 3 3 0 1]\n",
      "action : 2\n",
      "new state: [2 3 2 3 2 1]\n",
      "reward: -1.0\n",
      "epReward so far: -10002.0\n",
      "state : [2 3 2 3 2 1]\n",
      "action : 1\n",
      "new state: [2 3 2 3 1 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20002.0\n",
      "state : [2 3 2 3 1 2]\n",
      "action : 1\n",
      "new state: [2 2 3 3 2 0]\n",
      "reward: -0.0\n",
      "epReward so far: -20002.0\n",
      "final state: [2 2 3 3 2 0]\n",
      "Episode : 95\n",
      "state : [2 2 3 3 2 0]\n",
      "action : 2\n",
      "new state: [3 2 2 3 3 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 2 2 3 3 0]\n",
      "action : 0\n",
      "new state: [3 2 2 3 1 3]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [3 2 2 3 1 3]\n",
      "action : 0\n",
      "new state: [3 2 2 3 0 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "state : [3 2 2 3 0 0]\n",
      "action : 0\n",
      "new state: [3 2 2 3 1 3]\n",
      "reward: -0.0\n",
      "epReward so far: -10001.0\n",
      "state : [3 2 2 3 1 3]\n",
      "action : 0\n",
      "new state: [3 2 2 3 2 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "final state: [3 2 2 3 2 3]\n",
      "Episode : 96\n",
      "state : [3 2 2 3 2 3]\n",
      "action : 0\n",
      "new state: [2 2 2 4 3 3]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [2 2 2 4 3 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 0 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 2 2 4 0 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 3 3]\n",
      "reward: -1.0\n",
      "epReward so far: -10002.0\n",
      "state : [2 2 2 4 3 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 1 0]\n",
      "reward: -0.0\n",
      "epReward so far: -10002.0\n",
      "state : [2 2 2 4 1 0]\n",
      "action : 3\n",
      "new state: [2 2 2 4 0 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20002.0\n",
      "final state: [2 2 2 4 0 3]\n",
      "Episode : 97\n",
      "state : [2 2 2 4 0 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 0 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 2 2 4 0 0]\n",
      "action : 3\n",
      "new state: [2 2 2 4 1 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [2 2 2 4 1 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 3 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -30000.0\n",
      "state : [2 2 2 4 3 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 3 3]\n",
      "reward: -0.0\n",
      "epReward so far: -30000.0\n",
      "state : [2 2 2 4 3 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 3 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -40000.0\n",
      "final state: [2 2 2 4 3 2]\n",
      "Episode : 98\n",
      "state : [2 2 2 4 3 2]\n",
      "action : 3\n",
      "new state: [2 2 3 3 2 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 2 3 3 2 1]\n",
      "action : 2\n",
      "new state: [2 2 3 3 0 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 2 3 3 0 0]\n",
      "action : 2\n",
      "new state: [2 2 3 3 3 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [2 2 3 3 3 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 1 1]\n",
      "reward: -1.0\n",
      "epReward so far: -20001.0\n",
      "state : [2 2 3 3 1 1]\n",
      "action : 2\n",
      "new state: [2 2 3 3 2 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -30001.0\n",
      "final state: [2 2 3 3 2 3]\n",
      "Episode : 99\n",
      "state : [2 2 3 3 2 3]\n",
      "action : 2\n",
      "new state: [2 2 3 3 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 2 3 3 0 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 2 3]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 2 3 3 2 3]\n",
      "action : 2\n",
      "new state: [2 2 3 3 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "state : [2 2 3 3 0 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 0 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -30001.0\n",
      "state : [2 2 3 3 0 1]\n",
      "action : 2\n",
      "new state: [2 2 3 3 2 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -40001.0\n",
      "final state: [2 2 3 3 2 2]\n",
      "**************************************************\n",
      "Experiment complete\n",
      "**************************************************\n",
      "**************************************************\n",
      "Saving data\n",
      "**************************************************\n",
      "[[ 0.00000000e+00  0.00000000e+00 -3.00000000e+04  6.36500000e+03\n",
      "  -4.78049343e+00]\n",
      " [ 1.00000000e+00  0.00000000e+00 -5.00000000e+04  9.99700000e+03\n",
      "  -5.39022491e+00]\n",
      " [ 2.00000000e+00  0.00000000e+00 -3.00010000e+04  1.00130000e+04\n",
      "  -5.35469026e+00]\n",
      " [ 3.00000000e+00  0.00000000e+00 -2.00030000e+04  1.22810000e+04\n",
      "  -5.29112020e+00]\n",
      " [ 4.00000000e+00  0.00000000e+00 -4.00010000e+04  9.17400000e+03\n",
      "  -5.20446789e+00]\n",
      " [ 5.00000000e+00  0.00000000e+00 -2.00020000e+04  1.31540000e+04\n",
      "  -5.61605109e+00]\n",
      " [ 6.00000000e+00  0.00000000e+00 -3.00010000e+04  9.82900000e+03\n",
      "  -5.24103011e+00]\n",
      " [ 7.00000000e+00  0.00000000e+00 -3.00020000e+04  1.05290000e+04\n",
      "  -5.03134122e+00]\n",
      " [ 8.00000000e+00  0.00000000e+00 -2.00030000e+04  8.88900000e+03\n",
      "  -2.60589475e+00]\n",
      " [ 9.00000000e+00  0.00000000e+00 -3.00010000e+04  1.08530000e+04\n",
      "  -5.14350369e+00]\n",
      " [ 1.00000000e+01  0.00000000e+00 -2.00030000e+04  8.74500000e+03\n",
      "  -5.38797965e+00]\n",
      " [ 1.10000000e+01  0.00000000e+00 -3.00020000e+04  8.34100000e+03\n",
      "  -5.28029045e+00]\n",
      " [ 1.20000000e+01  0.00000000e+00 -1.00000000e+00  1.18590000e+04\n",
      "  -5.21270665e+00]\n",
      " [ 1.30000000e+01  0.00000000e+00 -5.00000000e+00  5.74100000e+03\n",
      "  -5.45244516e+00]\n",
      " [ 1.40000000e+01  0.00000000e+00 -3.00020000e+04  1.63210000e+04\n",
      "  -5.46599899e+00]\n",
      " [ 1.50000000e+01  0.00000000e+00 -3.00020000e+04  9.92500000e+03\n",
      "  -5.63717281e+00]\n",
      " [ 1.60000000e+01  0.00000000e+00 -2.00010000e+04  1.50870000e+04\n",
      "  -5.41799858e+00]\n",
      " [ 1.70000000e+01  0.00000000e+00 -1.00020000e+04  8.98800000e+03\n",
      "  -5.44750633e+00]\n",
      " [ 1.80000000e+01  0.00000000e+00 -2.00020000e+04  1.68330000e+04\n",
      "  -5.44171081e+00]\n",
      " [ 1.90000000e+01  0.00000000e+00 -4.00010000e+04  1.22890000e+04\n",
      "  -5.54597122e+00]\n",
      " [ 2.00000000e+01  0.00000000e+00 -1.00040000e+04  1.57250000e+04\n",
      "  -5.13504389e+00]\n",
      " [ 2.10000000e+01  0.00000000e+00 -3.00010000e+04  7.69300000e+03\n",
      "  -5.48656542e+00]\n",
      " [ 2.20000000e+01  0.00000000e+00 -2.00020000e+04  4.73700000e+03\n",
      "  -5.28955914e+00]\n",
      " [ 2.30000000e+01  0.00000000e+00 -3.00010000e+04  1.93410000e+04\n",
      "  -5.31081773e+00]\n",
      " [ 2.40000000e+01  0.00000000e+00 -2.00030000e+04  7.58770000e+04\n",
      "  -5.41285222e+00]\n",
      " [ 2.50000000e+01  0.00000000e+00 -2.00020000e+04  1.48730000e+04\n",
      "  -5.76772608e+00]\n",
      " [ 2.60000000e+01  0.00000000e+00 -1.00040000e+04  6.12500000e+03\n",
      "  -5.48202844e+00]\n",
      " [ 2.70000000e+01  0.00000000e+00 -3.00020000e+04  7.74900000e+03\n",
      "  -2.28886190e+00]\n",
      " [ 2.80000000e+01  0.00000000e+00 -1.00040000e+04  1.14210000e+04\n",
      "  -5.43043723e+00]\n",
      " [ 2.90000000e+01  0.00000000e+00 -2.00000000e+00  1.42200000e+04\n",
      "  -5.28385602e+00]\n",
      " [ 3.00000000e+01  0.00000000e+00 -3.00010000e+04  8.74100000e+03\n",
      "  -5.32071640e+00]\n",
      " [ 3.10000000e+01  0.00000000e+00 -3.00020000e+04  1.01050000e+04\n",
      "  -5.42047397e+00]\n",
      " [ 3.20000000e+01  0.00000000e+00 -1.00010000e+04  1.26640000e+04\n",
      "  -5.45942316e+00]\n",
      " [ 3.30000000e+01  0.00000000e+00 -2.00020000e+04  7.97200000e+03\n",
      "  -5.84860787e+00]\n",
      " [ 3.40000000e+01  0.00000000e+00 -1.00020000e+04  1.22930000e+04\n",
      "  -5.32193624e+00]\n",
      " [ 3.50000000e+01  0.00000000e+00 -1.00020000e+04  1.36450000e+04\n",
      "  -5.07889625e+00]\n",
      " [ 3.60000000e+01  0.00000000e+00 -1.00030000e+04  8.53600000e+03\n",
      "  -5.43918236e+00]\n",
      " [ 3.70000000e+01  0.00000000e+00 -3.00020000e+04  1.29810000e+04\n",
      "  -5.85175435e+00]\n",
      " [ 3.80000000e+01  0.00000000e+00 -2.00020000e+04  1.12570000e+04\n",
      "  -5.15715691e+00]\n",
      " [ 3.90000000e+01  0.00000000e+00 -2.00010000e+04  1.38090000e+04\n",
      "  -5.16622443e+00]\n",
      " [ 4.00000000e+01  0.00000000e+00 -3.00010000e+04  1.70090000e+04\n",
      "  -5.54780541e+00]\n",
      " [ 4.10000000e+01  0.00000000e+00 -1.00030000e+04  1.53720000e+04\n",
      "  -5.54170448e+00]\n",
      " [ 4.20000000e+01  0.00000000e+00 -4.00000000e+04  1.49210000e+04\n",
      "  -5.39878207e+00]\n",
      " [ 4.30000000e+01  0.00000000e+00 -1.00030000e+04  1.02160000e+04\n",
      "  -5.27991586e+00]\n",
      " [ 4.40000000e+01  0.00000000e+00 -2.00030000e+04  8.40900000e+03\n",
      "  -5.65091781e+00]\n",
      " [ 4.50000000e+01  0.00000000e+00 -2.00030000e+04  4.41400000e+03\n",
      "  -2.30131608e+00]\n",
      " [ 4.60000000e+01  0.00000000e+00 -3.00010000e+04  8.92500000e+03\n",
      "  -4.66343671e+00]\n",
      " [ 4.70000000e+01  0.00000000e+00 -3.00010000e+04  1.41330000e+04\n",
      "  -5.46554798e+00]\n",
      " [ 4.80000000e+01  0.00000000e+00 -2.00020000e+04  8.76000000e+03\n",
      "  -5.00699508e+00]\n",
      " [ 4.90000000e+01  0.00000000e+00 -2.00020000e+04  1.03210000e+04\n",
      "  -5.31915718e+00]\n",
      " [ 5.00000000e+01  0.00000000e+00 -2.00020000e+04  1.06170000e+04\n",
      "  -5.16059954e+00]\n",
      " [ 5.10000000e+01  0.00000000e+00 -1.00020000e+04  1.14800000e+04\n",
      "  -5.30590490e+00]\n",
      " [ 5.20000000e+01  0.00000000e+00 -3.00020000e+04  9.28100000e+03\n",
      "  -5.58229588e+00]\n",
      " [ 5.30000000e+01  0.00000000e+00 -1.00040000e+04  1.41810000e+04\n",
      "  -5.37194040e+00]\n",
      " [ 5.40000000e+01  0.00000000e+00 -3.00010000e+04  1.59330000e+04\n",
      "  -5.56983191e+00]\n",
      " [ 5.50000000e+01  0.00000000e+00 -1.00030000e+04  1.21650000e+04\n",
      "  -5.29330029e+00]\n",
      " [ 5.60000000e+01  0.00000000e+00 -2.00020000e+04  1.54970000e+04\n",
      "  -5.38282943e+00]\n",
      " [ 5.70000000e+01  0.00000000e+00 -3.00020000e+04  1.17410000e+04\n",
      "  -5.25025844e+00]\n",
      " [ 5.80000000e+01  0.00000000e+00 -2.00030000e+04  1.46970000e+04\n",
      "  -5.13804543e+00]\n",
      " [ 5.90000000e+01  0.00000000e+00 -4.00010000e+04  8.28100000e+03\n",
      "  -5.37019681e+00]\n",
      " [ 6.00000000e+01  0.00000000e+00 -3.00020000e+04  1.63290000e+04\n",
      "  -5.57114651e+00]\n",
      " [ 6.10000000e+01  0.00000000e+00 -3.00010000e+04  1.22850000e+04\n",
      "  -5.55987229e+00]\n",
      " [ 6.20000000e+01  0.00000000e+00 -1.00020000e+04  1.84320000e+04\n",
      "  -5.36917259e+00]\n",
      " [ 6.30000000e+01  0.00000000e+00 -3.00010000e+04  1.16610000e+04\n",
      "  -5.33476102e+00]\n",
      " [ 6.40000000e+01  0.00000000e+00 -4.00000000e+04  1.61370000e+04\n",
      "  -2.67021997e+00]\n",
      " [ 6.50000000e+01  0.00000000e+00 -4.00000000e+00  1.05480000e+04\n",
      "  -5.30595295e+00]\n",
      " [ 6.60000000e+01  0.00000000e+00 -1.00020000e+04  1.42250000e+04\n",
      "  -5.46453396e+00]\n",
      " [ 6.70000000e+01  0.00000000e+00 -2.00030000e+04  6.76500000e+03\n",
      "  -5.40385634e+00]\n",
      " [ 6.80000000e+01  0.00000000e+00 -2.00030000e+04  5.38500000e+03\n",
      "  -5.27515217e+00]\n",
      " [ 6.90000000e+01  0.00000000e+00 -2.00020000e+04  1.77130000e+04\n",
      "  -5.35620488e+00]\n",
      " [ 7.00000000e+01  0.00000000e+00 -2.00010000e+04  8.91900000e+03\n",
      "  -5.66100926e+00]\n",
      " [ 7.10000000e+01  0.00000000e+00 -2.00020000e+04  8.58000000e+03\n",
      "  -5.46695806e+00]\n",
      " [ 7.20000000e+01  0.00000000e+00 -1.00030000e+04  1.91970000e+04\n",
      "  -5.62295425e+00]\n",
      " [ 7.30000000e+01  0.00000000e+00 -2.00020000e+04  4.11520000e+04\n",
      "  -5.18188989e+00]\n",
      " [ 7.40000000e+01  0.00000000e+00 -2.00020000e+04  1.27980000e+04\n",
      "  -5.88737895e+00]\n",
      " [ 7.50000000e+01  0.00000000e+00 -5.00000000e+04  1.36770000e+04\n",
      "  -5.68606001e+00]\n",
      " [ 7.60000000e+01  0.00000000e+00 -2.00010000e+04  1.38170000e+04\n",
      "  -5.54749947e+00]\n",
      " [ 7.70000000e+01  0.00000000e+00 -2.00020000e+04  1.17170000e+04\n",
      "  -4.97146643e+00]\n",
      " [ 7.80000000e+01  0.00000000e+00 -3.00020000e+04  7.88500000e+03\n",
      "  -5.62658929e+00]\n",
      " [ 7.90000000e+01  0.00000000e+00 -4.00000000e+04  7.44400000e+03\n",
      "  -5.37384130e+00]\n",
      " [ 8.00000000e+01  0.00000000e+00 -3.00010000e+04  1.00210000e+04\n",
      "  -4.64131717e+00]\n",
      " [ 8.10000000e+01  0.00000000e+00 -2.00020000e+04  1.58480000e+04\n",
      "  -5.27055019e+00]\n",
      " [ 8.20000000e+01  0.00000000e+00 -2.00030000e+04  9.77700000e+03\n",
      "  -2.39326756e+00]\n",
      " [ 8.30000000e+01  0.00000000e+00 -3.00020000e+04  1.26890000e+04\n",
      "  -5.08824409e+00]\n",
      " [ 8.40000000e+01  0.00000000e+00 -2.00020000e+04  9.68100000e+03\n",
      "  -5.47933896e+00]\n",
      " [ 8.50000000e+01  0.00000000e+00 -3.00020000e+04  8.01300000e+03\n",
      "  -5.31367053e+00]\n",
      " [ 8.60000000e+01  0.00000000e+00 -1.00030000e+04  1.51330000e+04\n",
      "  -5.56471521e+00]\n",
      " [ 8.70000000e+01  0.00000000e+00 -2.00010000e+04  7.64100000e+03\n",
      "  -5.20333981e+00]\n",
      " [ 8.80000000e+01  0.00000000e+00 -2.00030000e+04  1.43690000e+04\n",
      "  -5.11666456e+00]\n",
      " [ 8.90000000e+01  0.00000000e+00 -1.00030000e+04  1.30850000e+04\n",
      "  -5.42160607e+00]\n",
      " [ 9.00000000e+01  0.00000000e+00 -1.00030000e+04  1.45730000e+04\n",
      "  -5.29648418e+00]\n",
      " [ 9.10000000e+01  0.00000000e+00 -2.00020000e+04  9.84000000e+03\n",
      "  -5.51198212e+00]\n",
      " [ 9.20000000e+01  0.00000000e+00 -4.00010000e+04  2.21130000e+04\n",
      "  -5.14657175e+00]\n",
      " [ 9.30000000e+01  0.00000000e+00 -2.00000000e+04  8.81700000e+03\n",
      "  -5.15869011e+00]\n",
      " [ 9.40000000e+01  0.00000000e+00 -4.00000000e+04  1.90810000e+04\n",
      "  -5.33372304e+00]\n",
      " [ 9.50000000e+01  0.00000000e+00 -4.00010000e+04  6.40100000e+03\n",
      "  -5.26800265e+00]\n",
      " [ 9.60000000e+01  0.00000000e+00 -2.00030000e+04  3.26610000e+04\n",
      "  -5.04957948e+00]\n",
      " [ 9.70000000e+01  0.00000000e+00 -3.00000000e+04  1.39770000e+04\n",
      "  -5.64591001e+00]\n",
      " [ 9.80000000e+01  0.00000000e+00 -4.00000000e+04  1.27140000e+04\n",
      "  -5.24423228e+00]\n",
      " [ 9.90000000e+01  0.00000000e+00 -1.00030000e+04  5.02900000e+03\n",
      "  -5.31057634e+00]\n",
      " [ 0.00000000e+00  1.00000000e+00 -3.00000000e+04  8.74400000e+03\n",
      "  -5.36040731e+00]\n",
      " [ 1.00000000e+00  1.00000000e+00 -1.00010000e+04  1.00600000e+04\n",
      "  -2.47820101e+00]\n",
      " [ 2.00000000e+00  1.00000000e+00 -2.00030000e+04  9.48500000e+03\n",
      "  -5.22561460e+00]\n",
      " [ 3.00000000e+00  1.00000000e+00 -2.00010000e+04  1.13130000e+04\n",
      "  -5.12064879e+00]\n",
      " [ 4.00000000e+00  1.00000000e+00 -2.00020000e+04  1.18520000e+04\n",
      "  -5.13841110e+00]\n",
      " [ 5.00000000e+00  1.00000000e+00 -3.00000000e+00  9.51600000e+03\n",
      "  -5.27068933e+00]\n",
      " [ 6.00000000e+00  1.00000000e+00 -2.00020000e+04  1.22250000e+04\n",
      "  -5.43327102e+00]\n",
      " [ 7.00000000e+00  1.00000000e+00 -1.00020000e+04  7.80500000e+03\n",
      "  -5.41934314e+00]\n",
      " [ 8.00000000e+00  1.00000000e+00 -2.00010000e+04  6.96100000e+03\n",
      "  -5.15562605e+00]\n",
      " [ 9.00000000e+00  1.00000000e+00 -2.00000000e+00  1.36130000e+04\n",
      "  -5.46887897e+00]\n",
      " [ 1.00000000e+01  1.00000000e+00 -2.00030000e+04  9.60100000e+03\n",
      "  -5.33599813e+00]\n",
      " [ 1.10000000e+01  1.00000000e+00 -1.00020000e+04  1.61890000e+04\n",
      "  -5.47865344e+00]\n",
      " [ 1.20000000e+01  1.00000000e+00 -1.00030000e+04  9.71700000e+03\n",
      "  -5.38912776e+00]\n",
      " [ 1.30000000e+01  1.00000000e+00 -3.00010000e+04  1.45330000e+04\n",
      "  -5.57642224e+00]\n",
      " [ 1.40000000e+01  1.00000000e+00 -3.00020000e+04  9.87700000e+03\n",
      "  -5.42365791e+00]\n",
      " [ 1.50000000e+01  1.00000000e+00 -2.00030000e+04  1.78570000e+04\n",
      "  -5.22397536e+00]\n",
      " [ 1.60000000e+01  1.00000000e+00 -2.00010000e+04  7.16700000e+03\n",
      "  -5.52272629e+00]\n",
      " [ 1.70000000e+01  1.00000000e+00 -3.00010000e+04  1.73010000e+04\n",
      "  -5.26081202e+00]\n",
      " [ 1.80000000e+01  1.00000000e+00 -3.00020000e+04  7.00900000e+03\n",
      "  -5.16113982e+00]\n",
      " [ 1.90000000e+01  1.00000000e+00 -2.00030000e+04  5.09400000e+03\n",
      "  -2.52315501e+00]\n",
      " [ 2.00000000e+01  1.00000000e+00 -1.00020000e+04  1.26940000e+04\n",
      "  -4.92208528e+00]\n",
      " [ 2.10000000e+01  1.00000000e+00 -2.00020000e+04  9.49700000e+03\n",
      "  -4.69488463e+00]\n",
      " [ 2.20000000e+01  1.00000000e+00 -3.00010000e+04  7.70100000e+03\n",
      "  -4.95229042e+00]\n",
      " [ 2.30000000e+01  1.00000000e+00 -4.00010000e+04  9.40100000e+03\n",
      "  -4.91920899e+00]\n",
      " [ 2.40000000e+01  1.00000000e+00 -3.00010000e+04  9.63700000e+03\n",
      "  -4.65498106e+00]\n",
      " [ 2.50000000e+01  1.00000000e+00 -4.00010000e+04  8.92100000e+03\n",
      "  -5.26430869e+00]\n",
      " [ 2.60000000e+01  1.00000000e+00 -3.00020000e+04  7.71700000e+03\n",
      "  -5.02842460e+00]\n",
      " [ 2.70000000e+01  1.00000000e+00 -5.00000000e+04  1.36210000e+04\n",
      "  -5.09242690e+00]\n",
      " [ 2.80000000e+01  1.00000000e+00 -2.00020000e+04  1.23850000e+04\n",
      "  -4.94005201e+00]\n",
      " [ 2.90000000e+01  1.00000000e+00 -1.00030000e+04  1.03850000e+04\n",
      "  -5.12120785e+00]\n",
      " [ 3.00000000e+01  1.00000000e+00 -1.00030000e+04  1.77170000e+04\n",
      "  -5.57359249e+00]\n",
      " [ 3.10000000e+01  1.00000000e+00 -1.00020000e+04  1.46270000e+04\n",
      "  -5.07660153e+00]\n",
      " [ 3.20000000e+01  1.00000000e+00 -1.00040000e+04  9.28500000e+03\n",
      "  -4.98657361e+00]\n",
      " [ 3.30000000e+01  1.00000000e+00 -3.00020000e+04  1.19410000e+04\n",
      "  -5.59216260e+00]\n",
      " [ 3.40000000e+01  1.00000000e+00 -1.00040000e+04  1.78130000e+04\n",
      "  -5.48392113e+00]\n",
      " [ 3.50000000e+01  1.00000000e+00 -2.00030000e+04  7.99300000e+03\n",
      "  -5.14183046e+00]\n",
      " [ 3.60000000e+01  1.00000000e+00 -2.00020000e+04  1.47410000e+04\n",
      "  -5.61821553e+00]\n",
      " [ 3.70000000e+01  1.00000000e+00 -2.00020000e+04  1.17930000e+04\n",
      "  -5.37045303e+00]\n",
      " [ 3.80000000e+01  1.00000000e+00 -2.00000000e+04  1.75160000e+04\n",
      "  -2.50408758e+00]\n",
      " [ 3.90000000e+01  1.00000000e+00 -2.00030000e+04  1.04970000e+04\n",
      "  -5.23819731e+00]\n",
      " [ 4.00000000e+01  1.00000000e+00 -1.00010000e+04  1.98900000e+04\n",
      "  -5.07404530e+00]\n",
      " [ 4.10000000e+01  1.00000000e+00 -2.00030000e+04  7.16900000e+03\n",
      "  -5.66767893e+00]\n",
      " [ 4.20000000e+01  1.00000000e+00 -4.00000000e+04  7.88100000e+03\n",
      "  -5.38516715e+00]\n",
      " [ 4.30000000e+01  1.00000000e+00 -4.00010000e+04  1.86410000e+04\n",
      "  -4.88832557e+00]\n",
      " [ 4.40000000e+01  1.00000000e+00 -3.00020000e+04  3.32410000e+04\n",
      "  -5.12368745e+00]\n",
      " [ 4.50000000e+01  1.00000000e+00 -2.00030000e+04  1.28900000e+04\n",
      "  -5.46436505e+00]\n",
      " [ 4.60000000e+01  1.00000000e+00 -2.00010000e+04  1.41120000e+04\n",
      "  -4.66490358e+00]\n",
      " [ 4.70000000e+01  1.00000000e+00 -2.00030000e+04  8.41700000e+03\n",
      "  -4.70255533e+00]\n",
      " [ 4.80000000e+01  1.00000000e+00 -4.00010000e+04  5.45700000e+03\n",
      "  -5.17398438e+00]\n",
      " [ 4.90000000e+01  1.00000000e+00 -2.00020000e+04  1.03410000e+04\n",
      "  -4.93175513e+00]\n",
      " [ 5.00000000e+01  1.00000000e+00 -1.00010000e+04  1.04690000e+04\n",
      "  -4.87474729e+00]\n",
      " [ 5.10000000e+01  1.00000000e+00 -3.00020000e+04  1.55650000e+04\n",
      "  -4.85913552e+00]\n",
      " [ 5.20000000e+01  1.00000000e+00 -3.00010000e+04  9.95700000e+03\n",
      "  -5.08484876e+00]\n",
      " [ 5.30000000e+01  1.00000000e+00 -2.00010000e+04  1.32610000e+04\n",
      "  -4.09540466e+00]\n",
      " [ 5.40000000e+01  1.00000000e+00 -4.00010000e+04  9.49700000e+03\n",
      "  -5.46250898e+00]\n",
      " [ 5.50000000e+01  1.00000000e+00 -5.00000000e+00  9.96500000e+03\n",
      "  -4.96526130e+00]\n",
      " [ 5.60000000e+01  1.00000000e+00 -3.00020000e+04  6.62900000e+03\n",
      "  -2.60267741e+00]\n",
      " [ 5.70000000e+01  1.00000000e+00 -2.00020000e+04  1.36610000e+04\n",
      "  -5.04025299e+00]\n",
      " [ 5.80000000e+01  1.00000000e+00 -1.00030000e+04  8.27300000e+03\n",
      "  -5.39509824e+00]\n",
      " [ 5.90000000e+01  1.00000000e+00 -3.00000000e+00  1.60910000e+04\n",
      "  -5.30192494e+00]\n",
      " [ 6.00000000e+01  1.00000000e+00 -4.00000000e+04  7.10500000e+03\n",
      "  -5.62546432e+00]\n",
      " [ 6.10000000e+01  1.00000000e+00 -2.00020000e+04  2.07410000e+04\n",
      "  -5.37497328e+00]\n",
      " [ 6.20000000e+01  1.00000000e+00 -2.00020000e+04  8.68900000e+03\n",
      "  -5.19229999e+00]\n",
      " [ 6.30000000e+01  1.00000000e+00 -3.00020000e+04  1.66250000e+04\n",
      "  -5.31867042e+00]\n",
      " [ 6.40000000e+01  1.00000000e+00 -1.00020000e+04  8.34500000e+03\n",
      "  -5.41461830e+00]\n",
      " [ 6.50000000e+01  1.00000000e+00 -2.00030000e+04  2.99580000e+04\n",
      "  -5.13294007e+00]\n",
      " [ 6.60000000e+01  1.00000000e+00 -1.00020000e+04  1.26880000e+04\n",
      "  -5.84021069e+00]\n",
      " [ 6.70000000e+01  1.00000000e+00 -3.00020000e+04  1.27100000e+04\n",
      "  -5.87779968e+00]\n",
      " [ 6.80000000e+01  1.00000000e+00 -4.00010000e+04  1.27140000e+04\n",
      "  -5.84613079e+00]\n",
      " [ 6.90000000e+01  1.00000000e+00 -2.00010000e+04  1.30070000e+04\n",
      "  -5.01885965e+00]\n",
      " [ 7.00000000e+01  1.00000000e+00 -2.00010000e+04  1.64330000e+04\n",
      "  -5.01299938e+00]\n",
      " [ 7.10000000e+01  1.00000000e+00 -1.00020000e+04  1.29640000e+04\n",
      "  -5.52553520e+00]\n",
      " [ 7.20000000e+01  1.00000000e+00 -2.00030000e+04  1.20650000e+04\n",
      "  -4.98762153e+00]\n",
      " [ 7.30000000e+01  1.00000000e+00 -2.00030000e+04  1.27060000e+04\n",
      "  -5.31023850e+00]\n",
      " [ 7.40000000e+01  1.00000000e+00 -1.00030000e+04  1.26900000e+04\n",
      "  -5.21599490e+00]\n",
      " [ 7.50000000e+01  1.00000000e+00 -4.00000000e+04  1.17770000e+04\n",
      "  -2.51765899e+00]\n",
      " [ 7.60000000e+01  1.00000000e+00 -2.00010000e+04  1.14730000e+04\n",
      "  -5.50832817e+00]\n",
      " [ 7.70000000e+01  1.00000000e+00 -1.00020000e+04  9.45100000e+03\n",
      "  -5.50809289e+00]\n",
      " [ 7.80000000e+01  1.00000000e+00 -3.00000000e+00  1.75890000e+04\n",
      "  -5.51748818e+00]\n",
      " [ 7.90000000e+01  1.00000000e+00 -1.00040000e+04  1.12650000e+04\n",
      "  -5.59986878e+00]\n",
      " [ 8.00000000e+01  1.00000000e+00 -4.00010000e+04  5.94500000e+03\n",
      "  -5.39704371e+00]\n",
      " [ 8.10000000e+01  1.00000000e+00 -3.00010000e+04  1.78770000e+04\n",
      "  -5.45027795e+00]\n",
      " [ 8.20000000e+01  1.00000000e+00 -2.00030000e+04  9.09700000e+03\n",
      "  -5.26837280e+00]\n",
      " [ 8.30000000e+01  1.00000000e+00 -1.00030000e+04  1.71840000e+04\n",
      "  -4.82386617e+00]\n",
      " [ 8.40000000e+01  1.00000000e+00 -2.00030000e+04  1.23810000e+04\n",
      "  -5.46481553e+00]\n",
      " [ 8.50000000e+01  1.00000000e+00 -2.00000000e+00  1.03150000e+04\n",
      "  -5.30999725e+00]\n",
      " [ 8.60000000e+01  1.00000000e+00 -3.00010000e+04  1.49840000e+04\n",
      "  -5.62460490e+00]\n",
      " [ 8.70000000e+01  1.00000000e+00 -2.00020000e+04  4.44500000e+03\n",
      "  -5.42663515e+00]\n",
      " [ 8.80000000e+01  1.00000000e+00 -2.00020000e+04  1.96850000e+04\n",
      "  -5.48506996e+00]\n",
      " [ 8.90000000e+01  1.00000000e+00 -4.00010000e+04  8.06900000e+03\n",
      "  -5.08292472e+00]\n",
      " [ 9.00000000e+01  1.00000000e+00 -3.00010000e+04  8.44800000e+03\n",
      "  -5.15488216e+00]\n",
      " [ 9.10000000e+01  1.00000000e+00 -4.00010000e+04  1.25610000e+04\n",
      "  -5.49430843e+00]\n",
      " [ 9.20000000e+01  1.00000000e+00 -1.00030000e+04  2.60610000e+04\n",
      "  -4.98117675e+00]\n",
      " [ 9.30000000e+01  1.00000000e+00 -1.00030000e+04  1.03850000e+04\n",
      "  -5.13621910e+00]\n",
      " [ 9.40000000e+01  1.00000000e+00 -2.00020000e+04  1.10610000e+04\n",
      "  -5.03214479e+00]\n",
      " [ 9.50000000e+01  1.00000000e+00 -2.00010000e+04  1.01200000e+04\n",
      "  -4.57095413e+00]\n",
      " [ 9.60000000e+01  1.00000000e+00 -2.00020000e+04  9.74900000e+03\n",
      "  -4.99467110e+00]\n",
      " [ 9.70000000e+01  1.00000000e+00 -4.00000000e+04  1.15850000e+04\n",
      "  -5.03104917e+00]\n",
      " [ 9.80000000e+01  1.00000000e+00 -3.00010000e+04  1.05200000e+04\n",
      "  -4.64505679e+00]\n",
      " [ 9.90000000e+01  1.00000000e+00 -4.00010000e+04  1.21210000e+04\n",
      "  -5.39331384e+00]]\n",
      "Writing to file data.csv\n",
      "**************************************************\n",
      "Data save complete\n",
      "**************************************************\n",
      "closest_car\n",
      "**************************************************\n",
      "Running experiment\n",
      "**************************************************\n",
      "Episode : 0\n",
      "state : [2 2 3 3 2 2]\n",
      "action : 2\n",
      "new state: [2 2 3 3 0 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 2 3 3 0 0]\n",
      "action : 0\n",
      "new state: [2 2 3 3 3 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 2 3 3 3 1]\n",
      "action : 3\n",
      "new state: [2 3 3 2 0 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 3 3 2 0 1]\n",
      "action : 0\n",
      "new state: [1 4 3 2 1 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 4 3 2 1 0]\n",
      "action : 1\n",
      "new state: [2 3 3 2 2 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "final state: [2 3 3 2 2 0]\n",
      "Episode : 1\n",
      "state : [2 3 3 2 2 0]\n",
      "action : 2\n",
      "new state: [3 3 2 2 2 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 3 2 2 2 0]\n",
      "action : 2\n",
      "new state: [4 3 1 2 2 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [4 3 1 2 2 0]\n",
      "action : 2\n",
      "new state: [5 3 0 2 2 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [5 3 0 2 2 0]\n",
      "action : 0\n",
      "new state: [5 3 0 2 1 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [5 3 0 2 1 1]\n",
      "action : 1\n",
      "new state: [5 3 0 2 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "final state: [5 3 0 2 0 2]\n",
      "Episode : 2\n",
      "state : [5 3 0 2 0 2]\n",
      "action : 0\n",
      "new state: [5 3 0 2 1 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [5 3 0 2 1 3]\n",
      "action : 1\n",
      "new state: [5 2 0 3 2 1]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [5 2 0 3 2 1]\n",
      "action : 0\n",
      "new state: [4 3 0 3 0 1]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [4 3 0 3 0 1]\n",
      "action : 0\n",
      "new state: [3 4 0 3 1 3]\n",
      "reward: -0.0\n",
      "epReward so far: -10001.0\n",
      "state : [3 4 0 3 1 3]\n",
      "action : 1\n",
      "new state: [3 3 0 4 1 0]\n",
      "reward: -0.0\n",
      "epReward so far: -10001.0\n",
      "final state: [3 3 0 4 1 0]\n",
      "Episode : 3\n",
      "state : [3 3 0 4 1 0]\n",
      "action : 1\n",
      "new state: [4 2 0 4 3 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [4 2 0 4 3 2]\n",
      "action : 3\n",
      "new state: [4 2 0 4 1 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [4 2 0 4 1 3]\n",
      "action : 1\n",
      "new state: [4 1 0 5 1 3]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [4 1 0 5 1 3]\n",
      "action : 1\n",
      "new state: [4 0 0 6 0 0]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [4 0 0 6 0 0]\n",
      "action : 0\n",
      "new state: [4 0 0 6 1 2]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "final state: [4 0 0 6 1 2]\n",
      "Episode : 4\n",
      "state : [4 0 0 6 1 2]\n",
      "action : 0\n",
      "new state: [3 0 1 6 0 1]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [3 0 1 6 0 1]\n",
      "action : 0\n",
      "new state: [2 1 1 6 3 3]\n",
      "reward: -0.0\n",
      "epReward so far: -1.0\n",
      "state : [2 1 1 6 3 3]\n",
      "action : 3\n",
      "new state: [2 1 1 6 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: -1.0\n",
      "state : [2 1 1 6 0 2]\n",
      "action : 0\n",
      "new state: [2 1 1 6 3 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 1 1 6 3 2]\n",
      "action : 3\n",
      "new state: [2 1 2 5 0 3]\n",
      "reward: -0.0\n",
      "epReward so far: -10001.0\n",
      "final state: [2 1 2 5 0 3]\n",
      "Episode : 5\n",
      "state : [2 1 2 5 0 3]\n",
      "action : 0\n",
      "new state: [1 1 2 6 3 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 1 2 6 3 2]\n",
      "action : 3\n",
      "new state: [1 1 3 5 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 1 3 5 2 2]\n",
      "action : 2\n",
      "new state: [1 1 3 5 3 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 1 3 5 3 3]\n",
      "action : 3\n",
      "new state: [1 1 3 5 3 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 1 3 5 3 0]\n",
      "action : 3\n",
      "new state: [2 1 3 4 2 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "final state: [2 1 3 4 2 0]\n",
      "Episode : 6\n",
      "state : [2 1 3 4 2 0]\n",
      "action : 2\n",
      "new state: [3 1 2 4 3 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 1 2 4 3 1]\n",
      "action : 3\n",
      "new state: [3 2 2 3 3 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 2 2 3 3 0]\n",
      "action : 3\n",
      "new state: [4 2 2 2 1 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [4 2 2 2 1 2]\n",
      "action : 1\n",
      "new state: [4 1 3 2 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [4 1 3 2 2 2]\n",
      "action : 2\n",
      "new state: [4 1 3 2 1 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "final state: [4 1 3 2 1 3]\n",
      "Episode : 7\n",
      "state : [4 1 3 2 1 3]\n",
      "action : 1\n",
      "new state: [4 1 3 2 0 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [4 1 3 2 0 3]\n",
      "action : 0\n",
      "new state: [3 1 3 3 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [3 1 3 3 0 2]\n",
      "action : 0\n",
      "new state: [2 1 4 3 3 1]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 1 4 3 3 1]\n",
      "action : 3\n",
      "new state: [2 1 4 3 0 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [2 1 4 3 0 0]\n",
      "action : 0\n",
      "new state: [2 1 4 3 1 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -30000.0\n",
      "final state: [2 1 4 3 1 2]\n",
      "Episode : 8\n",
      "state : [2 1 4 3 1 2]\n",
      "action : 1\n",
      "new state: [2 1 4 3 1 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 1 4 3 1 3]\n",
      "action : 1\n",
      "new state: [2 1 4 3 2 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [2 1 4 3 2 0]\n",
      "action : 2\n",
      "new state: [3 1 3 3 0 1]\n",
      "reward: -0.0\n",
      "epReward so far: -20000.0\n",
      "state : [3 1 3 3 0 1]\n",
      "action : 0\n",
      "new state: [2 2 3 3 1 0]\n",
      "reward: -0.0\n",
      "epReward so far: -20000.0\n",
      "state : [2 2 3 3 1 0]\n",
      "action : 1\n",
      "new state: [3 1 3 3 1 2]\n",
      "reward: -0.0\n",
      "epReward so far: -20000.0\n",
      "final state: [3 1 3 3 1 2]\n",
      "Episode : 9\n",
      "state : [3 1 3 3 1 2]\n",
      "action : 1\n",
      "new state: [3 0 4 3 3 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 0 4 3 3 2]\n",
      "action : 3\n",
      "new state: [3 0 5 2 0 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 0 5 2 0 1]\n",
      "action : 0\n",
      "new state: [2 1 5 2 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 1 5 2 2 2]\n",
      "action : 2\n",
      "new state: [2 1 5 2 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 1 5 2 0 2]\n",
      "action : 0\n",
      "new state: [1 1 6 2 1 2]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "final state: [1 1 6 2 1 2]\n",
      "Episode : 10\n",
      "state : [1 1 6 2 1 2]\n",
      "action : 1\n",
      "new state: [1 0 7 2 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 0 7 2 0 2]\n",
      "action : 0\n",
      "new state: [1 0 7 2 1 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [1 0 7 2 1 2]\n",
      "action : 0\n",
      "new state: [0 0 8 2 0 2]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [0 0 8 2 0 2]\n",
      "action : 2\n",
      "new state: [0 0 8 2 1 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "state : [0 0 8 2 1 0]\n",
      "action : 2\n",
      "new state: [1 0 7 2 3 0]\n",
      "reward: -1.0\n",
      "epReward so far: -20002.0\n",
      "final state: [1 0 7 2 3 0]\n",
      "Episode : 11\n",
      "state : [1 0 7 2 3 0]\n",
      "action : 3\n",
      "new state: [2 0 7 1 1 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 0 7 1 1 1]\n",
      "action : 0\n",
      "new state: [1 1 7 1 0 0]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [1 1 7 1 0 0]\n",
      "action : 0\n",
      "new state: [1 1 7 1 1 0]\n",
      "reward: -0.0\n",
      "epReward so far: -1.0\n",
      "state : [1 1 7 1 1 0]\n",
      "action : 1\n",
      "new state: [1 1 7 1 0 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "state : [1 1 7 1 0 3]\n",
      "action : 0\n",
      "new state: [0 1 7 2 3 1]\n",
      "reward: -0.0\n",
      "epReward so far: -10001.0\n",
      "final state: [0 1 7 2 3 1]\n",
      "Episode : 12\n",
      "state : [0 1 7 2 3 1]\n",
      "action : 3\n",
      "new state: [0 2 7 1 1 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 2 7 1 1 0]\n",
      "action : 1\n",
      "new state: [1 1 7 1 1 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 1 7 1 1 3]\n",
      "action : 1\n",
      "new state: [1 1 7 1 3 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [1 1 7 1 3 1]\n",
      "action : 3\n",
      "new state: [1 2 7 0 1 3]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [1 2 7 0 1 3]\n",
      "action : 1\n",
      "new state: [1 1 7 1 2 1]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "final state: [1 1 7 1 2 1]\n",
      "Episode : 13\n",
      "state : [1 1 7 1 2 1]\n",
      "action : 2\n",
      "new state: [1 2 6 1 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 2 6 1 2 2]\n",
      "action : 2\n",
      "new state: [1 2 6 1 1 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 2 6 1 1 3]\n",
      "action : 1\n",
      "new state: [1 2 6 1 1 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [1 2 6 1 1 1]\n",
      "action : 1\n",
      "new state: [1 2 6 1 0 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [1 2 6 1 0 3]\n",
      "action : 0\n",
      "new state: [1 2 6 1 0 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -30000.0\n",
      "final state: [1 2 6 1 0 1]\n",
      "Episode : 14\n",
      "state : [1 2 6 1 0 1]\n",
      "action : 0\n",
      "new state: [0 3 6 1 2 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 3 6 1 2 3]\n",
      "action : 2\n",
      "new state: [0 3 5 2 2 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 3 5 2 2 0]\n",
      "action : 2\n",
      "new state: [1 3 4 2 0 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 3 4 2 0 0]\n",
      "action : 0\n",
      "new state: [1 3 4 2 2 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 3 4 2 2 3]\n",
      "action : 2\n",
      "new state: [1 3 3 3 0 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "final state: [1 3 3 3 0 3]\n",
      "Episode : 15\n",
      "state : [1 3 3 3 0 3]\n",
      "action : 0\n",
      "new state: [0 3 3 4 2 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 3 3 4 2 0]\n",
      "action : 2\n",
      "new state: [1 3 2 4 2 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 3 2 4 2 3]\n",
      "action : 2\n",
      "new state: [1 3 1 5 3 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 3 1 5 3 3]\n",
      "action : 3\n",
      "new state: [1 3 1 5 2 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 3 1 5 2 3]\n",
      "action : 2\n",
      "new state: [1 3 1 5 3 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "final state: [1 3 1 5 3 2]\n",
      "Episode : 16\n",
      "state : [1 3 1 5 3 2]\n",
      "action : 3\n",
      "new state: [1 3 2 4 2 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 3 2 4 2 0]\n",
      "action : 2\n",
      "new state: [2 3 1 4 3 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 3 1 4 3 3]\n",
      "action : 3\n",
      "new state: [2 3 1 4 1 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 3 1 4 1 2]\n",
      "action : 1\n",
      "new state: [2 2 2 4 3 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 2 2 4 3 1]\n",
      "action : 3\n",
      "new state: [2 3 2 3 1 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "final state: [2 3 2 3 1 3]\n",
      "Episode : 17\n",
      "state : [2 3 2 3 1 3]\n",
      "action : 1\n",
      "new state: [2 2 2 4 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 2 2 4 2 2]\n",
      "action : 2\n",
      "new state: [2 2 2 4 2 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 2 2 4 2 1]\n",
      "action : 2\n",
      "new state: [2 3 1 4 0 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 3 1 4 0 0]\n",
      "action : 0\n",
      "new state: [2 3 1 4 3 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 3 1 4 3 3]\n",
      "action : 3\n",
      "new state: [2 3 1 4 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "final state: [2 3 1 4 2 2]\n",
      "Episode : 18\n",
      "state : [2 3 1 4 2 2]\n",
      "action : 2\n",
      "new state: [2 3 1 4 1 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 3 1 4 1 2]\n",
      "action : 1\n",
      "new state: [2 3 1 4 0 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 3 1 4 0 0]\n",
      "action : 0\n",
      "new state: [2 3 1 4 0 1]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 3 1 4 0 1]\n",
      "action : 0\n",
      "new state: [1 4 1 4 2 3]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [1 4 1 4 2 3]\n",
      "action : 2\n",
      "new state: [1 4 0 5 1 0]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "final state: [1 4 0 5 1 0]\n",
      "Episode : 19\n",
      "state : [1 4 0 5 1 0]\n",
      "action : 1\n",
      "new state: [2 3 0 5 3 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 3 0 5 3 2]\n",
      "action : 3\n",
      "new state: [2 3 0 5 3 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 3 0 5 3 2]\n",
      "action : 3\n",
      "new state: [2 3 0 5 2 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [2 3 0 5 2 0]\n",
      "action : 0\n",
      "new state: [2 3 0 5 0 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -30000.0\n",
      "state : [2 3 0 5 0 0]\n",
      "action : 0\n",
      "new state: [2 3 0 5 3 3]\n",
      "reward: -0.0\n",
      "epReward so far: -30000.0\n",
      "final state: [2 3 0 5 3 3]\n",
      "Episode : 20\n",
      "state : [2 3 0 5 3 3]\n",
      "action : 3\n",
      "new state: [2 3 0 5 1 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 3 0 5 1 3]\n",
      "action : 1\n",
      "new state: [2 3 0 5 0 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [2 3 0 5 0 1]\n",
      "action : 0\n",
      "new state: [1 4 0 5 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: -20000.0\n",
      "state : [1 4 0 5 2 2]\n",
      "action : 0\n",
      "new state: [0 4 1 5 1 2]\n",
      "reward: -1.0\n",
      "epReward so far: -20001.0\n",
      "state : [0 4 1 5 1 2]\n",
      "action : 1\n",
      "new state: [0 4 1 5 0 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -30001.0\n",
      "final state: [0 4 1 5 0 0]\n",
      "Episode : 21\n",
      "state : [0 4 1 5 0 0]\n",
      "action : 1\n",
      "new state: [0 4 1 5 2 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 4 1 5 2 0]\n",
      "action : 2\n",
      "new state: [1 4 0 5 3 1]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [1 4 0 5 3 1]\n",
      "action : 3\n",
      "new state: [1 5 0 4 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [1 5 0 4 2 2]\n",
      "action : 0\n",
      "new state: [1 5 0 4 1 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [1 5 0 4 1 1]\n",
      "action : 1\n",
      "new state: [1 5 0 4 3 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -30000.0\n",
      "final state: [1 5 0 4 3 2]\n",
      "Episode : 22\n",
      "state : [1 5 0 4 3 2]\n",
      "action : 3\n",
      "new state: [1 5 1 3 1 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 5 1 3 1 0]\n",
      "action : 1\n",
      "new state: [2 4 1 3 3 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 4 1 3 3 1]\n",
      "action : 3\n",
      "new state: [2 5 1 2 2 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 5 1 2 2 0]\n",
      "action : 2\n",
      "new state: [3 5 0 2 2 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 5 0 2 2 3]\n",
      "action : 0\n",
      "new state: [2 5 0 3 2 0]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "final state: [2 5 0 3 2 0]\n",
      "Episode : 23\n",
      "state : [2 5 0 3 2 0]\n",
      "action : 0\n",
      "new state: [2 5 0 3 0 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 5 0 3 0 3]\n",
      "action : 0\n",
      "new state: [2 5 0 3 3 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [2 5 0 3 3 0]\n",
      "action : 3\n",
      "new state: [3 5 0 2 1 0]\n",
      "reward: -0.0\n",
      "epReward so far: -20000.0\n",
      "state : [3 5 0 2 1 0]\n",
      "action : 1\n",
      "new state: [4 4 0 2 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: -20000.0\n",
      "state : [4 4 0 2 2 2]\n",
      "action : 0\n",
      "new state: [4 4 0 2 2 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -30000.0\n",
      "final state: [4 4 0 2 2 3]\n",
      "Episode : 24\n",
      "state : [4 4 0 2 2 3]\n",
      "action : 0\n",
      "new state: [4 4 0 2 1 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [4 4 0 2 1 1]\n",
      "action : 1\n",
      "new state: [4 4 0 2 2 1]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [4 4 0 2 2 1]\n",
      "action : 0\n",
      "new state: [3 5 0 2 2 2]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [3 5 0 2 2 2]\n",
      "action : 0\n",
      "new state: [3 5 0 2 0 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "state : [3 5 0 2 0 0]\n",
      "action : 0\n",
      "new state: [3 5 0 2 3 2]\n",
      "reward: -0.0\n",
      "epReward so far: -20001.0\n",
      "final state: [3 5 0 2 3 2]\n",
      "Episode : 25\n",
      "state : [3 5 0 2 3 2]\n",
      "action : 3\n",
      "new state: [3 5 1 1 3 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 5 1 1 3 1]\n",
      "action : 3\n",
      "new state: [3 6 1 0 1 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 6 1 0 1 1]\n",
      "action : 1\n",
      "new state: [3 6 1 0 2 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 6 1 0 2 0]\n",
      "action : 2\n",
      "new state: [4 6 0 0 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [4 6 0 0 0 2]\n",
      "action : 0\n",
      "new state: [3 6 1 0 1 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "final state: [3 6 1 0 1 1]\n",
      "Episode : 26\n",
      "state : [3 6 1 0 1 1]\n",
      "action : 1\n",
      "new state: [3 6 1 0 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [3 6 1 0 0 2]\n",
      "action : 0\n",
      "new state: [3 6 1 0 3 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [3 6 1 0 3 3]\n",
      "action : 0\n",
      "new state: [2 6 1 1 1 3]\n",
      "reward: -1.0\n",
      "epReward so far: -20001.0\n",
      "state : [2 6 1 1 1 3]\n",
      "action : 1\n",
      "new state: [2 5 1 2 1 2]\n",
      "reward: -0.0\n",
      "epReward so far: -20001.0\n",
      "state : [2 5 1 2 1 2]\n",
      "action : 1\n",
      "new state: [2 4 2 2 2 3]\n",
      "reward: -0.0\n",
      "epReward so far: -20001.0\n",
      "final state: [2 4 2 2 2 3]\n",
      "Episode : 27\n",
      "state : [2 4 2 2 2 3]\n",
      "action : 2\n",
      "new state: [2 4 1 3 2 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 4 1 3 2 0]\n",
      "action : 2\n",
      "new state: [3 4 0 3 3 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 4 0 3 3 0]\n",
      "action : 3\n",
      "new state: [3 4 0 3 1 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [3 4 0 3 1 1]\n",
      "action : 1\n",
      "new state: [3 4 0 3 3 2]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [3 4 0 3 3 2]\n",
      "action : 3\n",
      "new state: [3 4 1 2 2 0]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "final state: [3 4 1 2 2 0]\n",
      "Episode : 28\n",
      "state : [3 4 1 2 2 0]\n",
      "action : 2\n",
      "new state: [3 4 1 2 3 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [3 4 1 2 3 3]\n",
      "action : 3\n",
      "new state: [3 4 1 2 2 3]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [3 4 1 2 2 3]\n",
      "action : 2\n",
      "new state: [3 4 0 3 2 0]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [3 4 0 3 2 0]\n",
      "action : 0\n",
      "new state: [3 4 0 3 2 0]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [3 4 0 3 2 0]\n",
      "action : 0\n",
      "new state: [3 4 0 3 0 1]\n",
      "reward: -1.0\n",
      "epReward so far: -10002.0\n",
      "final state: [3 4 0 3 0 1]\n",
      "Episode : 29\n",
      "state : [3 4 0 3 0 1]\n",
      "action : 0\n",
      "new state: [2 5 0 3 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 5 0 3 2 2]\n",
      "action : 0\n",
      "new state: [1 5 1 3 2 3]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [1 5 1 3 2 3]\n",
      "action : 2\n",
      "new state: [1 5 0 4 3 1]\n",
      "reward: -0.0\n",
      "epReward so far: -1.0\n",
      "state : [1 5 0 4 3 1]\n",
      "action : 3\n",
      "new state: [1 6 0 3 3 2]\n",
      "reward: -0.0\n",
      "epReward so far: -1.0\n",
      "state : [1 6 0 3 3 2]\n",
      "action : 3\n",
      "new state: [1 6 0 3 2 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "final state: [1 6 0 3 2 0]\n",
      "Episode : 30\n",
      "state : [1 6 0 3 2 0]\n",
      "action : 0\n",
      "new state: [1 6 0 3 2 3]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [1 6 0 3 2 3]\n",
      "action : 0\n",
      "new state: [0 6 0 4 0 3]\n",
      "reward: -1.0\n",
      "epReward so far: -2.0\n",
      "state : [0 6 0 4 0 3]\n",
      "action : 1\n",
      "new state: [0 6 0 4 2 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10002.0\n",
      "state : [0 6 0 4 2 1]\n",
      "action : 1\n",
      "new state: [0 6 0 4 1 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20002.0\n",
      "state : [0 6 0 4 1 3]\n",
      "action : 1\n",
      "new state: [0 5 0 5 1 1]\n",
      "reward: -0.0\n",
      "epReward so far: -20002.0\n",
      "final state: [0 5 0 5 1 1]\n",
      "Episode : 31\n",
      "state : [0 5 0 5 1 1]\n",
      "action : 1\n",
      "new state: [0 5 0 5 0 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 5 0 5 0 1]\n",
      "action : 1\n",
      "new state: [0 5 0 5 3 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [0 5 0 5 3 2]\n",
      "action : 3\n",
      "new state: [0 5 1 4 2 0]\n",
      "reward: -0.0\n",
      "epReward so far: -20000.0\n",
      "state : [0 5 1 4 2 0]\n",
      "action : 2\n",
      "new state: [1 5 0 4 3 0]\n",
      "reward: -0.0\n",
      "epReward so far: -20000.0\n",
      "state : [1 5 0 4 3 0]\n",
      "action : 3\n",
      "new state: [2 5 0 3 0 0]\n",
      "reward: -0.0\n",
      "epReward so far: -20000.0\n",
      "final state: [2 5 0 3 0 0]\n",
      "Episode : 32\n",
      "state : [2 5 0 3 0 0]\n",
      "action : 0\n",
      "new state: [2 5 0 3 3 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 5 0 3 3 2]\n",
      "action : 3\n",
      "new state: [2 5 1 2 2 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 5 1 2 2 0]\n",
      "action : 2\n",
      "new state: [3 5 0 2 0 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 5 0 2 0 0]\n",
      "action : 0\n",
      "new state: [3 5 0 2 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 5 0 2 0 2]\n",
      "action : 0\n",
      "new state: [2 5 1 2 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "final state: [2 5 1 2 2 2]\n",
      "Episode : 33\n",
      "state : [2 5 1 2 2 2]\n",
      "action : 2\n",
      "new state: [2 5 1 2 0 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 5 1 2 0 3]\n",
      "action : 0\n",
      "new state: [1 5 1 3 3 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 5 1 3 3 0]\n",
      "action : 3\n",
      "new state: [1 5 1 3 3 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [1 5 1 3 3 0]\n",
      "action : 3\n",
      "new state: [2 5 1 2 3 1]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 5 1 2 3 1]\n",
      "action : 3\n",
      "new state: [2 5 1 2 1 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "final state: [2 5 1 2 1 0]\n",
      "Episode : 34\n",
      "state : [2 5 1 2 1 0]\n",
      "action : 1\n",
      "new state: [2 5 1 2 3 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 5 1 2 3 2]\n",
      "action : 3\n",
      "new state: [2 5 1 2 2 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [2 5 1 2 2 2]\n",
      "action : 2\n",
      "new state: [2 5 1 2 2 0]\n",
      "reward: -0.0\n",
      "epReward so far: -20000.0\n",
      "state : [2 5 1 2 2 0]\n",
      "action : 2\n",
      "new state: [3 5 0 2 2 1]\n",
      "reward: -0.0\n",
      "epReward so far: -20000.0\n",
      "state : [3 5 0 2 2 1]\n",
      "action : 0\n",
      "new state: [2 6 0 2 3 1]\n",
      "reward: -1.0\n",
      "epReward so far: -20001.0\n",
      "final state: [2 6 0 2 3 1]\n",
      "Episode : 35\n",
      "state : [2 6 0 2 3 1]\n",
      "action : 3\n",
      "new state: [2 6 0 2 1 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 6 0 2 1 1]\n",
      "action : 1\n",
      "new state: [2 6 0 2 1 1]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 6 0 2 1 1]\n",
      "action : 1\n",
      "new state: [2 6 0 2 3 3]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 6 0 2 3 3]\n",
      "action : 3\n",
      "new state: [2 6 0 2 3 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [2 6 0 2 3 2]\n",
      "action : 3\n",
      "new state: [2 6 0 2 3 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -30000.0\n",
      "final state: [2 6 0 2 3 3]\n",
      "Episode : 36\n",
      "state : [2 6 0 2 3 3]\n",
      "action : 3\n",
      "new state: [2 6 0 2 3 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 6 0 2 3 3]\n",
      "action : 3\n",
      "new state: [2 6 0 2 1 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 6 0 2 1 0]\n",
      "action : 1\n",
      "new state: [2 6 0 2 2 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [2 6 0 2 2 2]\n",
      "action : 0\n",
      "new state: [1 6 1 2 0 0]\n",
      "reward: -1.0\n",
      "epReward so far: -20001.0\n",
      "state : [1 6 1 2 0 0]\n",
      "action : 0\n",
      "new state: [1 6 1 2 2 3]\n",
      "reward: -0.0\n",
      "epReward so far: -20001.0\n",
      "final state: [1 6 1 2 2 3]\n",
      "Episode : 37\n",
      "state : [1 6 1 2 2 3]\n",
      "action : 2\n",
      "new state: [1 6 0 3 2 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 6 0 3 2 0]\n",
      "action : 0\n",
      "new state: [1 6 0 3 3 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [1 6 0 3 3 0]\n",
      "action : 3\n",
      "new state: [2 6 0 2 3 3]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 6 0 2 3 3]\n",
      "action : 3\n",
      "new state: [2 6 0 2 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [2 6 0 2 0 2]\n",
      "action : 0\n",
      "new state: [1 6 1 2 3 3]\n",
      "reward: -0.0\n",
      "epReward so far: -20000.0\n",
      "final state: [1 6 1 2 3 3]\n",
      "Episode : 38\n",
      "state : [1 6 1 2 3 3]\n",
      "action : 3\n",
      "new state: [1 6 1 2 3 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 6 1 2 3 0]\n",
      "action : 3\n",
      "new state: [1 6 1 2 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [1 6 1 2 0 2]\n",
      "action : 0\n",
      "new state: [0 6 2 2 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 6 2 2 0 2]\n",
      "action : 1\n",
      "new state: [0 5 3 2 3 3]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [0 5 3 2 3 3]\n",
      "action : 3\n",
      "new state: [0 5 3 2 0 0]\n",
      "reward: -0.0\n",
      "epReward so far: -10001.0\n",
      "final state: [0 5 3 2 0 0]\n",
      "Episode : 39\n",
      "state : [0 5 3 2 0 0]\n",
      "action : 1\n",
      "new state: [0 5 3 2 2 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 5 3 2 2 2]\n",
      "action : 2\n",
      "new state: [0 5 3 2 2 0]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 5 3 2 2 0]\n",
      "action : 2\n",
      "new state: [1 5 2 2 1 2]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [1 5 2 2 1 2]\n",
      "action : 1\n",
      "new state: [1 4 3 2 1 1]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [1 4 3 2 1 1]\n",
      "action : 1\n",
      "new state: [1 4 3 2 2 3]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "final state: [1 4 3 2 2 3]\n",
      "Episode : 40\n",
      "state : [1 4 3 2 2 3]\n",
      "action : 2\n",
      "new state: [1 4 3 2 0 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [1 4 3 2 0 3]\n",
      "action : 0\n",
      "new state: [0 4 3 3 3 3]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 4 3 3 3 3]\n",
      "action : 3\n",
      "new state: [0 4 3 3 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 4 3 3 0 2]\n",
      "action : 1\n",
      "new state: [0 4 3 3 1 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [0 4 3 3 1 1]\n",
      "action : 1\n",
      "new state: [0 4 3 3 3 2]\n",
      "reward: -0.0\n",
      "epReward so far: -20000.0\n",
      "final state: [0 4 3 3 3 2]\n",
      "Episode : 41\n",
      "state : [0 4 3 3 3 2]\n",
      "action : 3\n",
      "new state: [0 4 4 2 3 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 4 4 2 3 3]\n",
      "action : 3\n",
      "new state: [0 4 4 2 1 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 4 4 2 1 3]\n",
      "action : 1\n",
      "new state: [0 4 4 2 0 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [0 4 4 2 0 0]\n",
      "action : 1\n",
      "new state: [1 3 4 2 0 2]\n",
      "reward: -1.0\n",
      "epReward so far: -20001.0\n",
      "state : [1 3 4 2 0 2]\n",
      "action : 0\n",
      "new state: [1 3 4 2 1 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -30001.0\n",
      "final state: [1 3 4 2 1 2]\n",
      "Episode : 42\n",
      "state : [1 3 4 2 1 2]\n",
      "action : 1\n",
      "new state: [1 2 5 2 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 2 5 2 0 2]\n",
      "action : 0\n",
      "new state: [0 2 6 2 0 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 2 6 2 0 1]\n",
      "action : 1\n",
      "new state: [0 2 6 2 1 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 2 6 2 1 2]\n",
      "action : 1\n",
      "new state: [0 1 7 2 1 0]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 1 7 2 1 0]\n",
      "action : 1\n",
      "new state: [1 0 7 2 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "final state: [1 0 7 2 2 2]\n",
      "Episode : 43\n",
      "state : [1 0 7 2 2 2]\n",
      "action : 2\n",
      "new state: [1 0 7 2 3 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 0 7 2 3 1]\n",
      "action : 3\n",
      "new state: [1 1 7 1 0 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 1 7 1 0 1]\n",
      "action : 0\n",
      "new state: [1 1 7 1 0 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [1 1 7 1 0 3]\n",
      "action : 0\n",
      "new state: [1 1 7 1 1 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [1 1 7 1 1 2]\n",
      "action : 1\n",
      "new state: [1 1 7 1 0 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -30000.0\n",
      "final state: [1 1 7 1 0 1]\n",
      "Episode : 44\n",
      "state : [1 1 7 1 0 1]\n",
      "action : 0\n",
      "new state: [0 2 7 1 3 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 2 7 1 3 2]\n",
      "action : 3\n",
      "new state: [0 2 8 0 2 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 2 8 0 2 0]\n",
      "action : 2\n",
      "new state: [0 2 8 0 1 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 2 8 0 1 0]\n",
      "action : 1\n",
      "new state: [0 2 8 0 3 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [0 2 8 0 3 1]\n",
      "action : 1\n",
      "new state: [0 2 8 0 1 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -30000.0\n",
      "final state: [0 2 8 0 1 1]\n",
      "Episode : 45\n",
      "state : [0 2 8 0 1 1]\n",
      "action : 1\n",
      "new state: [0 2 8 0 2 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 2 8 0 2 1]\n",
      "action : 2\n",
      "new state: [0 2 8 0 0 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 2 8 0 0 1]\n",
      "action : 1\n",
      "new state: [0 2 8 0 3 1]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [0 2 8 0 3 1]\n",
      "action : 1\n",
      "new state: [0 2 8 0 2 0]\n",
      "reward: -1.0\n",
      "epReward so far: -10002.0\n",
      "state : [0 2 8 0 2 0]\n",
      "action : 2\n",
      "new state: [1 2 7 0 3 3]\n",
      "reward: -0.0\n",
      "epReward so far: -10002.0\n",
      "final state: [1 2 7 0 3 3]\n",
      "Episode : 46\n",
      "state : [1 2 7 0 3 3]\n",
      "action : 0\n",
      "new state: [1 2 7 0 3 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [1 2 7 0 3 0]\n",
      "action : 0\n",
      "new state: [1 2 7 0 1 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [1 2 7 0 1 1]\n",
      "action : 1\n",
      "new state: [1 2 7 0 2 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -30000.0\n",
      "state : [1 2 7 0 2 0]\n",
      "action : 2\n",
      "new state: [1 2 7 0 0 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -40000.0\n",
      "state : [1 2 7 0 0 0]\n",
      "action : 0\n",
      "new state: [1 2 7 0 0 1]\n",
      "reward: -0.0\n",
      "epReward so far: -40000.0\n",
      "final state: [1 2 7 0 0 1]\n",
      "Episode : 47\n",
      "state : [1 2 7 0 0 1]\n",
      "action : 0\n",
      "new state: [1 2 7 0 1 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [1 2 7 0 1 3]\n",
      "action : 1\n",
      "new state: [1 1 7 1 2 0]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [1 1 7 1 2 0]\n",
      "action : 2\n",
      "new state: [1 1 7 1 0 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [1 1 7 1 0 1]\n",
      "action : 0\n",
      "new state: [0 2 7 1 2 3]\n",
      "reward: -0.0\n",
      "epReward so far: -20000.0\n",
      "state : [0 2 7 1 2 3]\n",
      "action : 2\n",
      "new state: [0 2 6 2 1 0]\n",
      "reward: -0.0\n",
      "epReward so far: -20000.0\n",
      "final state: [0 2 6 2 1 0]\n",
      "Episode : 48\n",
      "state : [0 2 6 2 1 0]\n",
      "action : 1\n",
      "new state: [1 1 6 2 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 1 6 2 2 2]\n",
      "action : 2\n",
      "new state: [1 1 6 2 3 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 1 6 2 3 3]\n",
      "action : 3\n",
      "new state: [1 1 6 2 3 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 1 6 2 3 3]\n",
      "action : 3\n",
      "new state: [1 1 6 2 1 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 1 6 2 1 2]\n",
      "action : 1\n",
      "new state: [1 0 7 2 3 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "final state: [1 0 7 2 3 0]\n",
      "Episode : 49\n",
      "state : [1 0 7 2 3 0]\n",
      "action : 3\n",
      "new state: [2 0 7 1 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 0 7 1 0 2]\n",
      "action : 0\n",
      "new state: [1 0 8 1 1 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 0 8 1 1 0]\n",
      "action : 0\n",
      "new state: [1 0 8 1 2 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [1 0 8 1 2 3]\n",
      "action : 2\n",
      "new state: [1 0 8 1 0 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [1 0 8 1 0 0]\n",
      "action : 0\n",
      "new state: [1 0 8 1 2 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -30000.0\n",
      "final state: [1 0 8 1 2 2]\n",
      "Episode : 50\n",
      "state : [1 0 8 1 2 2]\n",
      "action : 2\n",
      "new state: [1 0 8 1 0 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 0 8 1 0 1]\n",
      "action : 0\n",
      "new state: [0 1 8 1 2 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 1 8 1 2 1]\n",
      "action : 2\n",
      "new state: [0 2 7 1 1 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 2 7 1 1 2]\n",
      "action : 1\n",
      "new state: [0 2 7 1 2 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 2 7 1 2 1]\n",
      "action : 2\n",
      "new state: [0 3 6 1 1 3]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "final state: [0 3 6 1 1 3]\n",
      "Episode : 51\n",
      "state : [0 3 6 1 1 3]\n",
      "action : 1\n",
      "new state: [0 2 6 2 1 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 2 6 2 1 2]\n",
      "action : 1\n",
      "new state: [0 1 7 2 2 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 1 7 2 2 3]\n",
      "action : 2\n",
      "new state: [0 1 6 3 2 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 1 6 3 2 0]\n",
      "action : 2\n",
      "new state: [1 1 5 3 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 1 5 3 0 2]\n",
      "action : 0\n",
      "new state: [0 1 6 3 0 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "final state: [0 1 6 3 0 0]\n",
      "Episode : 52\n",
      "state : [0 1 6 3 0 0]\n",
      "action : 1\n",
      "new state: [1 0 6 3 0 2]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [1 0 6 3 0 2]\n",
      "action : 0\n",
      "new state: [1 0 6 3 1 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "state : [1 0 6 3 1 1]\n",
      "action : 0\n",
      "new state: [0 1 6 3 2 1]\n",
      "reward: -1.0\n",
      "epReward so far: -10002.0\n",
      "state : [0 1 6 3 2 1]\n",
      "action : 2\n",
      "new state: [0 2 5 3 3 2]\n",
      "reward: -0.0\n",
      "epReward so far: -10002.0\n",
      "state : [0 2 5 3 3 2]\n",
      "action : 3\n",
      "new state: [0 2 6 2 0 0]\n",
      "reward: -0.0\n",
      "epReward so far: -10002.0\n",
      "final state: [0 2 6 2 0 0]\n",
      "Episode : 53\n",
      "state : [0 2 6 2 0 0]\n",
      "action : 1\n",
      "new state: [1 1 6 2 3 1]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [1 1 6 2 3 1]\n",
      "action : 3\n",
      "new state: [1 1 6 2 2 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "state : [1 1 6 2 2 2]\n",
      "action : 2\n",
      "new state: [1 1 6 2 1 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "state : [1 1 6 2 1 1]\n",
      "action : 1\n",
      "new state: [1 1 6 2 2 1]\n",
      "reward: -0.0\n",
      "epReward so far: -20001.0\n",
      "state : [1 1 6 2 2 1]\n",
      "action : 2\n",
      "new state: [1 2 5 2 2 1]\n",
      "reward: -0.0\n",
      "epReward so far: -20001.0\n",
      "final state: [1 2 5 2 2 1]\n",
      "Episode : 54\n",
      "state : [1 2 5 2 2 1]\n",
      "action : 2\n",
      "new state: [1 3 4 2 3 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 3 4 2 3 0]\n",
      "action : 3\n",
      "new state: [2 3 4 1 1 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 3 4 1 1 1]\n",
      "action : 1\n",
      "new state: [2 3 4 1 1 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 3 4 1 1 3]\n",
      "action : 1\n",
      "new state: [2 3 4 1 0 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 3 4 1 0 3]\n",
      "action : 0\n",
      "new state: [2 3 4 1 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "final state: [2 3 4 1 0 2]\n",
      "Episode : 55\n",
      "state : [2 3 4 1 0 2]\n",
      "action : 0\n",
      "new state: [1 3 5 1 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 3 5 1 0 2]\n",
      "action : 0\n",
      "new state: [1 3 5 1 2 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [1 3 5 1 2 1]\n",
      "action : 2\n",
      "new state: [1 4 4 1 0 1]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [1 4 4 1 0 1]\n",
      "action : 0\n",
      "new state: [0 5 4 1 0 3]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 5 4 1 0 3]\n",
      "action : 1\n",
      "new state: [0 4 4 2 1 1]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "final state: [0 4 4 2 1 1]\n",
      "Episode : 56\n",
      "state : [0 4 4 2 1 1]\n",
      "action : 1\n",
      "new state: [0 4 4 2 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 4 4 2 2 2]\n",
      "action : 2\n",
      "new state: [0 4 4 2 2 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 4 4 2 2 1]\n",
      "action : 2\n",
      "new state: [0 5 3 2 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 5 3 2 0 2]\n",
      "action : 1\n",
      "new state: [0 5 3 2 2 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [0 5 3 2 2 3]\n",
      "action : 2\n",
      "new state: [0 5 2 3 2 0]\n",
      "reward: -0.0\n",
      "epReward so far: -20000.0\n",
      "final state: [0 5 2 3 2 0]\n",
      "Episode : 57\n",
      "state : [0 5 2 3 2 0]\n",
      "action : 2\n",
      "new state: [1 5 1 3 3 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 5 1 3 3 3]\n",
      "action : 3\n",
      "new state: [1 5 1 3 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [1 5 1 3 0 2]\n",
      "action : 0\n",
      "new state: [0 5 2 3 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 5 2 3 0 2]\n",
      "action : 1\n",
      "new state: [0 5 2 3 0 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [0 5 2 3 0 3]\n",
      "action : 1\n",
      "new state: [0 4 2 4 1 1]\n",
      "reward: -1.0\n",
      "epReward so far: -20001.0\n",
      "final state: [0 4 2 4 1 1]\n",
      "Episode : 58\n",
      "state : [0 4 2 4 1 1]\n",
      "action : 1\n",
      "new state: [0 4 2 4 2 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 4 2 4 2 2]\n",
      "action : 2\n",
      "new state: [0 4 2 4 2 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [0 4 2 4 2 3]\n",
      "action : 2\n",
      "new state: [0 4 2 4 1 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -30000.0\n",
      "state : [0 4 2 4 1 2]\n",
      "action : 1\n",
      "new state: [0 4 2 4 3 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -40000.0\n",
      "state : [0 4 2 4 3 1]\n",
      "action : 3\n",
      "new state: [0 5 2 3 3 3]\n",
      "reward: -0.0\n",
      "epReward so far: -40000.0\n",
      "final state: [0 5 2 3 3 3]\n",
      "Episode : 59\n",
      "state : [0 5 2 3 3 3]\n",
      "action : 3\n",
      "new state: [0 5 2 3 3 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 5 2 3 3 3]\n",
      "action : 3\n",
      "new state: [0 5 2 3 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 5 2 3 0 2]\n",
      "action : 1\n",
      "new state: [0 5 2 3 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [0 5 2 3 0 2]\n",
      "action : 1\n",
      "new state: [0 5 2 3 2 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -30000.0\n",
      "state : [0 5 2 3 2 1]\n",
      "action : 2\n",
      "new state: [0 6 1 3 2 0]\n",
      "reward: -0.0\n",
      "epReward so far: -30000.0\n",
      "final state: [0 6 1 3 2 0]\n",
      "Episode : 60\n",
      "state : [0 6 1 3 2 0]\n",
      "action : 2\n",
      "new state: [1 6 0 3 3 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 6 0 3 3 1]\n",
      "action : 3\n",
      "new state: [1 7 0 2 1 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 7 0 2 1 3]\n",
      "action : 1\n",
      "new state: [1 6 0 3 2 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 6 0 3 2 3]\n",
      "action : 0\n",
      "new state: [0 6 0 4 2 2]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [0 6 0 4 2 2]\n",
      "action : 1\n",
      "new state: [0 6 0 4 2 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "final state: [0 6 0 4 2 2]\n",
      "Episode : 61\n",
      "state : [0 6 0 4 2 2]\n",
      "action : 1\n",
      "new state: [0 6 0 4 1 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 6 0 4 1 1]\n",
      "action : 1\n",
      "new state: [0 6 0 4 2 3]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 6 0 4 2 3]\n",
      "action : 1\n",
      "new state: [0 6 0 4 1 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [0 6 0 4 1 0]\n",
      "action : 1\n",
      "new state: [1 5 0 4 2 1]\n",
      "reward: -0.0\n",
      "epReward so far: -20000.0\n",
      "state : [1 5 0 4 2 1]\n",
      "action : 0\n",
      "new state: [1 5 0 4 0 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -30000.0\n",
      "final state: [1 5 0 4 0 1]\n",
      "Episode : 62\n",
      "state : [1 5 0 4 0 1]\n",
      "action : 0\n",
      "new state: [0 6 0 4 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 6 0 4 2 2]\n",
      "action : 1\n",
      "new state: [0 6 0 4 0 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 6 0 4 0 3]\n",
      "action : 1\n",
      "new state: [0 5 0 5 3 1]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [0 5 0 5 3 1]\n",
      "action : 3\n",
      "new state: [0 6 0 4 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: -10001.0\n",
      "state : [0 6 0 4 0 2]\n",
      "action : 1\n",
      "new state: [0 5 1 4 1 3]\n",
      "reward: -1.0\n",
      "epReward so far: -10002.0\n",
      "final state: [0 5 1 4 1 3]\n",
      "Episode : 63\n",
      "state : [0 5 1 4 1 3]\n",
      "action : 1\n",
      "new state: [0 4 1 5 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 4 1 5 2 2]\n",
      "action : 2\n",
      "new state: [0 4 1 5 3 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 4 1 5 3 3]\n",
      "action : 3\n",
      "new state: [0 4 1 5 3 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [0 4 1 5 3 1]\n",
      "action : 3\n",
      "new state: [0 4 1 5 3 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -30000.0\n",
      "state : [0 4 1 5 3 3]\n",
      "action : 3\n",
      "new state: [0 4 1 5 1 1]\n",
      "reward: -0.0\n",
      "epReward so far: -30000.0\n",
      "final state: [0 4 1 5 1 1]\n",
      "Episode : 64\n",
      "state : [0 4 1 5 1 1]\n",
      "action : 1\n",
      "new state: [0 4 1 5 0 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 4 1 5 0 1]\n",
      "action : 1\n",
      "new state: [0 4 1 5 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 4 1 5 0 2]\n",
      "action : 1\n",
      "new state: [0 4 1 5 1 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [0 4 1 5 1 3]\n",
      "action : 1\n",
      "new state: [0 3 1 6 3 2]\n",
      "reward: -0.0\n",
      "epReward so far: -20000.0\n",
      "state : [0 3 1 6 3 2]\n",
      "action : 3\n",
      "new state: [0 3 2 5 2 0]\n",
      "reward: -0.0\n",
      "epReward so far: -20000.0\n",
      "final state: [0 3 2 5 2 0]\n",
      "Episode : 65\n",
      "state : [0 3 2 5 2 0]\n",
      "action : 2\n",
      "new state: [1 3 1 5 2 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 3 1 5 2 0]\n",
      "action : 2\n",
      "new state: [1 3 1 5 3 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [1 3 1 5 3 1]\n",
      "action : 3\n",
      "new state: [1 3 1 5 3 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [1 3 1 5 3 1]\n",
      "action : 3\n",
      "new state: [1 4 1 4 2 1]\n",
      "reward: -0.0\n",
      "epReward so far: -20000.0\n",
      "state : [1 4 1 4 2 1]\n",
      "action : 2\n",
      "new state: [1 4 1 4 3 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -30000.0\n",
      "final state: [1 4 1 4 3 1]\n",
      "Episode : 66\n",
      "state : [1 4 1 4 3 1]\n",
      "action : 3\n",
      "new state: [1 5 1 3 3 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 5 1 3 3 2]\n",
      "action : 3\n",
      "new state: [1 5 2 2 1 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 5 2 2 1 1]\n",
      "action : 1\n",
      "new state: [1 5 2 2 1 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 5 2 2 1 2]\n",
      "action : 1\n",
      "new state: [1 4 3 2 0 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 4 3 2 0 3]\n",
      "action : 0\n",
      "new state: [1 4 3 2 0 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "final state: [1 4 3 2 0 0]\n",
      "Episode : 67\n",
      "state : [1 4 3 2 0 0]\n",
      "action : 0\n",
      "new state: [1 4 3 2 2 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [1 4 3 2 2 3]\n",
      "action : 2\n",
      "new state: [1 4 2 3 2 0]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [1 4 2 3 2 0]\n",
      "action : 2\n",
      "new state: [2 4 1 3 1 2]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 4 1 3 1 2]\n",
      "action : 1\n",
      "new state: [2 3 2 3 1 1]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 3 2 3 1 1]\n",
      "action : 1\n",
      "new state: [2 3 2 3 0 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "final state: [2 3 2 3 0 3]\n",
      "Episode : 68\n",
      "state : [2 3 2 3 0 3]\n",
      "action : 0\n",
      "new state: [1 3 2 4 0 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 3 2 4 0 1]\n",
      "action : 0\n",
      "new state: [0 4 2 4 0 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 4 2 4 0 3]\n",
      "action : 1\n",
      "new state: [0 4 2 4 3 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 4 2 4 3 1]\n",
      "action : 3\n",
      "new state: [0 4 2 4 2 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [0 4 2 4 2 2]\n",
      "action : 2\n",
      "new state: [0 4 2 4 0 3]\n",
      "reward: -0.0\n",
      "epReward so far: -20000.0\n",
      "final state: [0 4 2 4 0 3]\n",
      "Episode : 69\n",
      "state : [0 4 2 4 0 3]\n",
      "action : 1\n",
      "new state: [0 4 2 4 2 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 4 2 4 2 2]\n",
      "action : 2\n",
      "new state: [0 4 2 4 0 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [0 4 2 4 0 3]\n",
      "action : 1\n",
      "new state: [0 4 2 4 2 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -30000.0\n",
      "state : [0 4 2 4 2 3]\n",
      "action : 2\n",
      "new state: [0 4 1 5 1 3]\n",
      "reward: -0.0\n",
      "epReward so far: -30000.0\n",
      "state : [0 4 1 5 1 3]\n",
      "action : 1\n",
      "new state: [0 3 1 6 3 2]\n",
      "reward: -0.0\n",
      "epReward so far: -30000.0\n",
      "final state: [0 3 1 6 3 2]\n",
      "Episode : 70\n",
      "state : [0 3 1 6 3 2]\n",
      "action : 3\n",
      "new state: [0 3 2 5 2 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 3 2 5 2 3]\n",
      "action : 2\n",
      "new state: [0 3 1 6 1 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 3 1 6 1 2]\n",
      "action : 1\n",
      "new state: [0 2 2 6 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 2 2 6 0 2]\n",
      "action : 1\n",
      "new state: [0 1 3 6 0 1]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [0 1 3 6 0 1]\n",
      "action : 1\n",
      "new state: [0 1 3 6 2 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "final state: [0 1 3 6 2 1]\n",
      "Episode : 71\n",
      "state : [0 1 3 6 2 1]\n",
      "action : 2\n",
      "new state: [0 2 2 6 2 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 2 2 6 2 0]\n",
      "action : 2\n",
      "new state: [1 2 1 6 1 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 2 1 6 1 1]\n",
      "action : 1\n",
      "new state: [1 2 1 6 0 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [1 2 1 6 0 3]\n",
      "action : 0\n",
      "new state: [1 2 1 6 1 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [1 2 1 6 1 2]\n",
      "action : 1\n",
      "new state: [1 1 2 6 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: -20000.0\n",
      "final state: [1 1 2 6 0 2]\n",
      "Episode : 72\n",
      "state : [1 1 2 6 0 2]\n",
      "action : 0\n",
      "new state: [0 1 3 6 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 1 3 6 0 2]\n",
      "action : 1\n",
      "new state: [0 0 4 6 2 0]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [0 0 4 6 2 0]\n",
      "action : 2\n",
      "new state: [1 0 3 6 1 2]\n",
      "reward: -0.0\n",
      "epReward so far: -1.0\n",
      "state : [1 0 3 6 1 2]\n",
      "action : 0\n",
      "new state: [0 0 4 6 3 0]\n",
      "reward: -1.0\n",
      "epReward so far: -2.0\n",
      "state : [0 0 4 6 3 0]\n",
      "action : 3\n",
      "new state: [1 0 4 5 0 0]\n",
      "reward: -0.0\n",
      "epReward so far: -2.0\n",
      "final state: [1 0 4 5 0 0]\n",
      "Episode : 73\n",
      "state : [1 0 4 5 0 0]\n",
      "action : 0\n",
      "new state: [1 0 4 5 0 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 0 4 5 0 3]\n",
      "action : 0\n",
      "new state: [1 0 4 5 3 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [1 0 4 5 3 2]\n",
      "action : 3\n",
      "new state: [1 0 5 4 3 0]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [1 0 5 4 3 0]\n",
      "action : 3\n",
      "new state: [1 0 5 4 0 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [1 0 5 4 0 0]\n",
      "action : 0\n",
      "new state: [1 0 5 4 3 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -30000.0\n",
      "final state: [1 0 5 4 3 1]\n",
      "Episode : 74\n",
      "state : [1 0 5 4 3 1]\n",
      "action : 3\n",
      "new state: [1 0 5 4 2 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [1 0 5 4 2 0]\n",
      "action : 2\n",
      "new state: [2 0 4 4 1 3]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 0 4 4 1 3]\n",
      "action : 0\n",
      "new state: [2 0 4 4 0 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [2 0 4 4 0 3]\n",
      "action : 0\n",
      "new state: [2 0 4 4 0 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -30000.0\n",
      "state : [2 0 4 4 0 1]\n",
      "action : 0\n",
      "new state: [1 1 4 4 2 3]\n",
      "reward: -0.0\n",
      "epReward so far: -30000.0\n",
      "final state: [1 1 4 4 2 3]\n",
      "Episode : 75\n",
      "state : [1 1 4 4 2 3]\n",
      "action : 2\n",
      "new state: [1 1 3 5 3 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 1 3 5 3 3]\n",
      "action : 3\n",
      "new state: [1 1 3 5 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [1 1 3 5 0 2]\n",
      "action : 0\n",
      "new state: [0 1 4 5 0 3]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 1 4 5 0 3]\n",
      "action : 1\n",
      "new state: [0 1 4 5 2 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [0 1 4 5 2 2]\n",
      "action : 2\n",
      "new state: [0 1 4 5 2 3]\n",
      "reward: -0.0\n",
      "epReward so far: -20000.0\n",
      "final state: [0 1 4 5 2 3]\n",
      "Episode : 76\n",
      "state : [0 1 4 5 2 3]\n",
      "action : 2\n",
      "new state: [0 1 3 6 2 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 1 3 6 2 3]\n",
      "action : 2\n",
      "new state: [0 1 2 7 3 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 1 2 7 3 1]\n",
      "action : 3\n",
      "new state: [0 2 2 6 1 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 2 2 6 1 2]\n",
      "action : 1\n",
      "new state: [0 1 3 6 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 1 3 6 0 2]\n",
      "action : 1\n",
      "new state: [0 0 4 6 1 0]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "final state: [0 0 4 6 1 0]\n",
      "Episode : 77\n",
      "state : [0 0 4 6 1 0]\n",
      "action : 2\n",
      "new state: [1 0 3 6 2 2]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [1 0 3 6 2 2]\n",
      "action : 2\n",
      "new state: [1 0 3 6 2 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "state : [1 0 3 6 2 2]\n",
      "action : 2\n",
      "new state: [1 0 3 6 1 0]\n",
      "reward: -0.0\n",
      "epReward so far: -10001.0\n",
      "state : [1 0 3 6 1 0]\n",
      "action : 0\n",
      "new state: [1 0 3 6 1 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "state : [1 0 3 6 1 2]\n",
      "action : 0\n",
      "new state: [1 0 3 6 1 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -30001.0\n",
      "final state: [1 0 3 6 1 1]\n",
      "Episode : 78\n",
      "state : [1 0 3 6 1 1]\n",
      "action : 0\n",
      "new state: [1 0 3 6 3 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [1 0 3 6 3 2]\n",
      "action : 3\n",
      "new state: [1 0 4 5 1 1]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [1 0 4 5 1 1]\n",
      "action : 0\n",
      "new state: [0 1 4 5 3 3]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [0 1 4 5 3 3]\n",
      "action : 3\n",
      "new state: [0 1 4 5 1 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "state : [0 1 4 5 1 1]\n",
      "action : 1\n",
      "new state: [0 1 4 5 3 2]\n",
      "reward: -0.0\n",
      "epReward so far: -20001.0\n",
      "final state: [0 1 4 5 3 2]\n",
      "Episode : 79\n",
      "state : [0 1 4 5 3 2]\n",
      "action : 3\n",
      "new state: [0 1 5 4 0 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 1 5 4 0 0]\n",
      "action : 1\n",
      "new state: [0 1 5 4 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 1 5 4 0 2]\n",
      "action : 1\n",
      "new state: [0 1 5 4 1 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [0 1 5 4 1 3]\n",
      "action : 1\n",
      "new state: [0 0 5 5 3 2]\n",
      "reward: -0.0\n",
      "epReward so far: -20000.0\n",
      "state : [0 0 5 5 3 2]\n",
      "action : 3\n",
      "new state: [0 0 6 4 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: -20000.0\n",
      "final state: [0 0 6 4 2 2]\n",
      "Episode : 80\n",
      "state : [0 0 6 4 2 2]\n",
      "action : 2\n",
      "new state: [0 0 6 4 3 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 0 6 4 3 2]\n",
      "action : 3\n",
      "new state: [0 0 7 3 0 3]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 0 7 3 0 3]\n",
      "action : 2\n",
      "new state: [0 0 7 3 3 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [0 0 7 3 3 3]\n",
      "action : 3\n",
      "new state: [0 0 7 3 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: -20000.0\n",
      "state : [0 0 7 3 2 2]\n",
      "action : 2\n",
      "new state: [0 0 7 3 2 0]\n",
      "reward: -0.0\n",
      "epReward so far: -20000.0\n",
      "final state: [0 0 7 3 2 0]\n",
      "Episode : 81\n",
      "state : [0 0 7 3 2 0]\n",
      "action : 2\n",
      "new state: [1 0 6 3 1 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 0 6 3 1 2]\n",
      "action : 0\n",
      "new state: [0 0 7 3 2 2]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [0 0 7 3 2 2]\n",
      "action : 2\n",
      "new state: [0 0 7 3 3 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "state : [0 0 7 3 3 0]\n",
      "action : 3\n",
      "new state: [1 0 7 2 3 3]\n",
      "reward: -0.0\n",
      "epReward so far: -10001.0\n",
      "state : [1 0 7 2 3 3]\n",
      "action : 3\n",
      "new state: [1 0 7 2 1 0]\n",
      "reward: -0.0\n",
      "epReward so far: -10001.0\n",
      "final state: [1 0 7 2 1 0]\n",
      "Episode : 82\n",
      "state : [1 0 7 2 1 0]\n",
      "action : 0\n",
      "new state: [1 0 7 2 1 1]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [1 0 7 2 1 1]\n",
      "action : 0\n",
      "new state: [0 1 7 2 2 1]\n",
      "reward: -1.0\n",
      "epReward so far: -2.0\n",
      "state : [0 1 7 2 2 1]\n",
      "action : 2\n",
      "new state: [0 2 6 2 3 1]\n",
      "reward: -0.0\n",
      "epReward so far: -2.0\n",
      "state : [0 2 6 2 3 1]\n",
      "action : 3\n",
      "new state: [0 3 6 1 3 1]\n",
      "reward: -0.0\n",
      "epReward so far: -2.0\n",
      "state : [0 3 6 1 3 1]\n",
      "action : 3\n",
      "new state: [0 4 6 0 3 1]\n",
      "reward: -0.0\n",
      "epReward so far: -2.0\n",
      "final state: [0 4 6 0 3 1]\n",
      "Episode : 83\n",
      "state : [0 4 6 0 3 1]\n",
      "action : 1\n",
      "new state: [0 4 6 0 3 1]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [0 4 6 0 3 1]\n",
      "action : 1\n",
      "new state: [0 4 6 0 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "state : [0 4 6 0 0 2]\n",
      "action : 1\n",
      "new state: [0 3 7 0 1 2]\n",
      "reward: -1.0\n",
      "epReward so far: -10002.0\n",
      "state : [0 3 7 0 1 2]\n",
      "action : 1\n",
      "new state: [0 2 8 0 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: -10002.0\n",
      "state : [0 2 8 0 0 2]\n",
      "action : 1\n",
      "new state: [0 2 8 0 1 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20002.0\n",
      "final state: [0 2 8 0 1 2]\n",
      "Episode : 84\n",
      "state : [0 2 8 0 1 2]\n",
      "action : 1\n",
      "new state: [0 1 9 0 3 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 1 9 0 3 1]\n",
      "action : 1\n",
      "new state: [0 1 9 0 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 1 9 0 0 2]\n",
      "action : 1\n",
      "new state: [ 0  0 10  0  2  3]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [ 0  0 10  0  2  3]\n",
      "action : 2\n",
      "new state: [0 0 9 1 2 3]\n",
      "reward: -0.0\n",
      "epReward so far: -10001.0\n",
      "state : [0 0 9 1 2 3]\n",
      "action : 2\n",
      "new state: [0 0 8 2 3 2]\n",
      "reward: -0.0\n",
      "epReward so far: -10001.0\n",
      "final state: [0 0 8 2 3 2]\n",
      "Episode : 85\n",
      "state : [0 0 8 2 3 2]\n",
      "action : 3\n",
      "new state: [0 0 8 2 2 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 0 8 2 2 2]\n",
      "action : 2\n",
      "new state: [0 0 8 2 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 0 8 2 0 2]\n",
      "action : 2\n",
      "new state: [0 0 8 2 0 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [0 0 8 2 0 1]\n",
      "action : 2\n",
      "new state: [0 1 7 2 2 2]\n",
      "reward: -1.0\n",
      "epReward so far: -20001.0\n",
      "state : [0 1 7 2 2 2]\n",
      "action : 2\n",
      "new state: [0 1 7 2 3 0]\n",
      "reward: -0.0\n",
      "epReward so far: -20001.0\n",
      "final state: [0 1 7 2 3 0]\n",
      "Episode : 86\n",
      "state : [0 1 7 2 3 0]\n",
      "action : 3\n",
      "new state: [1 1 7 1 0 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 1 7 1 0 3]\n",
      "action : 0\n",
      "new state: [0 1 7 2 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 1 7 2 2 2]\n",
      "action : 2\n",
      "new state: [0 1 7 2 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 1 7 2 0 2]\n",
      "action : 1\n",
      "new state: [0 0 8 2 3 0]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [0 0 8 2 3 0]\n",
      "action : 3\n",
      "new state: [1 0 8 1 0 3]\n",
      "reward: -0.0\n",
      "epReward so far: -10001.0\n",
      "final state: [1 0 8 1 0 3]\n",
      "Episode : 87\n",
      "state : [1 0 8 1 0 3]\n",
      "action : 0\n",
      "new state: [0 0 8 2 2 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 0 8 2 2 3]\n",
      "action : 2\n",
      "new state: [0 0 7 3 3 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 0 7 3 3 1]\n",
      "action : 3\n",
      "new state: [0 1 7 2 3 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 1 7 2 3 3]\n",
      "action : 3\n",
      "new state: [0 1 7 2 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 1 7 2 0 2]\n",
      "action : 1\n",
      "new state: [0 0 8 2 1 0]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "final state: [0 0 8 2 1 0]\n",
      "Episode : 88\n",
      "state : [0 0 8 2 1 0]\n",
      "action : 2\n",
      "new state: [0 0 8 2 1 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 0 8 2 1 3]\n",
      "action : 2\n",
      "new state: [0 0 7 3 3 2]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [0 0 7 3 3 2]\n",
      "action : 3\n",
      "new state: [0 0 7 3 1 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "state : [0 0 7 3 1 3]\n",
      "action : 2\n",
      "new state: [0 0 6 4 0 3]\n",
      "reward: -1.0\n",
      "epReward so far: -20002.0\n",
      "state : [0 0 6 4 0 3]\n",
      "action : 2\n",
      "new state: [0 0 5 5 1 3]\n",
      "reward: -1.0\n",
      "epReward so far: -20003.0\n",
      "final state: [0 0 5 5 1 3]\n",
      "Episode : 89\n",
      "state : [0 0 5 5 1 3]\n",
      "action : 2\n",
      "new state: [0 0 5 5 3 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 0 5 5 3 2]\n",
      "action : 3\n",
      "new state: [0 0 6 4 1 2]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 0 6 4 1 2]\n",
      "action : 2\n",
      "new state: [0 0 6 4 0 2]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [0 0 6 4 0 2]\n",
      "action : 2\n",
      "new state: [0 0 6 4 3 1]\n",
      "reward: -1.0\n",
      "epReward so far: -10002.0\n",
      "state : [0 0 6 4 3 1]\n",
      "action : 3\n",
      "new state: [0 0 6 4 3 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20002.0\n",
      "final state: [0 0 6 4 3 2]\n",
      "Episode : 90\n",
      "state : [0 0 6 4 3 2]\n",
      "action : 3\n",
      "new state: [0 0 7 3 1 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 0 7 3 1 0]\n",
      "action : 2\n",
      "new state: [0 0 7 3 2 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 0 7 3 2 1]\n",
      "action : 2\n",
      "new state: [0 1 6 3 2 3]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 1 6 3 2 3]\n",
      "action : 2\n",
      "new state: [0 1 6 3 2 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [0 1 6 3 2 3]\n",
      "action : 2\n",
      "new state: [0 1 5 4 3 0]\n",
      "reward: -0.0\n",
      "epReward so far: -20000.0\n",
      "final state: [0 1 5 4 3 0]\n",
      "Episode : 91\n",
      "state : [0 1 5 4 3 0]\n",
      "action : 3\n",
      "new state: [1 1 5 3 3 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 1 5 3 3 0]\n",
      "action : 3\n",
      "new state: [2 1 5 2 0 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 1 5 2 0 3]\n",
      "action : 0\n",
      "new state: [2 1 5 2 1 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 1 5 2 1 0]\n",
      "action : 1\n",
      "new state: [3 0 5 2 3 2]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [3 0 5 2 3 2]\n",
      "action : 3\n",
      "new state: [3 0 6 1 2 0]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "final state: [3 0 6 1 2 0]\n",
      "Episode : 92\n",
      "state : [3 0 6 1 2 0]\n",
      "action : 2\n",
      "new state: [4 0 5 1 3 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [4 0 5 1 3 1]\n",
      "action : 3\n",
      "new state: [4 1 5 0 1 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [4 1 5 0 1 3]\n",
      "action : 1\n",
      "new state: [4 0 5 1 1 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [4 0 5 1 1 3]\n",
      "action : 0\n",
      "new state: [3 0 5 2 2 0]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [3 0 5 2 2 0]\n",
      "action : 2\n",
      "new state: [4 0 4 2 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: -1.0\n",
      "final state: [4 0 4 2 2 2]\n",
      "Episode : 93\n",
      "state : [4 0 4 2 2 2]\n",
      "action : 2\n",
      "new state: [4 0 4 2 3 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [4 0 4 2 3 3]\n",
      "action : 3\n",
      "new state: [4 0 4 2 3 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [4 0 4 2 3 3]\n",
      "action : 3\n",
      "new state: [4 0 4 2 2 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [4 0 4 2 2 3]\n",
      "action : 2\n",
      "new state: [4 0 3 3 3 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [4 0 3 3 3 0]\n",
      "action : 3\n",
      "new state: [5 0 3 2 1 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "final state: [5 0 3 2 1 2]\n",
      "Episode : 94\n",
      "state : [5 0 3 2 1 2]\n",
      "action : 0\n",
      "new state: [5 0 3 2 2 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [5 0 3 2 2 0]\n",
      "action : 2\n",
      "new state: [6 0 2 2 0 3]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [6 0 2 2 0 3]\n",
      "action : 0\n",
      "new state: [5 0 2 3 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [5 0 2 3 2 2]\n",
      "action : 2\n",
      "new state: [5 0 2 3 1 0]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [5 0 2 3 1 0]\n",
      "action : 0\n",
      "new state: [5 0 2 3 2 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "final state: [5 0 2 3 2 0]\n",
      "Episode : 95\n",
      "state : [5 0 2 3 2 0]\n",
      "action : 2\n",
      "new state: [6 0 1 3 2 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [6 0 1 3 2 3]\n",
      "action : 2\n",
      "new state: [6 0 0 4 2 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [6 0 0 4 2 1]\n",
      "action : 0\n",
      "new state: [6 0 0 4 0 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [6 0 0 4 0 0]\n",
      "action : 0\n",
      "new state: [6 0 0 4 3 2]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [6 0 0 4 3 2]\n",
      "action : 3\n",
      "new state: [6 0 1 3 2 1]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "final state: [6 0 1 3 2 1]\n",
      "Episode : 96\n",
      "state : [6 0 1 3 2 1]\n",
      "action : 2\n",
      "new state: [6 0 1 3 3 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [6 0 1 3 3 3]\n",
      "action : 3\n",
      "new state: [6 0 1 3 2 0]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [6 0 1 3 2 0]\n",
      "action : 2\n",
      "new state: [7 0 0 3 0 3]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [7 0 0 3 0 3]\n",
      "action : 0\n",
      "new state: [6 0 0 4 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [6 0 0 4 0 2]\n",
      "action : 0\n",
      "new state: [6 0 0 4 2 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "final state: [6 0 0 4 2 0]\n",
      "Episode : 97\n",
      "state : [6 0 0 4 2 0]\n",
      "action : 0\n",
      "new state: [6 0 0 4 3 1]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [6 0 0 4 3 1]\n",
      "action : 3\n",
      "new state: [6 1 0 3 0 3]\n",
      "reward: -0.0\n",
      "epReward so far: -1.0\n",
      "state : [6 1 0 3 0 3]\n",
      "action : 0\n",
      "new state: [5 1 0 4 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: -1.0\n",
      "state : [5 1 0 4 2 2]\n",
      "action : 0\n",
      "new state: [5 1 0 4 0 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "state : [5 1 0 4 0 1]\n",
      "action : 0\n",
      "new state: [4 2 0 4 0 0]\n",
      "reward: -0.0\n",
      "epReward so far: -10001.0\n",
      "final state: [4 2 0 4 0 0]\n",
      "Episode : 98\n",
      "state : [4 2 0 4 0 0]\n",
      "action : 0\n",
      "new state: [4 2 0 4 3 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [4 2 0 4 3 2]\n",
      "action : 3\n",
      "new state: [4 2 0 4 0 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [4 2 0 4 0 3]\n",
      "action : 0\n",
      "new state: [3 2 0 5 3 1]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [3 2 0 5 3 1]\n",
      "action : 3\n",
      "new state: [3 3 0 4 1 3]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [3 3 0 4 1 3]\n",
      "action : 1\n",
      "new state: [3 3 0 4 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "final state: [3 3 0 4 0 2]\n",
      "Episode : 99\n",
      "state : [3 3 0 4 0 2]\n",
      "action : 0\n",
      "new state: [3 3 0 4 3 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [3 3 0 4 3 3]\n",
      "action : 3\n",
      "new state: [3 3 0 4 0 0]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [3 3 0 4 0 0]\n",
      "action : 0\n",
      "new state: [3 3 0 4 0 0]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [3 3 0 4 0 0]\n",
      "action : 0\n",
      "new state: [3 3 0 4 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [3 3 0 4 2 2]\n",
      "action : 0\n",
      "new state: [3 3 0 4 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "final state: [3 3 0 4 0 2]\n",
      "Episode : 0\n",
      "state : [3 3 0 4 0 2]\n",
      "action : 0\n",
      "new state: [2 3 1 4 2 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 3 1 4 2 3]\n",
      "action : 2\n",
      "new state: [2 3 1 4 3 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 3 1 4 3 0]\n",
      "action : 3\n",
      "new state: [3 3 1 3 3 3]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [3 3 1 3 3 3]\n",
      "action : 3\n",
      "new state: [3 3 1 3 2 3]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [3 3 1 3 2 3]\n",
      "action : 2\n",
      "new state: [3 3 0 4 3 0]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "final state: [3 3 0 4 3 0]\n",
      "Episode : 1\n",
      "state : [3 3 0 4 3 0]\n",
      "action : 3\n",
      "new state: [4 3 0 3 1 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [4 3 0 3 1 0]\n",
      "action : 1\n",
      "new state: [4 3 0 3 2 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [4 3 0 3 2 0]\n",
      "action : 0\n",
      "new state: [4 3 0 3 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [4 3 0 3 0 2]\n",
      "action : 0\n",
      "new state: [3 3 1 3 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: -20000.0\n",
      "state : [3 3 1 3 2 2]\n",
      "action : 2\n",
      "new state: [3 3 1 3 0 3]\n",
      "reward: -0.0\n",
      "epReward so far: -20000.0\n",
      "final state: [3 3 1 3 0 3]\n",
      "Episode : 2\n",
      "state : [3 3 1 3 0 3]\n",
      "action : 0\n",
      "new state: [2 3 1 4 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 3 1 4 0 2]\n",
      "action : 0\n",
      "new state: [1 3 2 4 1 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 3 2 4 1 3]\n",
      "action : 1\n",
      "new state: [1 2 2 5 1 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 2 2 5 1 3]\n",
      "action : 1\n",
      "new state: [1 2 2 5 2 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [1 2 2 5 2 1]\n",
      "action : 2\n",
      "new state: [1 2 2 5 2 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "final state: [1 2 2 5 2 1]\n",
      "Episode : 3\n",
      "state : [1 2 2 5 2 1]\n",
      "action : 2\n",
      "new state: [1 3 1 5 1 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 3 1 5 1 1]\n",
      "action : 1\n",
      "new state: [1 3 1 5 1 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 3 1 5 1 3]\n",
      "action : 1\n",
      "new state: [1 3 1 5 1 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [1 3 1 5 1 3]\n",
      "action : 1\n",
      "new state: [1 2 1 6 2 3]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [1 2 1 6 2 3]\n",
      "action : 2\n",
      "new state: [1 2 1 6 3 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "final state: [1 2 1 6 3 3]\n",
      "Episode : 4\n",
      "state : [1 2 1 6 3 3]\n",
      "action : 3\n",
      "new state: [1 2 1 6 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 2 1 6 0 2]\n",
      "action : 0\n",
      "new state: [0 2 2 6 0 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 2 2 6 0 3]\n",
      "action : 1\n",
      "new state: [0 1 2 7 0 2]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [0 1 2 7 0 2]\n",
      "action : 1\n",
      "new state: [0 0 3 7 2 2]\n",
      "reward: -1.0\n",
      "epReward so far: -2.0\n",
      "state : [0 0 3 7 2 2]\n",
      "action : 2\n",
      "new state: [0 0 3 7 2 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10002.0\n",
      "final state: [0 0 3 7 2 3]\n",
      "Episode : 5\n",
      "state : [0 0 3 7 2 3]\n",
      "action : 2\n",
      "new state: [0 0 2 8 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 0 2 8 0 2]\n",
      "action : 2\n",
      "new state: [0 0 2 8 3 2]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [0 0 2 8 3 2]\n",
      "action : 3\n",
      "new state: [0 0 2 8 3 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "state : [0 0 2 8 3 1]\n",
      "action : 3\n",
      "new state: [0 1 2 7 1 1]\n",
      "reward: -0.0\n",
      "epReward so far: -10001.0\n",
      "state : [0 1 2 7 1 1]\n",
      "action : 1\n",
      "new state: [0 1 2 7 1 0]\n",
      "reward: -0.0\n",
      "epReward so far: -10001.0\n",
      "final state: [0 1 2 7 1 0]\n",
      "Episode : 6\n",
      "state : [0 1 2 7 1 0]\n",
      "action : 1\n",
      "new state: [0 1 2 7 1 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 1 2 7 1 3]\n",
      "action : 1\n",
      "new state: [0 0 2 8 0 1]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 0 2 8 0 1]\n",
      "action : 2\n",
      "new state: [0 1 1 8 2 0]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [0 1 1 8 2 0]\n",
      "action : 2\n",
      "new state: [1 1 0 8 0 0]\n",
      "reward: -0.0\n",
      "epReward so far: -10001.0\n",
      "state : [1 1 0 8 0 0]\n",
      "action : 0\n",
      "new state: [1 1 0 8 1 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "final state: [1 1 0 8 1 0]\n",
      "Episode : 7\n",
      "state : [1 1 0 8 1 0]\n",
      "action : 1\n",
      "new state: [2 0 0 8 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 0 0 8 0 2]\n",
      "action : 0\n",
      "new state: [1 0 1 8 0 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 0 1 8 0 3]\n",
      "action : 0\n",
      "new state: [1 0 1 8 1 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [1 0 1 8 1 3]\n",
      "action : 0\n",
      "new state: [0 0 1 9 3 3]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [0 0 1 9 3 3]\n",
      "action : 3\n",
      "new state: [0 0 1 9 2 3]\n",
      "reward: -0.0\n",
      "epReward so far: -10001.0\n",
      "final state: [0 0 1 9 2 3]\n",
      "Episode : 8\n",
      "state : [0 0 1 9 2 3]\n",
      "action : 2\n",
      "new state: [ 0  0  0 10  2  2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [ 0  0  0 10  2  2]\n",
      "action : 3\n",
      "new state: [0 0 1 9 2 1]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [0 0 1 9 2 1]\n",
      "action : 2\n",
      "new state: [0 1 0 9 1 0]\n",
      "reward: -0.0\n",
      "epReward so far: -1.0\n",
      "state : [0 1 0 9 1 0]\n",
      "action : 1\n",
      "new state: [1 0 0 9 2 3]\n",
      "reward: -0.0\n",
      "epReward so far: -1.0\n",
      "state : [1 0 0 9 2 3]\n",
      "action : 0\n",
      "new state: [1 0 0 9 3 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "final state: [1 0 0 9 3 0]\n",
      "Episode : 9\n",
      "state : [1 0 0 9 3 0]\n",
      "action : 3\n",
      "new state: [2 0 0 8 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 0 0 8 0 2]\n",
      "action : 0\n",
      "new state: [1 0 1 8 2 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 0 1 8 2 0]\n",
      "action : 2\n",
      "new state: [2 0 0 8 0 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 0 0 8 0 3]\n",
      "action : 0\n",
      "new state: [1 0 0 9 1 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 0 0 9 1 2]\n",
      "action : 0\n",
      "new state: [0 0 1 9 0 0]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "final state: [0 0 1 9 0 0]\n",
      "Episode : 10\n",
      "state : [0 0 1 9 0 0]\n",
      "action : 2\n",
      "new state: [0 0 1 9 0 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 0 1 9 0 1]\n",
      "action : 2\n",
      "new state: [0 0 1 9 0 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [0 0 1 9 0 0]\n",
      "action : 2\n",
      "new state: [1 0 0 9 1 1]\n",
      "reward: -1.0\n",
      "epReward so far: -20001.0\n",
      "state : [1 0 0 9 1 1]\n",
      "action : 0\n",
      "new state: [0 1 0 9 0 0]\n",
      "reward: -1.0\n",
      "epReward so far: -20002.0\n",
      "state : [0 1 0 9 0 0]\n",
      "action : 1\n",
      "new state: [1 0 0 9 2 3]\n",
      "reward: -1.0\n",
      "epReward so far: -20003.0\n",
      "final state: [1 0 0 9 2 3]\n",
      "Episode : 11\n",
      "state : [1 0 0 9 2 3]\n",
      "action : 0\n",
      "new state: [1 0 0 9 2 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [1 0 0 9 2 2]\n",
      "action : 0\n",
      "new state: [0 0 1 9 2 1]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [0 0 1 9 2 1]\n",
      "action : 2\n",
      "new state: [0 1 0 9 1 2]\n",
      "reward: -0.0\n",
      "epReward so far: -10001.0\n",
      "state : [0 1 0 9 1 2]\n",
      "action : 1\n",
      "new state: [0 0 1 9 0 1]\n",
      "reward: -0.0\n",
      "epReward so far: -10001.0\n",
      "state : [0 0 1 9 0 1]\n",
      "action : 2\n",
      "new state: [0 1 0 9 2 2]\n",
      "reward: -1.0\n",
      "epReward so far: -10002.0\n",
      "final state: [0 1 0 9 2 2]\n",
      "Episode : 12\n",
      "state : [0 1 0 9 2 2]\n",
      "action : 1\n",
      "new state: [0 1 0 9 3 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 1 0 9 3 0]\n",
      "action : 3\n",
      "new state: [0 1 0 9 0 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [0 1 0 9 0 1]\n",
      "action : 1\n",
      "new state: [0 1 0 9 3 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -30000.0\n",
      "state : [0 1 0 9 3 2]\n",
      "action : 3\n",
      "new state: [0 1 1 8 1 0]\n",
      "reward: -0.0\n",
      "epReward so far: -30000.0\n",
      "state : [0 1 1 8 1 0]\n",
      "action : 1\n",
      "new state: [1 0 1 8 1 0]\n",
      "reward: -0.0\n",
      "epReward so far: -30000.0\n",
      "final state: [1 0 1 8 1 0]\n",
      "Episode : 13\n",
      "state : [1 0 1 8 1 0]\n",
      "action : 0\n",
      "new state: [1 0 1 8 1 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [1 0 1 8 1 1]\n",
      "action : 0\n",
      "new state: [1 0 1 8 0 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [1 0 1 8 0 3]\n",
      "action : 0\n",
      "new state: [0 0 1 9 1 1]\n",
      "reward: -0.0\n",
      "epReward so far: -20000.0\n",
      "state : [0 0 1 9 1 1]\n",
      "action : 2\n",
      "new state: [0 1 0 9 1 2]\n",
      "reward: -1.0\n",
      "epReward so far: -20001.0\n",
      "state : [0 1 0 9 1 2]\n",
      "action : 1\n",
      "new state: [0 1 0 9 0 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -30001.0\n",
      "final state: [0 1 0 9 0 1]\n",
      "Episode : 14\n",
      "state : [0 1 0 9 0 1]\n",
      "action : 1\n",
      "new state: [0 1 0 9 3 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 1 0 9 3 0]\n",
      "action : 3\n",
      "new state: [1 1 0 8 2 3]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [1 1 0 8 2 3]\n",
      "action : 0\n",
      "new state: [1 1 0 8 2 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [1 1 0 8 2 2]\n",
      "action : 0\n",
      "new state: [0 1 1 8 0 0]\n",
      "reward: -1.0\n",
      "epReward so far: -20001.0\n",
      "state : [0 1 1 8 0 0]\n",
      "action : 1\n",
      "new state: [1 0 1 8 1 3]\n",
      "reward: -1.0\n",
      "epReward so far: -20002.0\n",
      "final state: [1 0 1 8 1 3]\n",
      "Episode : 15\n",
      "state : [1 0 1 8 1 3]\n",
      "action : 0\n",
      "new state: [0 0 1 9 2 1]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [0 0 1 9 2 1]\n",
      "action : 2\n",
      "new state: [0 0 1 9 3 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "state : [0 0 1 9 3 0]\n",
      "action : 3\n",
      "new state: [1 0 1 8 3 3]\n",
      "reward: -0.0\n",
      "epReward so far: -10001.0\n",
      "state : [1 0 1 8 3 3]\n",
      "action : 3\n",
      "new state: [1 0 1 8 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: -10001.0\n",
      "state : [1 0 1 8 2 2]\n",
      "action : 2\n",
      "new state: [1 0 1 8 2 1]\n",
      "reward: -0.0\n",
      "epReward so far: -10001.0\n",
      "final state: [1 0 1 8 2 1]\n",
      "Episode : 16\n",
      "state : [1 0 1 8 2 1]\n",
      "action : 2\n",
      "new state: [1 1 0 8 1 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 1 0 8 1 1]\n",
      "action : 1\n",
      "new state: [1 1 0 8 3 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 1 0 8 3 3]\n",
      "action : 3\n",
      "new state: [1 1 0 8 3 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 1 0 8 3 0]\n",
      "action : 3\n",
      "new state: [2 1 0 7 3 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 1 0 7 3 3]\n",
      "action : 3\n",
      "new state: [2 1 0 7 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "final state: [2 1 0 7 2 2]\n",
      "Episode : 17\n",
      "state : [2 1 0 7 2 2]\n",
      "action : 0\n",
      "new state: [2 1 0 7 1 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 1 0 7 1 2]\n",
      "action : 1\n",
      "new state: [2 0 1 7 1 1]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 0 1 7 1 1]\n",
      "action : 0\n",
      "new state: [1 1 1 7 1 0]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [1 1 1 7 1 0]\n",
      "action : 1\n",
      "new state: [2 0 1 7 3 3]\n",
      "reward: -0.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 0 1 7 3 3]\n",
      "action : 3\n",
      "new state: [2 0 1 7 3 3]\n",
      "reward: -0.0\n",
      "epReward so far: -10001.0\n",
      "final state: [2 0 1 7 3 3]\n",
      "Episode : 18\n",
      "state : [2 0 1 7 3 3]\n",
      "action : 3\n",
      "new state: [2 0 1 7 0 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 0 1 7 0 1]\n",
      "action : 0\n",
      "new state: [1 1 1 7 3 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 1 1 7 3 0]\n",
      "action : 3\n",
      "new state: [1 1 1 7 2 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [1 1 1 7 2 0]\n",
      "action : 2\n",
      "new state: [1 1 1 7 1 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [1 1 1 7 1 2]\n",
      "action : 1\n",
      "new state: [1 0 2 7 2 1]\n",
      "reward: -0.0\n",
      "epReward so far: -20000.0\n",
      "final state: [1 0 2 7 2 1]\n",
      "Episode : 19\n",
      "state : [1 0 2 7 2 1]\n",
      "action : 2\n",
      "new state: [1 0 2 7 2 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [1 0 2 7 2 1]\n",
      "action : 2\n",
      "new state: [1 0 2 7 1 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [1 0 2 7 1 3]\n",
      "action : 0\n",
      "new state: [0 0 2 8 2 1]\n",
      "reward: -1.0\n",
      "epReward so far: -20001.0\n",
      "state : [0 0 2 8 2 1]\n",
      "action : 2\n",
      "new state: [0 1 1 8 2 3]\n",
      "reward: -0.0\n",
      "epReward so far: -20001.0\n",
      "state : [0 1 1 8 2 3]\n",
      "action : 2\n",
      "new state: [0 1 0 9 1 3]\n",
      "reward: -0.0\n",
      "epReward so far: -20001.0\n",
      "final state: [0 1 0 9 1 3]\n",
      "Episode : 20\n",
      "state : [0 1 0 9 1 3]\n",
      "action : 1\n",
      "new state: [0 1 0 9 2 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 1 0 9 2 1]\n",
      "action : 1\n",
      "new state: [0 1 0 9 0 1]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [0 1 0 9 0 1]\n",
      "action : 1\n",
      "new state: [0 1 0 9 1 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "state : [0 1 0 9 1 2]\n",
      "action : 1\n",
      "new state: [0 0 1 9 2 1]\n",
      "reward: -0.0\n",
      "epReward so far: -20001.0\n",
      "state : [0 0 1 9 2 1]\n",
      "action : 2\n",
      "new state: [0 1 0 9 3 0]\n",
      "reward: -0.0\n",
      "epReward so far: -20001.0\n",
      "final state: [0 1 0 9 3 0]\n",
      "Episode : 21\n",
      "state : [0 1 0 9 3 0]\n",
      "action : 3\n",
      "new state: [1 1 0 8 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 1 0 8 0 2]\n",
      "action : 0\n",
      "new state: [0 1 1 8 0 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 1 1 8 0 3]\n",
      "action : 1\n",
      "new state: [0 1 1 8 1 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 1 1 8 1 2]\n",
      "action : 1\n",
      "new state: [0 0 2 8 3 0]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 0 2 8 3 0]\n",
      "action : 3\n",
      "new state: [0 0 2 8 3 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "final state: [0 0 2 8 3 3]\n",
      "Episode : 22\n",
      "state : [0 0 2 8 3 3]\n",
      "action : 3\n",
      "new state: [0 0 2 8 1 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 0 2 8 1 3]\n",
      "action : 2\n",
      "new state: [0 0 1 9 0 0]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [0 0 1 9 0 0]\n",
      "action : 2\n",
      "new state: [0 0 1 9 3 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "state : [0 0 1 9 3 3]\n",
      "action : 3\n",
      "new state: [0 0 1 9 0 3]\n",
      "reward: -0.0\n",
      "epReward so far: -10001.0\n",
      "state : [0 0 1 9 0 3]\n",
      "action : 2\n",
      "new state: [0 0 1 9 2 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "final state: [0 0 1 9 2 3]\n",
      "Episode : 23\n",
      "state : [0 0 1 9 2 3]\n",
      "action : 2\n",
      "new state: [ 0  0  0 10  0  3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [ 0  0  0 10  0  3]\n",
      "action : 3\n",
      "new state: [ 0  0  0 10  0  2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [ 0  0  0 10  0  2]\n",
      "action : 3\n",
      "new state: [ 0  0  0 10  0  3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [ 0  0  0 10  0  3]\n",
      "action : 3\n",
      "new state: [ 0  0  0 10  0  0]\n",
      "reward: -1.0\n",
      "epReward so far: -20001.0\n",
      "state : [ 0  0  0 10  0  0]\n",
      "action : 3\n",
      "new state: [ 0  0  0 10  0  0]\n",
      "reward: -10000.0\n",
      "epReward so far: -30001.0\n",
      "final state: [ 0  0  0 10  0  0]\n",
      "Episode : 24\n",
      "state : [ 0  0  0 10  0  0]\n",
      "action : 3\n",
      "new state: [ 0  0  0 10  3  0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [ 0  0  0 10  3  0]\n",
      "action : 3\n",
      "new state: [1 0 0 9 3 2]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [1 0 0 9 3 2]\n",
      "action : 3\n",
      "new state: [1 0 0 9 0 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [1 0 0 9 0 3]\n",
      "action : 0\n",
      "new state: [ 0  0  0 10  3  1]\n",
      "reward: -0.0\n",
      "epReward so far: -20000.0\n",
      "state : [ 0  0  0 10  3  1]\n",
      "action : 3\n",
      "new state: [0 1 0 9 1 2]\n",
      "reward: -0.0\n",
      "epReward so far: -20000.0\n",
      "final state: [0 1 0 9 1 2]\n",
      "Episode : 25\n",
      "state : [0 1 0 9 1 2]\n",
      "action : 1\n",
      "new state: [0 0 1 9 3 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 0 1 9 3 1]\n",
      "action : 3\n",
      "new state: [0 1 1 8 3 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 1 1 8 3 1]\n",
      "action : 3\n",
      "new state: [0 2 1 7 1 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 2 1 7 1 1]\n",
      "action : 1\n",
      "new state: [0 2 1 7 1 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 2 1 7 1 2]\n",
      "action : 1\n",
      "new state: [0 1 2 7 3 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "final state: [0 1 2 7 3 2]\n",
      "Episode : 26\n",
      "state : [0 1 2 7 3 2]\n",
      "action : 3\n",
      "new state: [0 1 3 6 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 1 3 6 0 2]\n",
      "action : 1\n",
      "new state: [0 0 4 6 0 2]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [0 0 4 6 0 2]\n",
      "action : 2\n",
      "new state: [0 0 4 6 3 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "state : [0 0 4 6 3 2]\n",
      "action : 3\n",
      "new state: [0 0 5 5 3 2]\n",
      "reward: -0.0\n",
      "epReward so far: -10001.0\n",
      "state : [0 0 5 5 3 2]\n",
      "action : 3\n",
      "new state: [0 0 6 4 0 1]\n",
      "reward: -0.0\n",
      "epReward so far: -10001.0\n",
      "final state: [0 0 6 4 0 1]\n",
      "Episode : 27\n",
      "state : [0 0 6 4 0 1]\n",
      "action : 2\n",
      "new state: [0 0 6 4 0 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 0 6 4 0 3]\n",
      "action : 2\n",
      "new state: [0 0 6 4 1 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [0 0 6 4 1 1]\n",
      "action : 2\n",
      "new state: [0 0 6 4 3 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -30000.0\n",
      "state : [0 0 6 4 3 3]\n",
      "action : 3\n",
      "new state: [0 0 6 4 0 1]\n",
      "reward: -0.0\n",
      "epReward so far: -30000.0\n",
      "state : [0 0 6 4 0 1]\n",
      "action : 2\n",
      "new state: [0 0 6 4 2 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -40000.0\n",
      "final state: [0 0 6 4 2 3]\n",
      "Episode : 28\n",
      "state : [0 0 6 4 2 3]\n",
      "action : 2\n",
      "new state: [0 0 6 4 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 0 6 4 0 2]\n",
      "action : 2\n",
      "new state: [0 0 6 4 0 2]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [0 0 6 4 0 2]\n",
      "action : 2\n",
      "new state: [0 0 6 4 1 0]\n",
      "reward: -1.0\n",
      "epReward so far: -10002.0\n",
      "state : [0 0 6 4 1 0]\n",
      "action : 2\n",
      "new state: [0 0 6 4 2 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20002.0\n",
      "state : [0 0 6 4 2 2]\n",
      "action : 2\n",
      "new state: [0 0 6 4 3 3]\n",
      "reward: -0.0\n",
      "epReward so far: -20002.0\n",
      "final state: [0 0 6 4 3 3]\n",
      "Episode : 29\n",
      "state : [0 0 6 4 3 3]\n",
      "action : 3\n",
      "new state: [0 0 6 4 3 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 0 6 4 3 0]\n",
      "action : 3\n",
      "new state: [1 0 6 3 1 2]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [1 0 6 3 1 2]\n",
      "action : 0\n",
      "new state: [0 0 7 3 3 3]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [0 0 7 3 3 3]\n",
      "action : 3\n",
      "new state: [0 0 7 3 1 0]\n",
      "reward: -0.0\n",
      "epReward so far: -10001.0\n",
      "state : [0 0 7 3 1 0]\n",
      "action : 2\n",
      "new state: [1 0 6 3 3 3]\n",
      "reward: -1.0\n",
      "epReward so far: -10002.0\n",
      "final state: [1 0 6 3 3 3]\n",
      "Episode : 30\n",
      "state : [1 0 6 3 3 3]\n",
      "action : 3\n",
      "new state: [1 0 6 3 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 0 6 3 2 2]\n",
      "action : 2\n",
      "new state: [1 0 6 3 2 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 0 6 3 2 1]\n",
      "action : 2\n",
      "new state: [1 1 5 3 2 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 1 5 3 2 0]\n",
      "action : 2\n",
      "new state: [1 1 5 3 2 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [1 1 5 3 2 2]\n",
      "action : 2\n",
      "new state: [1 1 5 3 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "final state: [1 1 5 3 0 2]\n",
      "Episode : 31\n",
      "state : [1 1 5 3 0 2]\n",
      "action : 0\n",
      "new state: [0 1 6 3 2 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 1 6 3 2 0]\n",
      "action : 2\n",
      "new state: [1 1 5 3 3 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 1 5 3 3 0]\n",
      "action : 3\n",
      "new state: [2 1 5 2 3 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 1 5 2 3 1]\n",
      "action : 3\n",
      "new state: [2 2 5 1 2 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 2 5 1 2 3]\n",
      "action : 2\n",
      "new state: [2 2 5 1 2 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "final state: [2 2 5 1 2 3]\n",
      "Episode : 32\n",
      "state : [2 2 5 1 2 3]\n",
      "action : 2\n",
      "new state: [2 2 4 2 1 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 2 4 2 1 3]\n",
      "action : 1\n",
      "new state: [2 2 4 2 0 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 2 4 2 0 3]\n",
      "action : 0\n",
      "new state: [1 2 4 3 2 1]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [1 2 4 3 2 1]\n",
      "action : 2\n",
      "new state: [1 2 4 3 3 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [1 2 4 3 3 0]\n",
      "action : 3\n",
      "new state: [2 2 4 2 1 0]\n",
      "reward: -0.0\n",
      "epReward so far: -20000.0\n",
      "final state: [2 2 4 2 1 0]\n",
      "Episode : 33\n",
      "state : [2 2 4 2 1 0]\n",
      "action : 1\n",
      "new state: [3 1 4 2 1 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 1 4 2 1 0]\n",
      "action : 1\n",
      "new state: [4 0 4 2 3 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [4 0 4 2 3 0]\n",
      "action : 3\n",
      "new state: [5 0 4 1 3 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [5 0 4 1 3 1]\n",
      "action : 3\n",
      "new state: [5 1 4 0 3 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [5 1 4 0 3 2]\n",
      "action : 0\n",
      "new state: [5 1 4 0 0 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "final state: [5 1 4 0 0 1]\n",
      "Episode : 34\n",
      "state : [5 1 4 0 0 1]\n",
      "action : 0\n",
      "new state: [4 2 4 0 3 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [4 2 4 0 3 2]\n",
      "action : 0\n",
      "new state: [3 2 5 0 0 3]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [3 2 5 0 0 3]\n",
      "action : 0\n",
      "new state: [2 2 5 1 2 1]\n",
      "reward: -0.0\n",
      "epReward so far: -1.0\n",
      "state : [2 2 5 1 2 1]\n",
      "action : 2\n",
      "new state: [2 3 4 1 0 0]\n",
      "reward: -0.0\n",
      "epReward so far: -1.0\n",
      "state : [2 3 4 1 0 0]\n",
      "action : 0\n",
      "new state: [2 3 4 1 3 0]\n",
      "reward: -0.0\n",
      "epReward so far: -1.0\n",
      "final state: [2 3 4 1 3 0]\n",
      "Episode : 35\n",
      "state : [2 3 4 1 3 0]\n",
      "action : 3\n",
      "new state: [3 3 4 0 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 3 4 0 0 2]\n",
      "action : 0\n",
      "new state: [2 3 5 0 0 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 3 5 0 0 1]\n",
      "action : 0\n",
      "new state: [1 4 5 0 3 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 4 5 0 3 1]\n",
      "action : 0\n",
      "new state: [0 5 5 0 0 0]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [0 5 5 0 0 0]\n",
      "action : 1\n",
      "new state: [1 4 5 0 1 0]\n",
      "reward: -1.0\n",
      "epReward so far: -2.0\n",
      "final state: [1 4 5 0 1 0]\n",
      "Episode : 36\n",
      "state : [1 4 5 0 1 0]\n",
      "action : 1\n",
      "new state: [2 3 5 0 1 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 3 5 0 1 3]\n",
      "action : 1\n",
      "new state: [2 2 5 1 0 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 2 5 1 0 1]\n",
      "action : 0\n",
      "new state: [2 2 5 1 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 2 5 1 0 2]\n",
      "action : 0\n",
      "new state: [1 2 6 1 0 1]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [1 2 6 1 0 1]\n",
      "action : 0\n",
      "new state: [1 2 6 1 2 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "final state: [1 2 6 1 2 0]\n",
      "Episode : 37\n",
      "state : [1 2 6 1 2 0]\n",
      "action : 2\n",
      "new state: [1 2 6 1 0 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [1 2 6 1 0 1]\n",
      "action : 0\n",
      "new state: [0 3 6 1 3 2]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 3 6 1 3 2]\n",
      "action : 3\n",
      "new state: [0 3 7 0 0 1]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 3 7 0 0 1]\n",
      "action : 1\n",
      "new state: [0 3 7 0 3 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [0 3 7 0 3 2]\n",
      "action : 1\n",
      "new state: [0 2 8 0 2 0]\n",
      "reward: -1.0\n",
      "epReward so far: -20001.0\n",
      "final state: [0 2 8 0 2 0]\n",
      "Episode : 38\n",
      "state : [0 2 8 0 2 0]\n",
      "action : 2\n",
      "new state: [1 2 7 0 1 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 2 7 0 1 0]\n",
      "action : 1\n",
      "new state: [2 1 7 0 0 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 1 7 0 0 1]\n",
      "action : 0\n",
      "new state: [1 2 7 0 3 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 2 7 0 3 2]\n",
      "action : 0\n",
      "new state: [1 2 7 0 1 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [1 2 7 0 1 0]\n",
      "action : 1\n",
      "new state: [2 1 7 0 1 2]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "final state: [2 1 7 0 1 2]\n",
      "Episode : 39\n",
      "state : [2 1 7 0 1 2]\n",
      "action : 1\n",
      "new state: [2 1 7 0 1 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 1 7 0 1 2]\n",
      "action : 1\n",
      "new state: [2 1 7 0 3 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [2 1 7 0 3 3]\n",
      "action : 0\n",
      "new state: [2 1 7 0 2 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -30000.0\n",
      "state : [2 1 7 0 2 0]\n",
      "action : 2\n",
      "new state: [2 1 7 0 3 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -40000.0\n",
      "state : [2 1 7 0 3 2]\n",
      "action : 0\n",
      "new state: [1 1 8 0 2 3]\n",
      "reward: -1.0\n",
      "epReward so far: -40001.0\n",
      "final state: [1 1 8 0 2 3]\n",
      "Episode : 40\n",
      "state : [1 1 8 0 2 3]\n",
      "action : 2\n",
      "new state: [1 1 7 1 3 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 1 7 1 3 2]\n",
      "action : 3\n",
      "new state: [1 1 8 0 2 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 1 8 0 2 0]\n",
      "action : 2\n",
      "new state: [2 1 7 0 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 1 7 0 2 2]\n",
      "action : 2\n",
      "new state: [2 1 7 0 1 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 1 7 0 1 0]\n",
      "action : 1\n",
      "new state: [3 0 7 0 1 0]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "final state: [3 0 7 0 1 0]\n",
      "Episode : 41\n",
      "state : [3 0 7 0 1 0]\n",
      "action : 0\n",
      "new state: [3 0 7 0 2 1]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [3 0 7 0 2 1]\n",
      "action : 2\n",
      "new state: [3 0 7 0 0 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "state : [3 0 7 0 0 3]\n",
      "action : 0\n",
      "new state: [2 0 7 1 3 1]\n",
      "reward: -0.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 0 7 1 3 1]\n",
      "action : 3\n",
      "new state: [2 0 7 1 2 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "state : [2 0 7 1 2 2]\n",
      "action : 2\n",
      "new state: [2 0 7 1 2 3]\n",
      "reward: -0.0\n",
      "epReward so far: -20001.0\n",
      "final state: [2 0 7 1 2 3]\n",
      "Episode : 42\n",
      "state : [2 0 7 1 2 3]\n",
      "action : 2\n",
      "new state: [2 0 6 2 3 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 0 6 2 3 2]\n",
      "action : 3\n",
      "new state: [2 0 7 1 3 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 0 7 1 3 1]\n",
      "action : 3\n",
      "new state: [2 1 7 0 3 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 1 7 0 3 1]\n",
      "action : 0\n",
      "new state: [2 1 7 0 1 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 1 7 0 1 0]\n",
      "action : 1\n",
      "new state: [3 0 7 0 2 3]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "final state: [3 0 7 0 2 3]\n",
      "Episode : 43\n",
      "state : [3 0 7 0 2 3]\n",
      "action : 2\n",
      "new state: [3 0 6 1 0 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 0 6 1 0 0]\n",
      "action : 0\n",
      "new state: [3 0 6 1 1 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [3 0 6 1 1 1]\n",
      "action : 0\n",
      "new state: [2 1 6 1 3 0]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 1 6 1 3 0]\n",
      "action : 3\n",
      "new state: [3 1 6 0 0 1]\n",
      "reward: -0.0\n",
      "epReward so far: -10001.0\n",
      "state : [3 1 6 0 0 1]\n",
      "action : 0\n",
      "new state: [2 2 6 0 0 0]\n",
      "reward: -0.0\n",
      "epReward so far: -10001.0\n",
      "final state: [2 2 6 0 0 0]\n",
      "Episode : 44\n",
      "state : [2 2 6 0 0 0]\n",
      "action : 0\n",
      "new state: [2 2 6 0 3 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 2 6 0 3 0]\n",
      "action : 0\n",
      "new state: [2 2 6 0 3 2]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [2 2 6 0 3 2]\n",
      "action : 0\n",
      "new state: [2 2 6 0 1 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 2 6 0 1 1]\n",
      "action : 1\n",
      "new state: [2 2 6 0 3 0]\n",
      "reward: -0.0\n",
      "epReward so far: -10001.0\n",
      "state : [2 2 6 0 3 0]\n",
      "action : 0\n",
      "new state: [2 2 6 0 0 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "final state: [2 2 6 0 0 1]\n",
      "Episode : 45\n",
      "state : [2 2 6 0 0 1]\n",
      "action : 0\n",
      "new state: [1 3 6 0 3 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 3 6 0 3 2]\n",
      "action : 0\n",
      "new state: [1 3 6 0 2 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [1 3 6 0 2 0]\n",
      "action : 2\n",
      "new state: [2 3 5 0 1 1]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 3 5 0 1 1]\n",
      "action : 1\n",
      "new state: [2 3 5 0 3 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [2 3 5 0 3 1]\n",
      "action : 0\n",
      "new state: [1 4 5 0 1 0]\n",
      "reward: -1.0\n",
      "epReward so far: -20001.0\n",
      "final state: [1 4 5 0 1 0]\n",
      "Episode : 46\n",
      "state : [1 4 5 0 1 0]\n",
      "action : 1\n",
      "new state: [2 3 5 0 1 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 3 5 0 1 2]\n",
      "action : 1\n",
      "new state: [2 3 5 0 1 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 3 5 0 1 1]\n",
      "action : 1\n",
      "new state: [2 3 5 0 3 1]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 3 5 0 3 1]\n",
      "action : 0\n",
      "new state: [2 3 5 0 2 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [2 3 5 0 2 3]\n",
      "action : 2\n",
      "new state: [2 3 4 1 2 1]\n",
      "reward: -0.0\n",
      "epReward so far: -20000.0\n",
      "final state: [2 3 4 1 2 1]\n",
      "Episode : 47\n",
      "state : [2 3 4 1 2 1]\n",
      "action : 2\n",
      "new state: [2 4 3 1 2 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 4 3 1 2 3]\n",
      "action : 2\n",
      "new state: [2 4 2 2 0 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 4 2 2 0 1]\n",
      "action : 0\n",
      "new state: [1 5 2 2 0 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 5 2 2 0 3]\n",
      "action : 0\n",
      "new state: [0 5 2 3 1 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 5 2 3 1 0]\n",
      "action : 1\n",
      "new state: [1 4 2 3 1 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "final state: [1 4 2 3 1 1]\n",
      "Episode : 48\n",
      "state : [1 4 2 3 1 1]\n",
      "action : 1\n",
      "new state: [1 4 2 3 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 4 2 3 2 2]\n",
      "action : 2\n",
      "new state: [1 4 2 3 2 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [1 4 2 3 2 3]\n",
      "action : 2\n",
      "new state: [1 4 2 3 2 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [1 4 2 3 2 3]\n",
      "action : 2\n",
      "new state: [1 4 2 3 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -30000.0\n",
      "state : [1 4 2 3 0 2]\n",
      "action : 0\n",
      "new state: [0 4 3 3 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: -30000.0\n",
      "final state: [0 4 3 3 0 2]\n",
      "Episode : 49\n",
      "state : [0 4 3 3 0 2]\n",
      "action : 1\n",
      "new state: [0 3 4 3 2 0]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [0 3 4 3 2 0]\n",
      "action : 2\n",
      "new state: [1 3 3 3 1 2]\n",
      "reward: -0.0\n",
      "epReward so far: -1.0\n",
      "state : [1 3 3 3 1 2]\n",
      "action : 1\n",
      "new state: [1 3 3 3 2 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "state : [1 3 3 3 2 1]\n",
      "action : 2\n",
      "new state: [1 3 3 3 3 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "state : [1 3 3 3 3 0]\n",
      "action : 3\n",
      "new state: [2 3 3 2 0 0]\n",
      "reward: -0.0\n",
      "epReward so far: -20001.0\n",
      "final state: [2 3 3 2 0 0]\n",
      "Episode : 50\n",
      "state : [2 3 3 2 0 0]\n",
      "action : 0\n",
      "new state: [2 3 3 2 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 3 3 2 0 2]\n",
      "action : 0\n",
      "new state: [1 3 4 2 2 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 3 4 2 2 0]\n",
      "action : 2\n",
      "new state: [2 3 3 2 0 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 3 3 2 0 0]\n",
      "action : 0\n",
      "new state: [2 3 3 2 0 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 3 3 2 0 0]\n",
      "action : 0\n",
      "new state: [2 3 3 2 1 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "final state: [2 3 3 2 1 1]\n",
      "Episode : 51\n",
      "state : [2 3 3 2 1 1]\n",
      "action : 1\n",
      "new state: [2 3 3 2 1 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 3 3 2 1 0]\n",
      "action : 1\n",
      "new state: [2 3 3 2 0 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 3 3 2 0 1]\n",
      "action : 0\n",
      "new state: [2 3 3 2 2 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [2 3 3 2 2 3]\n",
      "action : 2\n",
      "new state: [2 3 2 3 3 3]\n",
      "reward: -0.0\n",
      "epReward so far: -20000.0\n",
      "state : [2 3 2 3 3 3]\n",
      "action : 3\n",
      "new state: [2 3 2 3 1 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -30000.0\n",
      "final state: [2 3 2 3 1 3]\n",
      "Episode : 52\n",
      "state : [2 3 2 3 1 3]\n",
      "action : 1\n",
      "new state: [2 2 2 4 0 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 2 2 4 0 0]\n",
      "action : 0\n",
      "new state: [2 2 2 4 0 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 2 2 4 0 0]\n",
      "action : 0\n",
      "new state: [2 2 2 4 1 2]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 2 2 4 1 2]\n",
      "action : 1\n",
      "new state: [2 1 3 4 2 1]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 1 3 4 2 1]\n",
      "action : 2\n",
      "new state: [2 2 2 4 1 1]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "final state: [2 2 2 4 1 1]\n",
      "Episode : 53\n",
      "state : [2 2 2 4 1 1]\n",
      "action : 1\n",
      "new state: [2 2 2 4 3 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 2 2 4 3 3]\n",
      "action : 3\n",
      "new state: [2 2 2 4 1 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 2 2 4 1 0]\n",
      "action : 1\n",
      "new state: [3 1 2 4 0 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 1 2 4 0 1]\n",
      "action : 0\n",
      "new state: [2 2 2 4 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 2 2 4 2 2]\n",
      "action : 2\n",
      "new state: [2 2 2 4 2 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "final state: [2 2 2 4 2 1]\n",
      "Episode : 54\n",
      "state : [2 2 2 4 2 1]\n",
      "action : 2\n",
      "new state: [2 3 1 4 3 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 3 1 4 3 1]\n",
      "action : 3\n",
      "new state: [2 4 1 3 3 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 4 1 3 3 0]\n",
      "action : 3\n",
      "new state: [3 4 1 2 0 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 4 1 2 0 0]\n",
      "action : 0\n",
      "new state: [3 4 1 2 3 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [3 4 1 2 3 0]\n",
      "action : 3\n",
      "new state: [4 4 1 1 3 3]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "final state: [4 4 1 1 3 3]\n",
      "Episode : 55\n",
      "state : [4 4 1 1 3 3]\n",
      "action : 3\n",
      "new state: [4 4 1 1 2 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [4 4 1 1 2 0]\n",
      "action : 2\n",
      "new state: [5 4 0 1 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [5 4 0 1 2 2]\n",
      "action : 0\n",
      "new state: [4 4 1 1 0 0]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [4 4 1 1 0 0]\n",
      "action : 0\n",
      "new state: [4 4 1 1 2 3]\n",
      "reward: -0.0\n",
      "epReward so far: -10001.0\n",
      "state : [4 4 1 1 2 3]\n",
      "action : 2\n",
      "new state: [4 4 0 2 1 3]\n",
      "reward: -0.0\n",
      "epReward so far: -10001.0\n",
      "final state: [4 4 0 2 1 3]\n",
      "Episode : 56\n",
      "state : [4 4 0 2 1 3]\n",
      "action : 1\n",
      "new state: [4 3 0 3 2 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [4 3 0 3 2 3]\n",
      "action : 0\n",
      "new state: [3 3 0 4 0 0]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [3 3 0 4 0 0]\n",
      "action : 0\n",
      "new state: [3 3 0 4 3 2]\n",
      "reward: -0.0\n",
      "epReward so far: -1.0\n",
      "state : [3 3 0 4 3 2]\n",
      "action : 3\n",
      "new state: [3 3 0 4 1 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "state : [3 3 0 4 1 2]\n",
      "action : 1\n",
      "new state: [3 2 1 4 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: -10001.0\n",
      "final state: [3 2 1 4 0 2]\n",
      "Episode : 57\n",
      "state : [3 2 1 4 0 2]\n",
      "action : 0\n",
      "new state: [3 2 1 4 0 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [3 2 1 4 0 1]\n",
      "action : 0\n",
      "new state: [2 3 1 4 3 0]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 3 1 4 3 0]\n",
      "action : 3\n",
      "new state: [3 3 1 3 3 1]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [3 3 1 3 3 1]\n",
      "action : 3\n",
      "new state: [3 4 1 2 1 2]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [3 4 1 2 1 2]\n",
      "action : 1\n",
      "new state: [3 3 2 2 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "final state: [3 3 2 2 0 2]\n",
      "Episode : 58\n",
      "state : [3 3 2 2 0 2]\n",
      "action : 0\n",
      "new state: [2 3 3 2 1 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 3 3 2 1 1]\n",
      "action : 1\n",
      "new state: [2 3 3 2 2 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [2 3 3 2 2 0]\n",
      "action : 2\n",
      "new state: [3 3 2 2 2 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 3 2 2 2 1]\n",
      "action : 2\n",
      "new state: [3 3 2 2 2 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [3 3 2 2 2 1]\n",
      "action : 2\n",
      "new state: [3 3 2 2 1 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "final state: [3 3 2 2 1 1]\n",
      "Episode : 59\n",
      "state : [3 3 2 2 1 1]\n",
      "action : 1\n",
      "new state: [3 3 2 2 1 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 3 2 2 1 1]\n",
      "action : 1\n",
      "new state: [3 3 2 2 2 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 3 2 2 2 3]\n",
      "action : 2\n",
      "new state: [3 3 1 3 1 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 3 1 3 1 0]\n",
      "action : 1\n",
      "new state: [3 3 1 3 1 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [3 3 1 3 1 3]\n",
      "action : 1\n",
      "new state: [3 3 1 3 1 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "final state: [3 3 1 3 1 1]\n",
      "Episode : 60\n",
      "state : [3 3 1 3 1 1]\n",
      "action : 1\n",
      "new state: [3 3 1 3 3 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 3 1 3 3 3]\n",
      "action : 3\n",
      "new state: [3 3 1 3 1 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 3 1 3 1 2]\n",
      "action : 1\n",
      "new state: [3 2 2 3 0 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 2 2 3 0 0]\n",
      "action : 0\n",
      "new state: [3 2 2 3 1 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [3 2 2 3 1 3]\n",
      "action : 1\n",
      "new state: [3 1 2 4 2 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "final state: [3 1 2 4 2 3]\n",
      "Episode : 61\n",
      "state : [3 1 2 4 2 3]\n",
      "action : 2\n",
      "new state: [3 1 2 4 1 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [3 1 2 4 1 2]\n",
      "action : 1\n",
      "new state: [3 0 3 4 3 2]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [3 0 3 4 3 2]\n",
      "action : 3\n",
      "new state: [3 0 4 3 0 3]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [3 0 4 3 0 3]\n",
      "action : 0\n",
      "new state: [2 0 4 4 1 2]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [2 0 4 4 1 2]\n",
      "action : 0\n",
      "new state: [1 0 5 4 3 3]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "final state: [1 0 5 4 3 3]\n",
      "Episode : 62\n",
      "state : [1 0 5 4 3 3]\n",
      "action : 3\n",
      "new state: [1 0 5 4 0 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [1 0 5 4 0 3]\n",
      "action : 0\n",
      "new state: [0 0 5 5 1 3]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 0 5 5 1 3]\n",
      "action : 2\n",
      "new state: [0 0 5 5 0 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [0 0 5 5 0 1]\n",
      "action : 2\n",
      "new state: [0 0 5 5 3 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -30000.0\n",
      "state : [0 0 5 5 3 2]\n",
      "action : 3\n",
      "new state: [0 0 6 4 3 1]\n",
      "reward: -0.0\n",
      "epReward so far: -30000.0\n",
      "final state: [0 0 6 4 3 1]\n",
      "Episode : 63\n",
      "state : [0 0 6 4 3 1]\n",
      "action : 3\n",
      "new state: [0 1 6 3 3 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 1 6 3 3 0]\n",
      "action : 3\n",
      "new state: [1 1 6 2 3 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 1 6 2 3 2]\n",
      "action : 3\n",
      "new state: [1 1 6 2 3 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [1 1 6 2 3 1]\n",
      "action : 3\n",
      "new state: [1 2 6 1 1 2]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [1 2 6 1 1 2]\n",
      "action : 1\n",
      "new state: [1 1 7 1 1 2]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "final state: [1 1 7 1 1 2]\n",
      "Episode : 64\n",
      "state : [1 1 7 1 1 2]\n",
      "action : 1\n",
      "new state: [1 0 8 1 2 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 0 8 1 2 3]\n",
      "action : 2\n",
      "new state: [1 0 7 2 1 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 0 7 2 1 0]\n",
      "action : 0\n",
      "new state: [1 0 7 2 1 3]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [1 0 7 2 1 3]\n",
      "action : 0\n",
      "new state: [1 0 7 2 0 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "state : [1 0 7 2 0 3]\n",
      "action : 0\n",
      "new state: [0 0 7 3 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: -10001.0\n",
      "final state: [0 0 7 3 2 2]\n",
      "Episode : 65\n",
      "state : [0 0 7 3 2 2]\n",
      "action : 2\n",
      "new state: [0 0 7 3 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 0 7 3 2 2]\n",
      "action : 2\n",
      "new state: [0 0 7 3 0 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 0 7 3 0 3]\n",
      "action : 2\n",
      "new state: [0 0 6 4 2 1]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [0 0 6 4 2 1]\n",
      "action : 2\n",
      "new state: [0 1 5 4 3 3]\n",
      "reward: -0.0\n",
      "epReward so far: -1.0\n",
      "state : [0 1 5 4 3 3]\n",
      "action : 3\n",
      "new state: [0 1 5 4 3 1]\n",
      "reward: -0.0\n",
      "epReward so far: -1.0\n",
      "final state: [0 1 5 4 3 1]\n",
      "Episode : 66\n",
      "state : [0 1 5 4 3 1]\n",
      "action : 3\n",
      "new state: [0 2 5 3 1 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 2 5 3 1 1]\n",
      "action : 1\n",
      "new state: [0 2 5 3 0 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 2 5 3 0 1]\n",
      "action : 1\n",
      "new state: [0 2 5 3 0 3]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [0 2 5 3 0 3]\n",
      "action : 1\n",
      "new state: [0 2 5 3 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "state : [0 2 5 3 0 2]\n",
      "action : 1\n",
      "new state: [0 1 6 3 0 2]\n",
      "reward: -1.0\n",
      "epReward so far: -10002.0\n",
      "final state: [0 1 6 3 0 2]\n",
      "Episode : 67\n",
      "state : [0 1 6 3 0 2]\n",
      "action : 1\n",
      "new state: [0 1 6 3 1 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 1 6 3 1 1]\n",
      "action : 1\n",
      "new state: [0 1 6 3 3 1]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 1 6 3 3 1]\n",
      "action : 3\n",
      "new state: [0 2 6 2 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 2 6 2 2 2]\n",
      "action : 2\n",
      "new state: [0 2 6 2 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 2 6 2 2 2]\n",
      "action : 2\n",
      "new state: [0 2 6 2 0 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "final state: [0 2 6 2 0 3]\n",
      "Episode : 68\n",
      "state : [0 2 6 2 0 3]\n",
      "action : 1\n",
      "new state: [0 2 6 2 0 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 2 6 2 0 1]\n",
      "action : 1\n",
      "new state: [0 2 6 2 3 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [0 2 6 2 3 1]\n",
      "action : 3\n",
      "new state: [0 3 6 1 0 1]\n",
      "reward: -0.0\n",
      "epReward so far: -20000.0\n",
      "state : [0 3 6 1 0 1]\n",
      "action : 1\n",
      "new state: [0 3 6 1 0 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -30000.0\n",
      "state : [0 3 6 1 0 3]\n",
      "action : 1\n",
      "new state: [0 2 6 2 3 0]\n",
      "reward: -1.0\n",
      "epReward so far: -30001.0\n",
      "final state: [0 2 6 2 3 0]\n",
      "Episode : 69\n",
      "state : [0 2 6 2 3 0]\n",
      "action : 3\n",
      "new state: [1 2 6 1 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 2 6 1 0 2]\n",
      "action : 0\n",
      "new state: [0 2 7 1 1 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 2 7 1 1 0]\n",
      "action : 1\n",
      "new state: [1 1 7 1 3 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 1 7 1 3 2]\n",
      "action : 3\n",
      "new state: [1 1 7 1 3 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [1 1 7 1 3 1]\n",
      "action : 3\n",
      "new state: [1 2 7 0 1 1]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "final state: [1 2 7 0 1 1]\n",
      "Episode : 70\n",
      "state : [1 2 7 0 1 1]\n",
      "action : 1\n",
      "new state: [1 2 7 0 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 2 7 0 0 2]\n",
      "action : 0\n",
      "new state: [0 2 8 0 2 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 2 8 0 2 1]\n",
      "action : 2\n",
      "new state: [0 3 7 0 1 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 3 7 0 1 2]\n",
      "action : 1\n",
      "new state: [0 2 8 0 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 2 8 0 0 2]\n",
      "action : 1\n",
      "new state: [0 2 8 0 2 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "final state: [0 2 8 0 2 1]\n",
      "Episode : 71\n",
      "state : [0 2 8 0 2 1]\n",
      "action : 2\n",
      "new state: [0 3 7 0 3 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 3 7 0 3 0]\n",
      "action : 1\n",
      "new state: [1 2 7 0 1 2]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [1 2 7 0 1 2]\n",
      "action : 1\n",
      "new state: [1 1 8 0 3 0]\n",
      "reward: -0.0\n",
      "epReward so far: -1.0\n",
      "state : [1 1 8 0 3 0]\n",
      "action : 0\n",
      "new state: [1 1 8 0 0 0]\n",
      "reward: -1.0\n",
      "epReward so far: -2.0\n",
      "state : [1 1 8 0 0 0]\n",
      "action : 0\n",
      "new state: [1 1 8 0 0 3]\n",
      "reward: -0.0\n",
      "epReward so far: -2.0\n",
      "final state: [1 1 8 0 0 3]\n",
      "Episode : 72\n",
      "state : [1 1 8 0 0 3]\n",
      "action : 0\n",
      "new state: [1 1 8 0 3 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [1 1 8 0 3 1]\n",
      "action : 0\n",
      "new state: [0 2 8 0 2 2]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [0 2 8 0 2 2]\n",
      "action : 2\n",
      "new state: [0 2 8 0 0 0]\n",
      "reward: -0.0\n",
      "epReward so far: -10001.0\n",
      "state : [0 2 8 0 0 0]\n",
      "action : 1\n",
      "new state: [1 1 8 0 2 0]\n",
      "reward: -1.0\n",
      "epReward so far: -10002.0\n",
      "state : [1 1 8 0 2 0]\n",
      "action : 2\n",
      "new state: [1 1 8 0 3 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20002.0\n",
      "final state: [1 1 8 0 3 1]\n",
      "Episode : 73\n",
      "state : [1 1 8 0 3 1]\n",
      "action : 0\n",
      "new state: [1 1 8 0 1 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [1 1 8 0 1 1]\n",
      "action : 1\n",
      "new state: [1 1 8 0 2 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [1 1 8 0 2 0]\n",
      "action : 2\n",
      "new state: [1 1 8 0 1 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -30000.0\n",
      "state : [1 1 8 0 1 1]\n",
      "action : 1\n",
      "new state: [1 1 8 0 2 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -40000.0\n",
      "state : [1 1 8 0 2 2]\n",
      "action : 2\n",
      "new state: [1 1 8 0 3 0]\n",
      "reward: -0.0\n",
      "epReward so far: -40000.0\n",
      "final state: [1 1 8 0 3 0]\n",
      "Episode : 74\n",
      "state : [1 1 8 0 3 0]\n",
      "action : 0\n",
      "new state: [1 1 8 0 2 1]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [1 1 8 0 2 1]\n",
      "action : 2\n",
      "new state: [1 1 8 0 0 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "state : [1 1 8 0 0 1]\n",
      "action : 0\n",
      "new state: [0 2 8 0 1 0]\n",
      "reward: -0.0\n",
      "epReward so far: -10001.0\n",
      "state : [0 2 8 0 1 0]\n",
      "action : 1\n",
      "new state: [0 2 8 0 1 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "state : [0 2 8 0 1 0]\n",
      "action : 1\n",
      "new state: [1 1 8 0 3 1]\n",
      "reward: -0.0\n",
      "epReward so far: -20001.0\n",
      "final state: [1 1 8 0 3 1]\n",
      "Episode : 75\n",
      "state : [1 1 8 0 3 1]\n",
      "action : 0\n",
      "new state: [1 1 8 0 3 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [1 1 8 0 3 1]\n",
      "action : 0\n",
      "new state: [1 1 8 0 1 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [1 1 8 0 1 1]\n",
      "action : 1\n",
      "new state: [1 1 8 0 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: -20000.0\n",
      "state : [1 1 8 0 2 2]\n",
      "action : 2\n",
      "new state: [1 1 8 0 0 3]\n",
      "reward: -0.0\n",
      "epReward so far: -20000.0\n",
      "state : [1 1 8 0 0 3]\n",
      "action : 0\n",
      "new state: [0 1 8 1 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: -20000.0\n",
      "final state: [0 1 8 1 2 2]\n",
      "Episode : 76\n",
      "state : [0 1 8 1 2 2]\n",
      "action : 2\n",
      "new state: [0 1 8 1 3 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 1 8 1 3 2]\n",
      "action : 3\n",
      "new state: [0 1 9 0 1 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 1 9 0 1 0]\n",
      "action : 1\n",
      "new state: [1 0 9 0 0 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 0 9 0 0 0]\n",
      "action : 0\n",
      "new state: [1 0 9 0 3 2]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 0 9 0 3 2]\n",
      "action : 0\n",
      "new state: [1 0 9 0 0 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "final state: [1 0 9 0 0 3]\n",
      "Episode : 77\n",
      "state : [1 0 9 0 0 3]\n",
      "action : 0\n",
      "new state: [0 0 9 1 3 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 0 9 1 3 1]\n",
      "action : 3\n",
      "new state: [0 1 9 0 3 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 1 9 0 3 0]\n",
      "action : 1\n",
      "new state: [1 0 9 0 3 1]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [1 0 9 0 3 1]\n",
      "action : 0\n",
      "new state: [0 1 9 0 3 3]\n",
      "reward: -1.0\n",
      "epReward so far: -2.0\n",
      "state : [0 1 9 0 3 3]\n",
      "action : 1\n",
      "new state: [0 1 9 0 3 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10002.0\n",
      "final state: [0 1 9 0 3 3]\n",
      "Episode : 78\n",
      "state : [0 1 9 0 3 3]\n",
      "action : 1\n",
      "new state: [0 0 9 1 3 2]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [0 0 9 1 3 2]\n",
      "action : 3\n",
      "new state: [ 0  0 10  0  1  2]\n",
      "reward: -0.0\n",
      "epReward so far: -1.0\n",
      "state : [ 0  0 10  0  1  2]\n",
      "action : 2\n",
      "new state: [ 0  0 10  0  0  1]\n",
      "reward: -1.0\n",
      "epReward so far: -2.0\n",
      "state : [ 0  0 10  0  0  1]\n",
      "action : 2\n",
      "new state: [0 1 9 0 1 2]\n",
      "reward: -1.0\n",
      "epReward so far: -3.0\n",
      "state : [0 1 9 0 1 2]\n",
      "action : 1\n",
      "new state: [ 0  0 10  0  3  1]\n",
      "reward: -0.0\n",
      "epReward so far: -3.0\n",
      "final state: [ 0  0 10  0  3  1]\n",
      "Episode : 79\n",
      "state : [ 0  0 10  0  3  1]\n",
      "action : 2\n",
      "new state: [0 1 9 0 1 0]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [0 1 9 0 1 0]\n",
      "action : 1\n",
      "new state: [0 1 9 0 0 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "state : [0 1 9 0 0 1]\n",
      "action : 1\n",
      "new state: [0 1 9 0 0 3]\n",
      "reward: -1.0\n",
      "epReward so far: -10002.0\n",
      "state : [0 1 9 0 0 3]\n",
      "action : 1\n",
      "new state: [0 0 9 1 0 1]\n",
      "reward: -1.0\n",
      "epReward so far: -10003.0\n",
      "state : [0 0 9 1 0 1]\n",
      "action : 2\n",
      "new state: [0 1 8 1 1 3]\n",
      "reward: -1.0\n",
      "epReward so far: -10004.0\n",
      "final state: [0 1 8 1 1 3]\n",
      "Episode : 80\n",
      "state : [0 1 8 1 1 3]\n",
      "action : 1\n",
      "new state: [0 1 8 1 3 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 1 8 1 3 2]\n",
      "action : 3\n",
      "new state: [0 1 9 0 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 1 9 0 0 2]\n",
      "action : 1\n",
      "new state: [ 0  0 10  0  3  3]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [ 0  0 10  0  3  3]\n",
      "action : 2\n",
      "new state: [ 0  0 10  0  3  3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "state : [ 0  0 10  0  3  3]\n",
      "action : 2\n",
      "new state: [ 0  0 10  0  2  3]\n",
      "reward: -10000.0\n",
      "epReward so far: -30001.0\n",
      "final state: [ 0  0 10  0  2  3]\n",
      "Episode : 81\n",
      "state : [ 0  0 10  0  2  3]\n",
      "action : 2\n",
      "new state: [ 0  0 10  0  3  1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [ 0  0 10  0  3  1]\n",
      "action : 2\n",
      "new state: [ 0  0 10  0  2  1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [ 0  0 10  0  2  1]\n",
      "action : 2\n",
      "new state: [0 1 9 0 2 2]\n",
      "reward: -0.0\n",
      "epReward so far: -20000.0\n",
      "state : [0 1 9 0 2 2]\n",
      "action : 2\n",
      "new state: [0 1 9 0 1 3]\n",
      "reward: -0.0\n",
      "epReward so far: -20000.0\n",
      "state : [0 1 9 0 1 3]\n",
      "action : 1\n",
      "new state: [0 0 9 1 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: -20000.0\n",
      "final state: [0 0 9 1 0 2]\n",
      "Episode : 82\n",
      "state : [0 0 9 1 0 2]\n",
      "action : 2\n",
      "new state: [0 0 9 1 3 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 0 9 1 3 1]\n",
      "action : 3\n",
      "new state: [0 0 9 1 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [0 0 9 1 0 2]\n",
      "action : 2\n",
      "new state: [0 0 9 1 3 1]\n",
      "reward: -1.0\n",
      "epReward so far: -20001.0\n",
      "state : [0 0 9 1 3 1]\n",
      "action : 3\n",
      "new state: [0 1 9 0 3 1]\n",
      "reward: -0.0\n",
      "epReward so far: -20001.0\n",
      "state : [0 1 9 0 3 1]\n",
      "action : 1\n",
      "new state: [0 1 9 0 1 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -30001.0\n",
      "final state: [0 1 9 0 1 1]\n",
      "Episode : 83\n",
      "state : [0 1 9 0 1 1]\n",
      "action : 1\n",
      "new state: [0 1 9 0 1 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 1 9 0 1 0]\n",
      "action : 1\n",
      "new state: [0 1 9 0 3 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 1 9 0 3 1]\n",
      "action : 1\n",
      "new state: [0 1 9 0 3 1]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [0 1 9 0 3 1]\n",
      "action : 1\n",
      "new state: [0 1 9 0 3 1]\n",
      "reward: -1.0\n",
      "epReward so far: -10002.0\n",
      "state : [0 1 9 0 3 1]\n",
      "action : 1\n",
      "new state: [0 1 9 0 0 0]\n",
      "reward: -1.0\n",
      "epReward so far: -10003.0\n",
      "final state: [0 1 9 0 0 0]\n",
      "Episode : 84\n",
      "state : [0 1 9 0 0 0]\n",
      "action : 1\n",
      "new state: [1 0 9 0 0 3]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [1 0 9 0 0 3]\n",
      "action : 0\n",
      "new state: [1 0 9 0 3 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "state : [1 0 9 0 3 2]\n",
      "action : 0\n",
      "new state: [ 0  0 10  0  3  0]\n",
      "reward: -1.0\n",
      "epReward so far: -10002.0\n",
      "state : [ 0  0 10  0  3  0]\n",
      "action : 2\n",
      "new state: [ 0  0 10  0  0  2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20002.0\n",
      "state : [ 0  0 10  0  0  2]\n",
      "action : 2\n",
      "new state: [ 0  0 10  0  2  3]\n",
      "reward: -1.0\n",
      "epReward so far: -20003.0\n",
      "final state: [ 0  0 10  0  2  3]\n",
      "Episode : 85\n",
      "state : [ 0  0 10  0  2  3]\n",
      "action : 2\n",
      "new state: [0 0 9 1 3 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 0 9 1 3 0]\n",
      "action : 3\n",
      "new state: [1 0 9 0 1 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 0 9 0 1 1]\n",
      "action : 0\n",
      "new state: [0 1 9 0 1 0]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [0 1 9 0 1 0]\n",
      "action : 1\n",
      "new state: [1 0 9 0 2 3]\n",
      "reward: -0.0\n",
      "epReward so far: -1.0\n",
      "state : [1 0 9 0 2 3]\n",
      "action : 2\n",
      "new state: [1 0 9 0 3 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "final state: [1 0 9 0 3 1]\n",
      "Episode : 86\n",
      "state : [1 0 9 0 3 1]\n",
      "action : 0\n",
      "new state: [1 0 9 0 2 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [1 0 9 0 2 1]\n",
      "action : 2\n",
      "new state: [1 1 8 0 0 0]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [1 1 8 0 0 0]\n",
      "action : 0\n",
      "new state: [1 1 8 0 3 3]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [1 1 8 0 3 3]\n",
      "action : 0\n",
      "new state: [0 1 8 1 0 0]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [0 1 8 1 0 0]\n",
      "action : 1\n",
      "new state: [0 1 8 1 2 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "final state: [0 1 8 1 2 1]\n",
      "Episode : 87\n",
      "state : [0 1 8 1 2 1]\n",
      "action : 2\n",
      "new state: [0 1 8 1 0 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 1 8 1 0 1]\n",
      "action : 1\n",
      "new state: [0 1 8 1 3 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [0 1 8 1 3 2]\n",
      "action : 3\n",
      "new state: [0 1 9 0 1 2]\n",
      "reward: -0.0\n",
      "epReward so far: -20000.0\n",
      "state : [0 1 9 0 1 2]\n",
      "action : 1\n",
      "new state: [ 0  0 10  0  0  2]\n",
      "reward: -0.0\n",
      "epReward so far: -20000.0\n",
      "state : [ 0  0 10  0  0  2]\n",
      "action : 2\n",
      "new state: [ 0  0 10  0  1  2]\n",
      "reward: -1.0\n",
      "epReward so far: -20001.0\n",
      "final state: [ 0  0 10  0  1  2]\n",
      "Episode : 88\n",
      "state : [ 0  0 10  0  1  2]\n",
      "action : 2\n",
      "new state: [ 0  0 10  0  3  2]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [ 0  0 10  0  3  2]\n",
      "action : 2\n",
      "new state: [ 0  0 10  0  2  0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10001.0\n",
      "state : [ 0  0 10  0  2  0]\n",
      "action : 2\n",
      "new state: [1 0 9 0 2 3]\n",
      "reward: -0.0\n",
      "epReward so far: -10001.0\n",
      "state : [1 0 9 0 2 3]\n",
      "action : 2\n",
      "new state: [1 0 8 1 3 2]\n",
      "reward: -0.0\n",
      "epReward so far: -10001.0\n",
      "state : [1 0 8 1 3 2]\n",
      "action : 3\n",
      "new state: [1 0 8 1 0 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "final state: [1 0 8 1 0 0]\n",
      "Episode : 89\n",
      "state : [1 0 8 1 0 0]\n",
      "action : 0\n",
      "new state: [1 0 8 1 1 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [1 0 8 1 1 3]\n",
      "action : 0\n",
      "new state: [1 0 8 1 2 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [1 0 8 1 2 3]\n",
      "action : 2\n",
      "new state: [1 0 7 2 1 1]\n",
      "reward: -0.0\n",
      "epReward so far: -20000.0\n",
      "state : [1 0 7 2 1 1]\n",
      "action : 0\n",
      "new state: [1 0 7 2 3 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -30000.0\n",
      "state : [1 0 7 2 3 1]\n",
      "action : 3\n",
      "new state: [1 1 7 1 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: -30000.0\n",
      "final state: [1 1 7 1 0 2]\n",
      "Episode : 90\n",
      "state : [1 1 7 1 0 2]\n",
      "action : 0\n",
      "new state: [0 1 8 1 2 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 1 8 1 2 3]\n",
      "action : 2\n",
      "new state: [0 1 8 1 1 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 1 8 1 1 2]\n",
      "action : 1\n",
      "new state: [0 0 9 1 3 3]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 0 9 1 3 3]\n",
      "action : 3\n",
      "new state: [0 0 9 1 1 3]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 0 9 1 1 3]\n",
      "action : 2\n",
      "new state: [0 0 9 1 1 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "final state: [0 0 9 1 1 1]\n",
      "Episode : 91\n",
      "state : [0 0 9 1 1 1]\n",
      "action : 2\n",
      "new state: [0 0 9 1 2 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 0 9 1 2 1]\n",
      "action : 2\n",
      "new state: [0 1 8 1 0 3]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 1 8 1 0 3]\n",
      "action : 1\n",
      "new state: [0 1 8 1 3 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [0 1 8 1 3 2]\n",
      "action : 3\n",
      "new state: [0 1 9 0 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: -20000.0\n",
      "state : [0 1 9 0 0 2]\n",
      "action : 1\n",
      "new state: [0 1 9 0 3 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -30000.0\n",
      "final state: [0 1 9 0 3 3]\n",
      "Episode : 92\n",
      "state : [0 1 9 0 3 3]\n",
      "action : 1\n",
      "new state: [0 0 9 1 2 1]\n",
      "reward: -1.0\n",
      "epReward so far: -1.0\n",
      "state : [0 0 9 1 2 1]\n",
      "action : 2\n",
      "new state: [0 1 8 1 1 2]\n",
      "reward: -0.0\n",
      "epReward so far: -1.0\n",
      "state : [0 1 8 1 1 2]\n",
      "action : 1\n",
      "new state: [0 0 9 1 0 2]\n",
      "reward: -0.0\n",
      "epReward so far: -1.0\n",
      "state : [0 0 9 1 0 2]\n",
      "action : 2\n",
      "new state: [0 0 9 1 2 1]\n",
      "reward: -1.0\n",
      "epReward so far: -2.0\n",
      "state : [0 0 9 1 2 1]\n",
      "action : 2\n",
      "new state: [0 0 9 1 3 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10002.0\n",
      "final state: [0 0 9 1 3 2]\n",
      "Episode : 93\n",
      "state : [0 0 9 1 3 2]\n",
      "action : 3\n",
      "new state: [0 0 9 1 1 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 0 9 1 1 2]\n",
      "action : 2\n",
      "new state: [0 0 9 1 0 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [0 0 9 1 0 0]\n",
      "action : 2\n",
      "new state: [1 0 8 1 3 1]\n",
      "reward: -1.0\n",
      "epReward so far: -20001.0\n",
      "state : [1 0 8 1 3 1]\n",
      "action : 3\n",
      "new state: [1 0 8 1 1 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -30001.0\n",
      "state : [1 0 8 1 1 1]\n",
      "action : 0\n",
      "new state: [0 1 8 1 2 2]\n",
      "reward: -1.0\n",
      "epReward so far: -30002.0\n",
      "final state: [0 1 8 1 2 2]\n",
      "Episode : 94\n",
      "state : [0 1 8 1 2 2]\n",
      "action : 2\n",
      "new state: [0 1 8 1 0 3]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 1 8 1 0 3]\n",
      "action : 1\n",
      "new state: [0 1 8 1 0 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 1 8 1 0 1]\n",
      "action : 1\n",
      "new state: [0 1 8 1 2 1]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [0 1 8 1 2 1]\n",
      "action : 2\n",
      "new state: [0 2 7 1 1 2]\n",
      "reward: -0.0\n",
      "epReward so far: -10001.0\n",
      "state : [0 2 7 1 1 2]\n",
      "action : 1\n",
      "new state: [0 1 8 1 2 0]\n",
      "reward: -0.0\n",
      "epReward so far: -10001.0\n",
      "final state: [0 1 8 1 2 0]\n",
      "Episode : 95\n",
      "state : [0 1 8 1 2 0]\n",
      "action : 2\n",
      "new state: [1 1 7 1 3 0]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [1 1 7 1 3 0]\n",
      "action : 3\n",
      "new state: [1 1 7 1 1 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [1 1 7 1 1 3]\n",
      "action : 1\n",
      "new state: [1 0 7 2 0 0]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [1 0 7 2 0 0]\n",
      "action : 0\n",
      "new state: [1 0 7 2 1 3]\n",
      "reward: -0.0\n",
      "epReward so far: -10000.0\n",
      "state : [1 0 7 2 1 3]\n",
      "action : 0\n",
      "new state: [1 0 7 2 2 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "final state: [1 0 7 2 2 3]\n",
      "Episode : 96\n",
      "state : [1 0 7 2 2 3]\n",
      "action : 2\n",
      "new state: [1 0 7 2 3 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [1 0 7 2 3 3]\n",
      "action : 3\n",
      "new state: [1 0 7 2 0 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [1 0 7 2 0 3]\n",
      "action : 0\n",
      "new state: [0 0 7 3 3 3]\n",
      "reward: -0.0\n",
      "epReward so far: -20000.0\n",
      "state : [0 0 7 3 3 3]\n",
      "action : 3\n",
      "new state: [0 0 7 3 1 0]\n",
      "reward: -0.0\n",
      "epReward so far: -20000.0\n",
      "state : [0 0 7 3 1 0]\n",
      "action : 2\n",
      "new state: [0 0 7 3 0 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -30000.0\n",
      "final state: [0 0 7 3 0 3]\n",
      "Episode : 97\n",
      "state : [0 0 7 3 0 3]\n",
      "action : 2\n",
      "new state: [0 0 7 3 0 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 0 7 3 0 0]\n",
      "action : 2\n",
      "new state: [0 0 7 3 1 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [0 0 7 3 1 3]\n",
      "action : 2\n",
      "new state: [0 0 7 3 3 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -30000.0\n",
      "state : [0 0 7 3 3 3]\n",
      "action : 3\n",
      "new state: [0 0 7 3 3 3]\n",
      "reward: -0.0\n",
      "epReward so far: -30000.0\n",
      "state : [0 0 7 3 3 3]\n",
      "action : 3\n",
      "new state: [0 0 7 3 3 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -40000.0\n",
      "final state: [0 0 7 3 3 2]\n",
      "Episode : 98\n",
      "state : [0 0 7 3 3 2]\n",
      "action : 3\n",
      "new state: [0 0 8 2 2 1]\n",
      "reward: -0.0\n",
      "epReward so far: 0.0\n",
      "state : [0 0 8 2 2 1]\n",
      "action : 2\n",
      "new state: [0 0 8 2 0 0]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 0 8 2 0 0]\n",
      "action : 2\n",
      "new state: [0 0 8 2 3 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20000.0\n",
      "state : [0 0 8 2 3 2]\n",
      "action : 3\n",
      "new state: [0 0 8 2 1 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -30000.0\n",
      "state : [0 0 8 2 1 1]\n",
      "action : 2\n",
      "new state: [0 0 8 2 2 3]\n",
      "reward: -10000.0\n",
      "epReward so far: -40000.0\n",
      "final state: [0 0 8 2 2 3]\n",
      "Episode : 99\n",
      "state : [0 0 8 2 2 3]\n",
      "action : 2\n",
      "new state: [0 0 8 2 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -10000.0\n",
      "state : [0 0 8 2 0 2]\n",
      "action : 2\n",
      "new state: [0 0 8 2 2 3]\n",
      "reward: -1.0\n",
      "epReward so far: -10001.0\n",
      "state : [0 0 8 2 2 3]\n",
      "action : 2\n",
      "new state: [0 0 8 2 0 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -20001.0\n",
      "state : [0 0 8 2 0 2]\n",
      "action : 2\n",
      "new state: [0 0 8 2 0 1]\n",
      "reward: -10000.0\n",
      "epReward so far: -30001.0\n",
      "state : [0 0 8 2 0 1]\n",
      "action : 2\n",
      "new state: [0 0 8 2 2 2]\n",
      "reward: -10000.0\n",
      "epReward so far: -40001.0\n",
      "final state: [0 0 8 2 2 2]\n",
      "**************************************************\n",
      "Experiment complete\n",
      "**************************************************\n",
      "**************************************************\n",
      "Saving data\n",
      "**************************************************\n",
      "[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  1.40160000e+04\n",
      "  -5.22026380e+00]\n",
      " [ 1.00000000e+00  0.00000000e+00 -1.00000000e+04  1.03340000e+04\n",
      "  -4.96584246e+00]\n",
      " [ 2.00000000e+00  0.00000000e+00 -1.00010000e+04  1.36530000e+04\n",
      "  -5.16873458e+00]\n",
      " [ 3.00000000e+00  0.00000000e+00 -1.00000000e+04  1.17040000e+04\n",
      "  -5.46054418e+00]\n",
      " [ 4.00000000e+00  0.00000000e+00 -1.00010000e+04  1.10760000e+04\n",
      "  -4.98277536e+00]\n",
      " [ 5.00000000e+00  0.00000000e+00  0.00000000e+00  1.14560000e+04\n",
      "  -5.20611892e+00]\n",
      " [ 6.00000000e+00  0.00000000e+00 -1.00000000e+04  1.55450000e+04\n",
      "  -5.30941849e+00]\n",
      " [ 7.00000000e+00  0.00000000e+00 -3.00000000e+04  1.24050000e+04\n",
      "  -5.20698898e+00]\n",
      " [ 8.00000000e+00  0.00000000e+00 -2.00000000e+04  1.41370000e+04\n",
      "  -4.57065466e+00]\n",
      " [ 9.00000000e+00  0.00000000e+00 -1.00000000e+04  1.57580000e+04\n",
      "  -4.38530884e+00]\n",
      " [ 1.00000000e+01  0.00000000e+00 -2.00020000e+04  8.81200000e+03\n",
      "  -4.16311873e+00]\n",
      " [ 1.10000000e+01  0.00000000e+00 -1.00010000e+04  7.74400000e+03\n",
      "  -1.16052441e+00]\n",
      " [ 1.20000000e+01  0.00000000e+00 -1.00000000e+04  1.35230000e+04\n",
      "  -4.90778585e+00]\n",
      " [ 1.30000000e+01  0.00000000e+00 -3.00000000e+04  1.14590000e+04\n",
      "  -4.61663300e+00]\n",
      " [ 1.40000000e+01  0.00000000e+00  0.00000000e+00  1.07760000e+04\n",
      "  -5.03532886e+00]\n",
      " [ 1.50000000e+01  0.00000000e+00 -1.00000000e+04  9.67300000e+03\n",
      "  -5.29667458e+00]\n",
      " [ 1.60000000e+01  0.00000000e+00  0.00000000e+00  1.32160000e+04\n",
      "  -5.06516849e+00]\n",
      " [ 1.70000000e+01  0.00000000e+00  0.00000000e+00  1.31280000e+04\n",
      "  -5.36702513e+00]\n",
      " [ 1.80000000e+01  0.00000000e+00 -1.00000000e+04  1.68960000e+04\n",
      "  -4.84318885e+00]\n",
      " [ 1.90000000e+01  0.00000000e+00 -3.00000000e+04  1.26320000e+04\n",
      "  -5.30015583e+00]\n",
      " [ 2.00000000e+01  0.00000000e+00 -3.00010000e+04  1.08530000e+04\n",
      "  -5.10421700e+00]\n",
      " [ 2.10000000e+01  0.00000000e+00 -3.00000000e+04  1.20530000e+04\n",
      "  -5.37651895e+00]\n",
      " [ 2.20000000e+01  0.00000000e+00 -1.00000000e+00  9.76900000e+03\n",
      "  -5.23622360e+00]\n",
      " [ 2.30000000e+01  0.00000000e+00 -3.00000000e+04  1.43730000e+04\n",
      "  -4.89943132e+00]\n",
      " [ 2.40000000e+01  0.00000000e+00 -2.00010000e+04  1.48890000e+04\n",
      "  -5.20242960e+00]\n",
      " [ 2.50000000e+01  0.00000000e+00  0.00000000e+00  1.09040000e+04\n",
      "  -4.96324692e+00]\n",
      " [ 2.60000000e+01  0.00000000e+00 -2.00010000e+04  1.48730000e+04\n",
      "  -5.43737241e+00]\n",
      " [ 2.70000000e+01  0.00000000e+00 -1.00000000e+04  9.73900000e+03\n",
      "  -5.14936161e+00]\n",
      " [ 2.80000000e+01  0.00000000e+00 -1.00020000e+04  1.46210000e+04\n",
      "  -5.68045422e+00]\n",
      " [ 2.90000000e+01  0.00000000e+00 -1.00010000e+04  1.46200000e+04\n",
      "  -5.37368704e+00]\n",
      " [ 3.00000000e+01  0.00000000e+00 -2.00020000e+04  1.77210000e+04\n",
      "  -2.92609366e+00]\n",
      " [ 3.10000000e+01  0.00000000e+00 -2.00000000e+04  3.60900000e+03\n",
      "  -5.58705794e+00]\n",
      " [ 3.20000000e+01  0.00000000e+00  0.00000000e+00  8.32800000e+03\n",
      "  -5.73960815e+00]\n",
      " [ 3.30000000e+01  0.00000000e+00 -2.00000000e+04  1.26960000e+04\n",
      "  -5.45305727e+00]\n",
      " [ 3.40000000e+01  0.00000000e+00 -2.00010000e+04  8.16100000e+03\n",
      "  -5.28286952e+00]\n",
      " [ 3.50000000e+01  0.00000000e+00 -3.00000000e+04  9.87700000e+03\n",
      "  -5.31323483e+00]\n",
      " [ 3.60000000e+01  0.00000000e+00 -2.00010000e+04  1.89240000e+04\n",
      "  -5.31279932e+00]\n",
      " [ 3.70000000e+01  0.00000000e+00 -2.00000000e+04  1.07320000e+04\n",
      "  -5.25545278e+00]\n",
      " [ 3.80000000e+01  0.00000000e+00 -1.00010000e+04  1.14880000e+04\n",
      "  -5.15129445e+00]\n",
      " [ 3.90000000e+01  0.00000000e+00 -1.00000000e+04  3.89510000e+04\n",
      "  -4.68226380e+00]\n",
      " [ 4.00000000e+01  0.00000000e+00 -2.00000000e+04  1.51950000e+04\n",
      "  -5.39704371e+00]\n",
      " [ 4.10000000e+01  0.00000000e+00 -3.00010000e+04  1.37210000e+04\n",
      "  -5.55375908e+00]\n",
      " [ 4.20000000e+01  0.00000000e+00 -1.00000000e+04  1.26920000e+04\n",
      "  -5.46149803e+00]\n",
      " [ 4.30000000e+01  0.00000000e+00 -3.00000000e+04  1.27000000e+04\n",
      "  -5.32222922e+00]\n",
      " [ 4.40000000e+01  0.00000000e+00 -3.00000000e+04  1.38110000e+04\n",
      "  -5.35741822e+00]\n",
      " [ 4.50000000e+01  0.00000000e+00 -1.00020000e+04  1.21280000e+04\n",
      "  -4.48056353e+00]\n",
      " [ 4.60000000e+01  0.00000000e+00 -4.00000000e+04  1.00090000e+04\n",
      "  -5.51470176e+00]\n",
      " [ 4.70000000e+01  0.00000000e+00 -2.00000000e+04  1.34650000e+04\n",
      "  -5.22317887e+00]\n",
      " [ 4.80000000e+01  0.00000000e+00  0.00000000e+00  1.23840000e+04\n",
      "  -2.08504958e+00]\n",
      " [ 4.90000000e+01  0.00000000e+00 -3.00000000e+04  1.27000000e+04\n",
      "  -5.39326140e+00]\n",
      " [ 5.00000000e+01  0.00000000e+00 -1.00000000e+04  1.61500000e+04\n",
      "  -5.67543574e+00]\n",
      " [ 5.10000000e+01  0.00000000e+00  0.00000000e+00  2.11200000e+04\n",
      "  -4.95997795e+00]\n",
      " [ 5.20000000e+01  0.00000000e+00 -1.00020000e+04  1.70170000e+04\n",
      "  -5.37476737e+00]\n",
      " [ 5.30000000e+01  0.00000000e+00 -2.00010000e+04  1.60770000e+04\n",
      "  -5.10225537e+00]\n",
      " [ 5.40000000e+01  0.00000000e+00 -2.00000000e+04  1.46660000e+04\n",
      "  -5.56901898e+00]\n",
      " [ 5.50000000e+01  0.00000000e+00 -1.00010000e+04  7.27200000e+03\n",
      "  -5.37801538e+00]\n",
      " [ 5.60000000e+01  0.00000000e+00 -2.00000000e+04  1.80200000e+04\n",
      "  -5.10276503e+00]\n",
      " [ 5.70000000e+01  0.00000000e+00 -2.00010000e+04  9.54800000e+03\n",
      "  -5.44501842e+00]\n",
      " [ 5.80000000e+01  0.00000000e+00 -4.00000000e+04  2.78150000e+04\n",
      "  -5.00938533e+00]\n",
      " [ 5.90000000e+01  0.00000000e+00 -3.00000000e+04  1.38080000e+04\n",
      "  -5.25349034e+00]\n",
      " [ 6.00000000e+01  0.00000000e+00 -1.00010000e+04  5.16200000e+03\n",
      "  -5.40922284e+00]\n",
      " [ 6.10000000e+01  0.00000000e+00 -3.00000000e+04  1.21890000e+04\n",
      "  -4.85618929e+00]\n",
      " [ 6.20000000e+01  0.00000000e+00 -1.00020000e+04  1.22640000e+04\n",
      "  -5.32958184e+00]\n",
      " [ 6.30000000e+01  0.00000000e+00 -3.00000000e+04  7.08000000e+03\n",
      "  -5.13193020e+00]\n",
      " [ 6.40000000e+01  0.00000000e+00 -2.00000000e+04  9.81200000e+03\n",
      "  -5.38080781e+00]\n",
      " [ 6.50000000e+01  0.00000000e+00 -3.00000000e+04  1.25840000e+04\n",
      "  -4.11764614e+00]\n",
      " [ 6.60000000e+01  0.00000000e+00 -1.00000000e+04  1.26820000e+04\n",
      "  -5.15455172e+00]\n",
      " [ 6.70000000e+01  0.00000000e+00 -2.00000000e+04  9.37500000e+03\n",
      "  -1.59847942e+00]\n",
      " [ 6.80000000e+01  0.00000000e+00 -2.00000000e+04  1.15510000e+04\n",
      "  -4.91878478e+00]\n",
      " [ 6.90000000e+01  0.00000000e+00 -3.00000000e+04  1.20450000e+04\n",
      "  -5.24626714e+00]\n",
      " [ 7.00000000e+01  0.00000000e+00 -1.00010000e+04  1.76180000e+04\n",
      "  -5.30773235e+00]\n",
      " [ 7.10000000e+01  0.00000000e+00 -2.00000000e+04  1.23990000e+04\n",
      "  -5.35762059e+00]\n",
      " [ 7.20000000e+01  0.00000000e+00 -2.00000000e+00  1.22440000e+04\n",
      "  -5.50269667e+00]\n",
      " [ 7.30000000e+01  0.00000000e+00 -3.00000000e+04  1.70960000e+04\n",
      "  -5.21823710e+00]\n",
      " [ 7.40000000e+01  0.00000000e+00 -3.00000000e+04  1.14210000e+04\n",
      "  -5.41060899e+00]\n",
      " [ 7.50000000e+01  0.00000000e+00 -2.00000000e+04  9.90000000e+03\n",
      "  -5.25472212e+00]\n",
      " [ 7.60000000e+01  0.00000000e+00 -1.00000000e+00  1.45210000e+04\n",
      "  -5.34330342e+00]\n",
      " [ 7.70000000e+01  0.00000000e+00 -3.00010000e+04  1.41130000e+04\n",
      "  -5.46003957e+00]\n",
      " [ 7.80000000e+01  0.00000000e+00 -2.00010000e+04  1.86170000e+04\n",
      "  -4.99052653e+00]\n",
      " [ 7.90000000e+01  0.00000000e+00 -2.00000000e+04  1.03960000e+04\n",
      "  -5.38761462e+00]\n",
      " [ 8.00000000e+01  0.00000000e+00 -2.00000000e+04  1.42330000e+04\n",
      "  -5.54633779e+00]\n",
      " [ 8.10000000e+01  0.00000000e+00 -1.00010000e+04  1.62440000e+04\n",
      "  -5.78309327e+00]\n",
      " [ 8.20000000e+01  0.00000000e+00 -2.00000000e+00  1.73890000e+04\n",
      "  -4.81057371e+00]\n",
      " [ 8.30000000e+01  0.00000000e+00 -2.00020000e+04  1.78610000e+04\n",
      "  -5.21929400e+00]\n",
      " [ 8.40000000e+01  0.00000000e+00 -1.00010000e+04  6.34000000e+03\n",
      "  -5.21191907e+00]\n",
      " [ 8.50000000e+01  0.00000000e+00 -2.00010000e+04  8.24000000e+03\n",
      "  -1.31846310e+00]\n",
      " [ 8.60000000e+01  0.00000000e+00 -1.00010000e+04  1.44510000e+04\n",
      "  -3.16166174e+00]\n",
      " [ 8.70000000e+01  0.00000000e+00 -1.00000000e+00  1.19290000e+04\n",
      "  -3.59488239e+00]\n",
      " [ 8.80000000e+01  0.00000000e+00 -2.00030000e+04  1.04810000e+04\n",
      "  -2.48127213e+00]\n",
      " [ 8.90000000e+01  0.00000000e+00 -2.00020000e+04  1.79790000e+04\n",
      "  -3.48946805e+00]\n",
      " [ 9.00000000e+01  0.00000000e+00 -2.00000000e+04  1.55960000e+04\n",
      "  -3.21959826e+00]\n",
      " [ 9.10000000e+01  0.00000000e+00 -1.00000000e+04  1.16510000e+04\n",
      "  -3.37298971e+00]\n",
      " [ 9.20000000e+01  0.00000000e+00 -1.00000000e+00  1.73940000e+04\n",
      "  -2.66079145e+00]\n",
      " [ 9.30000000e+01  0.00000000e+00  0.00000000e+00  1.35120000e+04\n",
      "  -3.36906169e+00]\n",
      " [ 9.40000000e+01  0.00000000e+00 -2.00000000e+04  2.08940000e+04\n",
      "  -3.79953989e+00]\n",
      " [ 9.50000000e+01  0.00000000e+00 -1.00000000e+04  1.28350000e+04\n",
      "  -4.11340884e+00]\n",
      " [ 9.60000000e+01  0.00000000e+00 -2.00000000e+04  1.81050000e+04\n",
      "  -3.32284616e+00]\n",
      " [ 9.70000000e+01  0.00000000e+00 -1.00010000e+04  1.35050000e+04\n",
      "  -3.05516200e+00]\n",
      " [ 9.80000000e+01  0.00000000e+00 -2.00000000e+04  1.41800000e+04\n",
      "  -4.62028127e+00]\n",
      " [ 9.90000000e+01  0.00000000e+00 -2.00000000e+04  1.30010000e+04\n",
      "  -4.82582631e+00]\n",
      " [ 0.00000000e+00  1.00000000e+00 -1.00000000e+04  9.31200000e+03\n",
      "  -4.99126287e+00]\n",
      " [ 1.00000000e+00  1.00000000e+00 -2.00000000e+04  1.51160000e+04\n",
      "  -4.99185935e+00]\n",
      " [ 2.00000000e+00  1.00000000e+00 -2.00000000e+04  9.97000000e+03\n",
      "  -5.14199358e+00]\n",
      " [ 3.00000000e+00  1.00000000e+00 -2.00000000e+04  1.10070000e+04\n",
      "  -5.30879188e+00]\n",
      " [ 4.00000000e+00  1.00000000e+00 -1.00020000e+04  1.66750000e+04\n",
      "  -5.07389290e+00]\n",
      " [ 5.00000000e+00  1.00000000e+00 -1.00010000e+04  1.14200000e+04\n",
      "  -5.04787052e+00]\n",
      " [ 6.00000000e+00  1.00000000e+00 -2.00010000e+04  7.82500000e+03\n",
      "  -4.83787916e+00]\n",
      " [ 7.00000000e+00  1.00000000e+00 -1.00010000e+04  1.82670000e+04\n",
      "  -4.65771629e+00]\n",
      " [ 8.00000000e+00  1.00000000e+00 -1.00010000e+04  1.35920000e+04\n",
      "  -4.97304958e+00]\n",
      " [ 9.00000000e+00  1.00000000e+00 -1.00000000e+00  9.36100000e+03\n",
      "  -5.16201320e+00]\n",
      " [ 1.00000000e+01  1.00000000e+00 -2.00030000e+04  1.89210000e+04\n",
      "  -4.65814356e+00]\n",
      " [ 1.10000000e+01  1.00000000e+00 -1.00020000e+04  7.88500000e+03\n",
      "  -5.01630280e+00]\n",
      " [ 1.20000000e+01  1.00000000e+00 -3.00000000e+04  3.28620000e+04\n",
      "  -4.97304958e+00]\n",
      " [ 1.30000000e+01  1.00000000e+00 -3.00010000e+04  6.94100000e+03\n",
      "  -5.15575009e+00]\n",
      " [ 1.40000000e+01  1.00000000e+00 -2.00020000e+04  1.41770000e+04\n",
      "  -5.34310392e+00]\n",
      " [ 1.50000000e+01  1.00000000e+00 -1.00010000e+04  1.44010000e+04\n",
      "  -4.36964481e+00]\n",
      " [ 1.60000000e+01  1.00000000e+00  0.00000000e+00  1.50080000e+04\n",
      "  -3.35913714e+00]\n",
      " [ 1.70000000e+01  1.00000000e+00 -1.00010000e+04  1.57490000e+04\n",
      "  -3.75053314e+00]\n",
      " [ 1.80000000e+01  1.00000000e+00 -2.00000000e+04  1.56630000e+04\n",
      "  -4.88119913e+00]\n",
      " [ 1.90000000e+01  1.00000000e+00 -2.00010000e+04  1.53370000e+04\n",
      "  -4.51654379e+00]\n",
      " [ 2.00000000e+01  1.00000000e+00 -2.00010000e+04  1.49290000e+04\n",
      "  -4.21937706e+00]\n",
      " [ 2.10000000e+01  1.00000000e+00 -2.00000000e+04  1.62230000e+04\n",
      "  -4.43652459e+00]\n",
      " [ 2.20000000e+01  1.00000000e+00 -2.00010000e+04  1.61760000e+04\n",
      "  -4.63761611e+00]\n",
      " [ 2.30000000e+01  1.00000000e+00 -3.00010000e+04  1.38220000e+04\n",
      "  -5.45289030e+00]\n",
      " [ 2.40000000e+01  1.00000000e+00 -2.00000000e+04  1.76790000e+04\n",
      "  -4.98437653e+00]\n",
      " [ 2.50000000e+01  1.00000000e+00  0.00000000e+00  1.32240000e+04\n",
      "  -5.57271376e+00]\n",
      " [ 2.60000000e+01  1.00000000e+00 -1.00010000e+04  9.56400000e+03\n",
      "  -5.22543725e+00]\n",
      " [ 2.70000000e+01  1.00000000e+00 -4.00000000e+04  2.31050000e+04\n",
      "  -5.27482608e+00]\n",
      " [ 2.80000000e+01  1.00000000e+00 -2.00020000e+04  5.56100000e+03\n",
      "  -5.55215977e+00]\n",
      " [ 2.90000000e+01  1.00000000e+00 -1.00020000e+04  2.28610000e+04\n",
      "  -5.32262000e+00]\n",
      " [ 3.00000000e+01  1.00000000e+00 -1.00000000e+04  1.35180000e+04\n",
      "  -5.26911351e+00]\n",
      " [ 3.10000000e+01  1.00000000e+00 -1.00000000e+04  2.18010000e+04\n",
      "  -4.90762453e+00]\n",
      " [ 3.20000000e+01  1.00000000e+00 -2.00000000e+04  1.19960000e+04\n",
      "  -4.86468322e+00]\n",
      " [ 3.30000000e+01  1.00000000e+00 -1.00000000e+04  1.65850000e+04\n",
      "  -5.33327852e+00]\n",
      " [ 3.40000000e+01  1.00000000e+00 -1.00000000e+00  2.74360000e+04\n",
      "  -5.06751281e+00]\n",
      " [ 3.50000000e+01  1.00000000e+00 -2.00000000e+00  6.74600000e+03\n",
      "  -5.25545278e+00]\n",
      " [ 3.60000000e+01  1.00000000e+00 -2.00000000e+04  2.05190000e+04\n",
      "  -5.40906303e+00]\n",
      " [ 3.70000000e+01  1.00000000e+00 -2.00010000e+04  1.36250000e+04\n",
      "  -5.64052425e+00]\n",
      " [ 3.80000000e+01  1.00000000e+00 -1.00000000e+04  6.95800000e+03\n",
      "  -5.33897344e+00]\n",
      " [ 3.90000000e+01  1.00000000e+00 -4.00010000e+04  1.27140000e+04\n",
      "  -5.62738415e+00]\n",
      " [ 4.00000000e+01  1.00000000e+00 -1.00000000e+04  1.74780000e+04\n",
      "  -5.17133503e+00]\n",
      " [ 4.10000000e+01  1.00000000e+00 -2.00010000e+04  1.43970000e+04\n",
      "  -2.65230975e+00]\n",
      " [ 4.20000000e+01  1.00000000e+00 -1.00000000e+04  1.53820000e+04\n",
      "  -3.93530018e+00]\n",
      " [ 4.30000000e+01  1.00000000e+00 -1.00010000e+04  1.66640000e+04\n",
      "  -4.22818554e+00]\n",
      " [ 4.40000000e+01  1.00000000e+00 -2.00010000e+04  1.41440000e+04\n",
      "  -3.73153394e+00]\n",
      " [ 4.50000000e+01  1.00000000e+00 -2.00010000e+04  1.57400000e+04\n",
      "  -4.64342053e+00]\n",
      " [ 4.60000000e+01  1.00000000e+00 -2.00000000e+04  1.83000000e+04\n",
      "  -4.55257693e+00]\n",
      " [ 4.70000000e+01  1.00000000e+00  0.00000000e+00  2.00960000e+04\n",
      "  -5.11682362e+00]\n",
      " [ 4.80000000e+01  1.00000000e+00 -3.00000000e+04  1.92960000e+04\n",
      "  -5.15129445e+00]\n",
      " [ 4.90000000e+01  1.00000000e+00 -2.00010000e+04  1.60170000e+04\n",
      "  -5.08596640e+00]\n",
      " [ 5.00000000e+01  1.00000000e+00  0.00000000e+00  1.61760000e+04\n",
      "  -5.51689468e+00]\n",
      " [ 5.10000000e+01  1.00000000e+00 -3.00000000e+04  1.21440000e+04\n",
      "  -5.36651451e+00]\n",
      " [ 5.20000000e+01  1.00000000e+00 -1.00000000e+04  1.39040000e+04\n",
      "  -5.31338004e+00]\n",
      " [ 5.30000000e+01  1.00000000e+00  0.00000000e+00  1.91280000e+04\n",
      "  -5.52871218e+00]\n",
      " [ 5.40000000e+01  1.00000000e+00 -1.00000000e+04  8.67800000e+03\n",
      "  -5.02566166e+00]\n",
      " [ 5.50000000e+01  1.00000000e+00 -1.00010000e+04  1.69490000e+04\n",
      "  -5.46859625e+00]\n",
      " [ 5.60000000e+01  1.00000000e+00 -1.00010000e+04  2.28240000e+04\n",
      "  -4.86162818e+00]\n",
      " [ 5.70000000e+01  1.00000000e+00 -1.00000000e+04  1.35810000e+04\n",
      "  -5.52266661e+00]\n",
      " [ 5.80000000e+01  1.00000000e+00 -2.00000000e+04  9.36200000e+03\n",
      "  -5.09238809e+00]\n",
      " [ 5.90000000e+01  1.00000000e+00 -2.00000000e+04  1.31060000e+04\n",
      "  -3.53058372e+00]\n",
      " [ 6.00000000e+01  1.00000000e+00  0.00000000e+00  1.23120000e+04\n",
      "  -5.27226764e+00]\n",
      " [ 6.10000000e+01  1.00000000e+00 -1.00010000e+04  1.72370000e+04\n",
      "  -4.95020098e+00]\n",
      " [ 6.20000000e+01  1.00000000e+00 -3.00000000e+04  1.67890000e+04\n",
      "  -4.77700501e+00]\n",
      " [ 6.30000000e+01  1.00000000e+00 -1.00000000e+04  9.15500000e+03\n",
      "  -5.34725155e+00]\n",
      " [ 6.40000000e+01  1.00000000e+00 -1.00010000e+04  1.15110000e+04\n",
      "  -5.07069776e+00]\n",
      " [ 6.50000000e+01  1.00000000e+00 -1.00000000e+00  1.85630000e+04\n",
      "  -4.68368119e+00]\n",
      " [ 6.60000000e+01  1.00000000e+00 -1.00020000e+04  1.68230000e+04\n",
      "  -4.57298370e+00]\n",
      " [ 6.70000000e+01  1.00000000e+00 -2.00000000e+04  1.61610000e+04\n",
      "  -5.07214192e+00]\n",
      " [ 6.80000000e+01  1.00000000e+00 -3.00010000e+04  1.53010000e+04\n",
      "  -4.53677767e+00]\n",
      " [ 6.90000000e+01  1.00000000e+00 -1.00000000e+04  1.72220000e+04\n",
      "  -5.17664076e+00]\n",
      " [ 7.00000000e+01  1.00000000e+00 -1.00000000e+04  1.74810000e+04\n",
      "  -5.22836746e+00]\n",
      " [ 7.10000000e+01  1.00000000e+00 -2.00000000e+00  2.11800000e+04\n",
      "  -5.10870396e+00]\n",
      " [ 7.20000000e+01  1.00000000e+00 -2.00020000e+04  1.21050000e+04\n",
      "  -4.66926630e+00]\n",
      " [ 7.30000000e+01  1.00000000e+00 -4.00000000e+04  1.29850000e+04\n",
      "  -5.82298009e+00]\n",
      " [ 7.40000000e+01  1.00000000e+00 -2.00010000e+04  1.98530000e+04\n",
      "  -4.87097693e+00]\n",
      " [ 7.50000000e+01  1.00000000e+00 -2.00000000e+04  2.40410000e+04\n",
      "  -4.65972856e+00]\n",
      " [ 7.60000000e+01  1.00000000e+00 -1.00000000e+04  7.24100000e+03\n",
      "  -5.34081263e+00]\n",
      " [ 7.70000000e+01  1.00000000e+00 -1.00020000e+04  8.55500000e+03\n",
      "  -5.60380897e+00]\n",
      " [ 7.80000000e+01  1.00000000e+00 -3.00000000e+00  3.02560000e+04\n",
      "  -3.30410161e+00]\n",
      " [ 7.90000000e+01  1.00000000e+00 -1.00040000e+04  1.35830000e+04\n",
      "  -5.67543574e+00]\n",
      " [ 8.00000000e+01  1.00000000e+00 -3.00010000e+04  8.48300000e+03\n",
      "  -5.10205942e+00]\n",
      " [ 8.10000000e+01  1.00000000e+00 -2.00000000e+04  1.39110000e+04\n",
      "  -5.18265416e+00]\n",
      " [ 8.20000000e+01  1.00000000e+00 -3.00010000e+04  1.33170000e+04\n",
      "  -5.27366234e+00]\n",
      " [ 8.30000000e+01  1.00000000e+00 -1.00030000e+04  7.96800000e+03\n",
      "  -5.30192494e+00]\n",
      " [ 8.40000000e+01  1.00000000e+00 -2.00030000e+04  1.05710000e+04\n",
      "  -5.61408749e+00]\n",
      " [ 8.50000000e+01  1.00000000e+00 -1.00010000e+04  1.09370000e+04\n",
      "  -5.49355461e+00]\n",
      " [ 8.60000000e+01  1.00000000e+00 -2.00010000e+04  1.27060000e+04\n",
      "  -5.32330423e+00]\n",
      " [ 8.70000000e+01  1.00000000e+00 -2.00010000e+04  1.27240000e+04\n",
      "  -5.32442934e+00]\n",
      " [ 8.80000000e+01  1.00000000e+00 -2.00010000e+04  1.39790000e+04\n",
      "  -5.30855098e+00]\n",
      " [ 8.90000000e+01  1.00000000e+00 -3.00000000e+04  1.87090000e+04\n",
      "  -4.86045801e+00]\n",
      " [ 9.00000000e+01  1.00000000e+00 -2.00000000e+04  1.20280000e+04\n",
      "  -5.44496320e+00]\n",
      " [ 9.10000000e+01  1.00000000e+00 -3.00000000e+04  1.96610000e+04\n",
      "  -4.84479333e+00]\n",
      " [ 9.20000000e+01  1.00000000e+00 -1.00020000e+04  1.79330000e+04\n",
      "  -5.03240060e+00]\n",
      " [ 9.30000000e+01  1.00000000e+00 -3.00020000e+04  2.23090000e+04\n",
      "  -5.16706045e+00]\n",
      " [ 9.40000000e+01  1.00000000e+00 -1.00010000e+04  1.10080000e+04\n",
      "  -5.38454322e+00]\n",
      " [ 9.50000000e+01  1.00000000e+00 -2.00000000e+04  1.24440000e+04\n",
      "  -4.90019959e+00]\n",
      " [ 9.60000000e+01  1.00000000e+00 -3.00000000e+04  2.06050000e+04\n",
      "  -2.69568957e+00]\n",
      " [ 9.70000000e+01  1.00000000e+00 -4.00000000e+04  1.11690000e+04\n",
      "  -5.32707588e+00]\n",
      " [ 9.80000000e+01  1.00000000e+00 -4.00000000e+04  5.37200000e+03\n",
      "  -5.37858358e+00]\n",
      " [ 9.90000000e+01  1.00000000e+00 -4.00010000e+04  1.84810000e+04\n",
      "  -5.46459026e+00]]\n",
      "Writing to file data.csv\n",
      "**************************************************\n",
      "Data save complete\n",
      "**************************************************\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Step 6: Generate Figures\n",
    "\n",
    "Create a chart to compare the different heuristic functions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "fig_path = '../figures/'\n",
    "fig_name = 'rideshare'+'_line_plot'+'.pdf'\n",
    "or_suite.plots.plot_line_plots(path_list_line, algo_list_line, fig_path, fig_name, int(nEps / 40)+1)\n",
    "\n",
    "additional_metric = {'MRT': lambda traj : or_suite.utils.mean_response_time(traj, lambda x, y : np.abs(x-y)), 'RTV': lambda traj : or_suite.utils.response_time_variance(traj, lambda x, y : np.abs(x-y))}\n",
    "fig_name = 'rideshare'+'_'+'_radar_plot'+'.pdf'\n",
    "or_suite.plots.plot_radar_plots(path_list_radar, algo_list_radar,\n",
    "fig_path, fig_name,\n",
    "additional_metric\n",
    ")\n",
    "\n",
    "# TODO: Import figures and display\n"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/rideshare_metric_Random_10/data.csv'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/l5/43sgrxps7lbcv1k2xnpw4g5h0000gn/T/ipykernel_8200/2688220950.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfig_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../figures/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfig_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'rideshare'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_line_plot'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.pdf'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mor_suite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplots\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_line_plots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_list_line\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malgo_list_line\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfig_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfig_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnEps\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0madditional_metric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'MRT'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mtraj\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mor_suite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_response_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'RTV'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mtraj\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mor_suite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse_time_variance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Developer/ORSuite/or_suite/plots.py\u001b[0m in \u001b[0;36mplot_line_plots\u001b[0;34m(path_list, algo_list, fig_path, fig_name, plot_freq)\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[0;31m# reads in data for each algorithm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0malgo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0malgo_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m         df = pd.read_csv(path_list[index] +\n\u001b[0m\u001b[1;32m    255\u001b[0m                          '/data.csv').groupby(['episode']).mean()\n\u001b[1;32m    256\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'episode'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/ORSuite/lib/python3.8/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/ORSuite/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/ORSuite/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/ORSuite/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/ORSuite/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/ORSuite/lib/python3.8/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/ORSuite/lib/python3.8/site-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \"\"\"\n\u001b[0;32m--> 222\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/ORSuite/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    699\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    702\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/rideshare_metric_Random_10/data.csv'"
     ]
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "Failed to process string with tex because latex could not be found",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda3/envs/ORSuite/lib/python3.8/site-packages/matplotlib/texmanager.py\u001b[0m in \u001b[0;36m_run_checked_subprocess\u001b[0;34m(self, command, tex, cwd)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m             report = subprocess.check_output(\n\u001b[0m\u001b[1;32m    253\u001b[0m                 \u001b[0mcommand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcwd\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcwd\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexcache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/ORSuite/lib/python3.8/subprocess.py\u001b[0m in \u001b[0;36mcheck_output\u001b[0;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n\u001b[0m\u001b[1;32m    412\u001b[0m                **kwargs).stdout\n",
      "\u001b[0;32m/opt/anaconda3/envs/ORSuite/lib/python3.8/subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/ORSuite/lib/python3.8/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001b[0m\n\u001b[1;32m    853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 854\u001b[0;31m             self._execute_child(args, executable, preexec_fn, close_fds,\n\u001b[0m\u001b[1;32m    855\u001b[0m                                 \u001b[0mpass_fds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/ORSuite/lib/python3.8/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)\u001b[0m\n\u001b[1;32m   1701\u001b[0m                         \u001b[0merr_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1702\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1703\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'latex'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda3/envs/ORSuite/lib/python3.8/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    339\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mprinter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m             \u001b[0;31m# Finally look for special method names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/ORSuite/lib/python3.8/site-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(fig)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'png'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'retina'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'png2x'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mretina_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/ORSuite/lib/python3.8/site-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(fig, fmt, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mFigureCanvasBase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_io\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytes_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfmt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'svg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/ORSuite/lib/python3.8/site-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2228\u001b[0m                        else suppress())\n\u001b[1;32m   2229\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2230\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2232\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbbox_inches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/ORSuite/lib/python3.8/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdraw_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rasterizing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_rasterizing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/ORSuite/lib/python3.8/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/ORSuite/lib/python3.8/site-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   2782\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tight_layout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2783\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2784\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtight_layout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tight_parameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2785\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2786\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/ORSuite/lib/python3.8/site-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36mtight_layout\u001b[0;34m(self, pad, h_pad, w_pad, rect)\u001b[0m\n\u001b[1;32m   3162\u001b[0m                else suppress())\n\u001b[1;32m   3163\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3164\u001b[0;31m             kwargs = get_tight_layout_figure(\n\u001b[0m\u001b[1;32m   3165\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubplotspec_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3166\u001b[0m                 pad=pad, h_pad=h_pad, w_pad=w_pad, rect=rect)\n",
      "\u001b[0;32m/opt/anaconda3/envs/ORSuite/lib/python3.8/site-packages/matplotlib/tight_layout.py\u001b[0m in \u001b[0;36mget_tight_layout_figure\u001b[0;34m(fig, axes_list, subplotspec_list, renderer, pad, h_pad, w_pad, rect)\u001b[0m\n\u001b[1;32m    310\u001b[0m                               (colNum2 + 1) * div_col - 1))\n\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m     kwargs = auto_adjust_subplotpars(fig, renderer,\n\u001b[0m\u001b[1;32m    313\u001b[0m                                      \u001b[0mnrows_ncols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_nrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_ncols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m                                      \u001b[0mnum1num2_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum1num2_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/ORSuite/lib/python3.8/site-packages/matplotlib/tight_layout.py\u001b[0m in \u001b[0;36mauto_adjust_subplotpars\u001b[0;34m(fig, renderer, nrows_ncols, num1num2_list, subplot_list, ax_bbox_list, pad, h_pad, w_pad, rect)\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_visible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                     \u001b[0mbb\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tightbbox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfor_layout_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                     \u001b[0mbb\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tightbbox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/ORSuite/lib/python3.8/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36mget_tightbbox\u001b[0;34m(self, renderer, call_axes_locator, bbox_extra_artists, for_layout_only)\u001b[0m\n\u001b[1;32m   4428\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxaxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_visible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4429\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4430\u001b[0;31m                     bb_xaxis = self.xaxis.get_tightbbox(\n\u001b[0m\u001b[1;32m   4431\u001b[0m                         renderer, for_layout_only=for_layout_only)\n\u001b[1;32m   4432\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/ORSuite/lib/python3.8/site-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36mget_tightbbox\u001b[0;34m(self, renderer, for_layout_only)\u001b[0m\n\u001b[1;32m   1086\u001b[0m         \u001b[0mticks_to_draw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_ticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1088\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_label_position\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1089\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m         \u001b[0;31m# go back to just this axis's tick labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/ORSuite/lib/python3.8/site-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36m_update_label_position\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   2083\u001b[0m         \u001b[0;31m# get bounding boxes for this axis and any siblings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2084\u001b[0m         \u001b[0;31m# that have been set by `fig.align_xlabels()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2085\u001b[0;31m         \u001b[0mbboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbboxes2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tick_boxes_siblings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2086\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2087\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_position\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/ORSuite/lib/python3.8/site-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36m_get_tick_boxes_siblings\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   1871\u001b[0m             \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1872\u001b[0m             \u001b[0mticks_to_draw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_ticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1873\u001b[0;31m             \u001b[0mtlb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtlb2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tick_bboxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mticks_to_draw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1874\u001b[0m             \u001b[0mbboxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtlb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1875\u001b[0m             \u001b[0mbboxes2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtlb2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/ORSuite/lib/python3.8/site-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36m_get_tick_bboxes\u001b[0;34m(self, ticks, renderer)\u001b[0m\n\u001b[1;32m   1066\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_tick_bboxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mticks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m         \u001b[0;34m\"\"\"Return lists of bboxes for ticks' label1's and label2's.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1068\u001b[0;31m         return ([tick.label1.get_window_extent(renderer)\n\u001b[0m\u001b[1;32m   1069\u001b[0m                  for tick in ticks if tick.label1.get_visible()],\n\u001b[1;32m   1070\u001b[0m                 [tick.label2.get_window_extent(renderer)\n",
      "\u001b[0;32m/opt/anaconda3/envs/ORSuite/lib/python3.8/site-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1066\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_tick_bboxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mticks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m         \u001b[0;34m\"\"\"Return lists of bboxes for ticks' label1's and label2's.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1068\u001b[0;31m         return ([tick.label1.get_window_extent(renderer)\n\u001b[0m\u001b[1;32m   1069\u001b[0m                  for tick in ticks if tick.label1.get_visible()],\n\u001b[1;32m   1070\u001b[0m                 [tick.label2.get_window_extent(renderer)\n",
      "\u001b[0;32m/opt/anaconda3/envs/ORSuite/lib/python3.8/site-packages/matplotlib/text.py\u001b[0m in \u001b[0;36mget_window_extent\u001b[0;34m(self, renderer, dpi)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m             \u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_layout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_renderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_unitless_position\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/ORSuite/lib/python3.8/site-packages/matplotlib/text.py\u001b[0m in \u001b[0;36m_get_layout\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0;31m# Full vertical extent of font, including ascenders and descenders:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m         _, lp_h, lp_d = renderer.get_text_width_height_descent(\n\u001b[0m\u001b[1;32m    307\u001b[0m             \u001b[0;34m\"lp\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fontproperties\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m             ismath=\"TeX\" if self.get_usetex() else False)\n",
      "\u001b[0;32m/opt/anaconda3/envs/ORSuite/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mget_text_width_height_descent\u001b[0;34m(self, s, prop, ismath)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mtexmanager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_texmanager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mfontsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_size_in_points\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             w, h, d = texmanager.get_text_width_height_descent(\n\u001b[0m\u001b[1;32m    230\u001b[0m                 s, fontsize, renderer=self)\n\u001b[1;32m    231\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/ORSuite/lib/python3.8/site-packages/matplotlib/texmanager.py\u001b[0m in \u001b[0;36mget_text_width_height_descent\u001b[0;34m(self, tex, fontsize, renderer)\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m             \u001b[0;31m# use dviread.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m             \u001b[0mdvifile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_dvi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mdviread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDvi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdvifile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m72\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdpi_fraction\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdvi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m                 \u001b[0mpage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdvi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/ORSuite/lib/python3.8/site-packages/matplotlib/texmanager.py\u001b[0m in \u001b[0;36mmake_dvi\u001b[0;34m(self, tex, fontsize)\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0;31m# and thus replace() works atomically.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mTemporaryDirectory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdvifile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtmpdir\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m                 self._run_checked_subprocess(\n\u001b[0m\u001b[1;32m    292\u001b[0m                     [\"latex\", \"-interaction=nonstopmode\", \"--halt-on-error\",\n\u001b[1;32m    293\u001b[0m                      texfile], tex, cwd=tmpdir)\n",
      "\u001b[0;32m/opt/anaconda3/envs/ORSuite/lib/python3.8/site-packages/matplotlib/texmanager.py\u001b[0m in \u001b[0;36m_run_checked_subprocess\u001b[0;34m(self, command, tex, cwd)\u001b[0m\n\u001b[1;32m    254\u001b[0m                 stderr=subprocess.STDOUT)\n\u001b[1;32m    255\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mFileNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m    257\u001b[0m                 \u001b[0;34m'Failed to process string with tex because {} could not be '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m                 'found'.format(command[0])) from exc\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to process string with tex because latex could not be found"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 1080x360 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Visualization Demo\n",
    "\n",
    "The following demo includes a command-line interface with a visualization to demonstrate how the Ambulance Routing code works with 2 ambulances."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following commands need to be used in the terminal to install the necessary packages:\n",
    "* `apt-get install -y xvfb python-opengl`\n",
    "* `pip install gym pyvirtualdisplay`\n",
    "* `pip install scikit-learn-extra`\n",
    "* `pip install stable_baselines3`\n",
    "* `pip install pyglet==1.5.16`\n",
    "\n",
    "The first two are needed for the simulation but the rest should already be installed from the code demo above."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Simulation\n",
    "\n",
    "Run the cell below to run the simulation. In this simulation, you are trying to minimize the distance traveled by two ambulances to respond to incoming calls. The parameters in the problem are set up so the distance traveled to respond to a call is three times as costly as the distance traveled between calls. The reward is the negative cost, because reward is something that you want to maximize. Your goal therefore is to keep the reward as close to zero as possible.\n",
    "\n",
    "These are some questions you could ask yourself when choosing actions:\n",
    "\n",
    "* Should I focus more on minimizing distance traveled between calls, or distance traveled to respond to a call?\n",
    "\n",
    "* Do calls seem to arrive more in a certain part of the range $0$ to $1$? Can I take advantage of that?\n",
    "\n",
    "* Am I able to improve your performance over multiple rounds? (once you've finished you can re-run the cell to try again!)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(500, 800))\n",
    "display.start()\n",
    "\n",
    "import or_suite\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "import time\n",
    "import copy\n",
    "\n",
    "a = 5\n",
    "b = 2\n",
    "CONFIG = {'epLen': 5,\n",
    "    'arrival_dist': lambda x : np.random.beta(a,b), \n",
    "    'alpha': 0.25, \n",
    "    'starting_state': np.array([0.0, 0.0]), \n",
    "    'num_ambulance': 2,\n",
    "    'norm': 1\n",
    "}\n",
    "\n",
    "alpha = CONFIG['alpha']\n",
    "epLen = CONFIG['epLen']\n",
    "state = CONFIG['starting_state']\n",
    "num_ambulance = CONFIG['num_ambulance']\n",
    "\n",
    "agent = or_suite.agents.ambulance.command_line_metric.commandLineAgent(epLen)\n",
    "env = gym.make('Ambulance-v0', config=CONFIG)\n",
    "env.reset()\n",
    "\n",
    "\n",
    "done = False\n",
    "your_rewards = []\n",
    "heuristic_agent_rewards = []\n",
    "your_total_reward = 0\n",
    "heuristic_agent_total_reward = 0\n",
    "\n",
    "median_est = (a - 1/3)/(a + b - 2/3)\n",
    "heuristic_agent_states = [state]\n",
    "\n",
    "x_axis = ['Your Reward So Far', 'RL Algorithm Reward So Far']\n",
    "\n",
    "\n",
    "def display_animation(screen, time_to_display):\n",
    "    plt.imshow(screen)\n",
    "    ipythondisplay.clear_output(wait=True)\n",
    "    if time_to_display is not None:\n",
    "        ipythondisplay.display(plt.gcf())\n",
    "        time.sleep(time_to_display)\n",
    "\n",
    "def plot_rewards():\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_axes([0,0,1,1])\n",
    "    y_axis = [your_total_reward, heuristic_agent_total_reward]\n",
    "    ax.bar(x_axis, y_axis)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "while not done:\n",
    "    action = agent.greedy(state, 0)\n",
    "    state, reward, done, info = env.step(action)\n",
    "    your_rewards.append(reward)\n",
    "    your_total_reward += reward\n",
    "\n",
    "    # by comparing the final state with the action the user chose, we can figure out where the most recent arrival was\n",
    "    previous_arrival_ind = np.argmax(np.abs(action - state))\n",
    "    previous_arrival = state[previous_arrival_ind]\n",
    "\n",
    "    # the heuristic agent always chooses to put all the ambulances at the median estimate\n",
    "    heuristic_agent_action = np.full(num_ambulance, median_est)\n",
    "\n",
    "    # the state will have one ambulance where the call arrived, and all other ambulances at the median estimate\n",
    "    # doesn't matter which ambulance responds to the call because they're all at the same place\n",
    "    heuristic_agent_state = np.concatenate([np.full(num_ambulance - 1, median_est), [previous_arrival]])\n",
    "    heuristic_agent_states.append(heuristic_agent_state)\n",
    "\n",
    "    heuristic_agent_reward = -1 * (alpha * np.sum(np.abs(heuristic_agent_states[-2] - heuristic_agent_action)) + (1 - alpha) * np.sum(np.abs(heuristic_agent_action - heuristic_agent_state)))\n",
    "    heuristic_agent_rewards.append(heuristic_agent_reward)\n",
    "    heuristic_agent_total_reward += heuristic_agent_reward\n",
    "\n",
    "    screen1, screen2, screen3 = env.render(mode='rgb_array')\n",
    "\n",
    "    # display each step of the environment for 2 seconds\n",
    "    display_animation(screen1, 2)\n",
    "    display_animation(screen2, 2)\n",
    "    display_animation(screen3, None)\n",
    "\n",
    "    # plot your reward vs the agent's reward\n",
    "    plot_rewards()\n",
    "    time.sleep(2)\n",
    "\n",
    "    print(\"\\nThe most recent call arrival was at \" + str(previous_arrival) + \", and ambulance \" + str(previous_arrival_ind+1) + \" responded to the call.\\n\")\n",
    "\n",
    "    time.sleep(2)\n",
    "\n",
    "\n",
    "ipythondisplay.clear_output(wait=True)\n",
    "env.close()\n",
    "\n",
    "if np.sum(your_rewards) >= np.sum(heuristic_agent_rewards):\n",
    "    print(\"CONGRATS! You beat the RL algorithm.\")\n",
    "else:\n",
    "    print(\"You did not get a better reward than the RL algorithm.\")\n",
    "\n",
    "print(\"\\nYour total reward over all iterations was \", round(sum(your_rewards),3))\n",
    "print(\"The RL algorithm's total reward over all iterations was \", round(sum(heuristic_agent_rewards),3), \"\\n\")\n",
    "\n",
    "plot_rewards()"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 518.4x320.4 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/sean/anaconda3/envs/ORSuite/lib/python3.8/site-packages/IPython/core/pylabtools.py:132: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 518.4x320.4 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "The most recent call arrival was at 0.8320872, and ambulance 2 responded to the call.\n",
      "\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-6f9f52849e7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m   \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgreedy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m   \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0myour_rewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/ORSuite/or_suite/agents/ambulance/command_line_metric.py\u001b[0m in \u001b[0;36mgreedy\u001b[0;34m(self, state, timestep, epsilon)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mwhile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Where do you want to position ambulance \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mambulance\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"? (choose a number between 0 and 1)\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                 \u001b[0mnew_loc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                     \u001b[0mnew_loc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_loc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ORSuite/lib/python3.8/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m             )\n\u001b[0;32m--> 848\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    849\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ORSuite/lib/python3.8/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    890\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 892\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "metadata": {
    "scrolled": true
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('ORSuite': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "interpreter": {
   "hash": "4c142dd05be15695d7d0f22ccf2092ca7b90c6a6af78118fc1f80db7c62802c3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}